Semi-automated Construction of Decision Rules to Predict Morbidities from Clinical Texts. In this study the authors describe the system submitted by the team of University of Szeged to the second i2b2 Challenge in Natural Language Processing for Clinical Data. The challenge focused on the development of automatic systems that analyzed clinical discharge summary texts and addressed the following question: “Who's obese and what co-morbidities do they (definitely/most likely) have?”. Target diseases included obesity and its 15 most frequent comorbidities exhibited by patients, while the target labels corresponded to expert judgments based on textual evidence and intuition (separately).
A flexible framework to experiment with ontology learning techniques. Ontology learning refers to extracting conceptual knowledge from several sources and building an ontology from scratch, enriching, or adapting an existing ontology. It uses methods from a diverse spectrum of fields such as natural language processing, artificial intelligence and machine learning. However, a crucial challenging issue is to quantitatively evaluate the usefulness and accuracy of both techniques and combinations of techniques, when applied to ontology learning. It is an interesting problem because there are no published comparative studies. We are developing a flexible framework for ontology learning from text which provides a cyclical process that involves the successive application of various NLP techniques and learning algorithms for concept extraction and ontology modelling. The framework provides support to evaluate the usefulness and accuracy of different techniques and possible combinations of techniques into specific processes, to deal with the above challenge. We show our framework’s efficacy as a workbench for testing and evaluating concept identification. Our initial experiment supports our assumption about the usefulness of our approach.
Building an ontology of pulmonary diseases with natural language processing tools using textual corpora. Pathologies and acts are classified in thesauri to help physicians to code their activity. In practice, the use of thesauri is not sufficient to reduce variability in coding and thesauri are not suitable for computer processing. We think the automation of the coding task requires a conceptual modeling of medical items: an ontology. Our task is to help lung specialists code acts and diagnoses with software that represents medical knowledge of this concerned specialty by an ontology. The objective of the reported work was to build an ontology of pulmonary diseases dedicated to the coding process. To carry out this objective, we develop a precise methodological process for the knowledge engineer in order to build various types of medical ontologies. This process is based on the need to express precisely in natural language the meaning of each concept using differential semantics principles. A differential ontology is a hierarchy of concepts and relationships organized according to their similarities and differences. Our main research hypothesis is to apply natural language processing tools to corpora to develop the resources needed to build the ontology. We consider two corpora, one composed of patient discharge summaries and the other being a teaching book. We propose to combine two approaches to enrich the ontology building: (i) a method which consists of building terminological resources through distributional analysis and (ii) a method based on the observation of corpus sequences in order to reveal semantic relationships. Our ontology currently includes 1550 concepts and the software implementing the coding process is still under development. Results show that the proposed approach is operational and indicates that the combination of these methods and the comparison of the resulting terminological structures give interesting clues to a knowledge engineer for the building of an ontology.
Supporting the discovery and labeling of non-taxonomic relationships in ontology learning. Ontology learning (OL) from texts has been suggested as a technology that helps to reduce the bottleneck of knowledge acquisition in the construction of domain ontologies. In this learning process, the discovery, and possibly also labeling, of non-taxonomic relationships has been identified as one of the most difficult and often neglected problems. In this paper, we propose a technique that addresses this issue by analyzing a domain text corpus to extract verbs frequently applied for linking certain pairs of concepts. Integrated in an ontology building process, this technique aims to reduce the work-load of knowledge engineers and domain experts by suggesting candidate relationships that might become part of the ontology as well as prospective labels for them.
Ontology learning from biomedical natural language documents using UMLS. The generation of new knowledge is continuous in biomedical domains, thus biomedical literature is becoming harder to understand. Ontologies provide vocabulary standardization, so they can be helpful to facilitate the understanding of biomedical texts. In this work, a methodology for building biomedical ontologies from texts is presented. This approach relies on natural language processing and incremental knowledge acquisition techniques to obtain the relevant concepts and relations to be included in an OWL ontology. Additionally, we provide an algorithm to connect the isolated concepts regions in the ontology using UMLS. We also discuss in this paper the experiment carried out to validate our approach and its positive results in terms of performance and scalability.
Infrastructure for dynamic knowledge integration—Automated biomedical ontology extension using textual resources. We present a novel ontology integration technique that explicitly takes the dynamics and data-intensiveness of e-health and biomedicine application domains into account. Changing and growing knowledge, possibly contained in unstructured natural language resources, is handled by application of cutting-edge Semantic Web technologies. In particular, semi-automatic integration of ontology learning results into a manually developed ontology is employed. This integration bases on automatic negotiation of agreed alignments, inconsistency resolution and natural language generation methods. Their novel combination alleviates the end-user effort in the incorporation of new knowledge to large extent. This allows for efficient application in many practical use cases, as we show in the paper.
Towards open ontology learning and filtering. Open ontology learning is the process of extracting a domain ontology from a knowledge source in an unsupervised way. Due to its unsupervised nature, it requires filtering mechanisms to rate the importance and correctness of the extracted knowledge. This paper presents OntoCmaps, a domain-independent and open ontology learning tool that extracts deep semantic representations from corpora. OntoCmaps generates rich conceptual representations in the form of concept maps and proposes an innovative filtering mechanism based on metrics from graph theory. Our results show that using metrics such as Betweenness, PageRank, Hits and Degree centrality outperforms the results of standard text-based metrics (TF-IDF, term frequency) for concept identification. We propose voting schemes based on these metrics that provide a good performance in relationship identification, which again provides better results (in terms of precision and F-measure) than other traditional metrics such as frequency of co-occurrences. The approach is evaluated against a gold standard and is compared to the ontology learning tool Text2Onto. The OntoCmaps generated ontology is more expressive than Text2Onto ontology especially in conceptual relationships and leads to better results in terms of precision, recall and F-measure.
Learning ontological rules to extract multiple relations of genic interactions from text. Information extraction (IE) systems have been proposed in recent years to extract genic interactions from bibliographical resources. They are limited to single interaction relations, and have to face a trade-off between recall and precision, by focusing either on specific interactions (for precision), or general and unspecified interactions of biological entities (for recall). Yet, biologists need to process more complex data from literature, in order to study biological pathways. An ontology is an adequate formal representation to model this sophisticated knowledge. However, the tight integration of IE systems and ontologies is still a current research issue, a fortiori with complex ones that go beyond hierarchies.
Learning non-taxonomic relationships from web documents for domain ontology construction. In recent years, much effort has been put in ontology learning. However, the knowledge acquisition process is typically focused in the taxonomic aspect. The discovery of non-taxonomic relationships is often neglected, even though it is a fundamental point in structuring domain knowledge. This paper presents an automatic and unsupervised methodology that addresses the non-taxonomic learning process for constructing domain ontologies. It is able to discover domain-related verbs, extract non-taxonomically related concepts and label relationships, using the Web as corpus. The paper also discusses how the obtained relationships can be automatically evaluated against WordNet and presents encouraging results for several domains.
Learning domain ontologies for semantic Web service descriptions. High quality domain ontologies are essential for successful employment of semantic Web services. However, their acquisition is difficult and costly, thus hampering the development of this field. In this paper we report on the first stage of research that aims to develop (semi-)automatic ontology learning tools in the context of Web services that can support domain experts in the ontology building task. The goal of this first stage was to get a better understanding of the problem at hand and to determine which techniques might be feasible to use. To this end, we developed a framework for (semi-)automatic ontology learning from textual sources attached to Web services. The framework exploits the fact that these sources are expressed in a specific sublanguage, making them amenable to automatic analysis. We implement two methods in this framework, which differ in the complexity of the employed linguistic analysis. We evaluate the methods in two different domains, verifying the quality of the extracted ontologies against high quality hand-built ontologies of these domains. Our evaluation lead to a set of valuable conclusions on which further work can be based. First, it appears that our method, while tailored for the Web services context, might be applicable across different domains. Second, we concluded that deeper linguistic analysis is likely to lead to better results. Finally, the evaluation metrics indicate that good results can be achieved using only relatively simple, off the shelf techniques. Indeed, the novelty of our work is not in the used natural language processing methods but rather in the way they are put together in a generic framework specialized for the context of Web services.
Refining non-taxonomic relation labels with external structured data to support ontology learning. This paper presents a method to integrate external knowledge sources such as DBpedia and OpenCyc into an ontology learning system that automatically suggests labels for unknown relations in domain ontologies based on large corpora of unstructured text. The method extracts and aggregates verb vectors from semantic relations identified in the corpus. It composes a knowledge base which consists of (i) verb centroids for known relations between domain concepts, (ii) mappings between concept pairs and the types of known relations, and (iii) ontological knowledge retrieved from external sources. Applying semantic inference and validation to this knowledge base improves the quality of suggested relation labels. A formal evaluation compares the accuracy and average ranking precision of this hybrid method with the performance of methods that solely rely on corpus data and those that are only based on reasoning and external data sources.
Supporting the discovery and labeling of non-taxonomic relationships in ontology learning. Ontology learning (OL) from texts has been suggested as a technology that helps to reduce the bottleneck of knowledge acquisition in the construction of domain ontologies. In this learning process, the discovery, and possibly also labeling, of non-taxonomic relationships has been identified as one of the most difficult and often neglected problems. In this paper, we propose a technique that addresses this issue by analyzing a domain text corpus to extract verbs frequently applied for linking certain pairs of concepts. Integrated in an ontology building process, this technique aims to reduce the work-load of knowledge engineers and domain experts by suggesting candidate relationships that might become part of the ontology as well as prospective labels for them.
A methodology to learn ontological attributes from the Web. Class descriptors such as attributes, features or meronyms are rarely considered when developing ontologies. Even WordNet only includes a reduced amount of part-of relationships. However, these data are crucial for defining concepts such as those considered in classical knowledge representation models. Some attempts have been made to extract those relations from text using general meronymy detection patterns; however, there has been very little work on learning expressive class attributes (including associated domain, range or data values) at an ontological level. In this paper we take this background into consideration when proposing and implementing an automatic, non-supervised and domain-independent methodology to extend ontological classes in terms of learning concept attributes, data-types, value ranges and measurement units. In order to present a general solution and minimize the data sparseness of pattern-based approaches, we use the Web as a massive learning corpus to retrieve data and to infer information distribution using highly contextualized queries aimed at improving the quality of the result. This corpus is also automatically updated in an adaptive manner according to the knowledge already acquired and the learning throughput. Results have been manually checked by means of an expert-based concept-per-concept evaluation for several well distinguished domains showing reliable results and a reasonable learning performance.
A Lexico-Semantic Pattern Language for LearningOntology Instances from Text. The Semantic Web aims to extend the World Wide Web with a layer of semantic information, so that it is understandable not only by humans, but also by computers. At its core, the Semantic Web consists of ontologies that describe the meaning of concepts in a certain domain or across domains. The domain ontologies are mostly created and maintained by domain experts using manual, time-intensive processes. In this paper, we propose a rule-based method for learning ontology instances from text that helps domain experts with the ontology population process. In this method we define a lexico-semantic pattern language that, in addition to the lexical and syntactical information present in lexico-syntactic rules, also makes use of semantic information. We show that the lexico-semantic patterns are superior to lexico-syntactic patterns with respect to efficiency and effectivity. When applied to event relation recognition in text-based news items in the domains of finance and politics using Hermes, an ontology-driven news personalization service, our approach has a precision and recall of approximately 80% and 70%, respectively.
Learning ontologies from natural language texts. Research on ontology is becoming increasingly widespread in the computer science community. The major problems in building ontologies are the bottleneck of knowledge acquisition and time-consuming construction of various ontologies for various domains/applications. Meanwhile moving toward automation of ontology construction is a solution. We proposed an automatic ontology building approach. In this approach, the system starts from a small ontology kernel and constructs the ontology through text understanding automatically. The kernel contains the primitive concepts, relations and operators to build an ontology. The features of our proposed model are being domain/application independent, building ontologies upon a small primary kernel, learning words, concepts, taxonomic and non-taxonomic relations and axioms and applying a symbolic, hybrid ontology learning approach consisting of logical, linguistic based, template driven and semantic analysis methods.  Hasti is an ongoing project to implement and test the automatic ontology building approach. It extracts lexical and ontological knowledge from Persian (Farsi) texts.  In this paper, at first, we will describe some ontology engineering problems, which motivated our approach. In the next sections, after a brief description of Hasti, its features and its architecture, we will discuss its components in detail. In each part, the learning algorithms will be described. Then some experimental results will be discussed and at last, we will have an overview of related works and will introduce a general framework to compare ontology learning systems and will compare Hasti with related works according to the framework.
Learning relation axioms from text: An automatic Web-based approach. Even though expressive ontology representation languages like OWL have been proposed in the last years, most available ontologies only focus on taxonomical and, in a few cases, non-taxonomical knowledge. A way to add expressivity to an ontology is to include axioms that describe the basic logical properties of the modeled relationships. However, due to the manual knowledge acquisition bottleneck, axioms are usually missing in handcrafted ontologies. Moreover, automatic or semi-automatic ontology learning approaches aimed to acquire axioms from textual resources to enrich ontologies are scarce. This paper presents a novel methodology to learn axioms associated to non-taxonomic relationships modeled in an ontology in an automatic and unsupervised way, using the Web as corpus of textual resources. The proposed method is based on the use of specially tailored linguistic patterns, the exploitation of Web search engines as massive information retrieval tools, the application of shallow natural language processing and the assessment of semantic relatedness by means of Web-scale co-occurrence statistics. The paper describes the learning algorithm and presents promising results on the assessment of the following axioms: functional, inverse, symmetrical, transitive, reflexive and inverse functional properties.
GRAONTO: A graph-based approach for automatic construction of domain ontology. Extracting domain knowledge and taking its full advantage has been an important way to reducing costs and accelerating processes in domain-related applications. Domain ontology, providing a common and unambiguous understanding of a domain for both the users and the system to communicate with each other via a set of representational primitives, has been proposed as an important and natural approach to represent domain knowledge. Most domain knowledge about domain entities with their properties and relationships is embodied in document collections. Thus, extracting ontologies from these documents is an important means of ontology construction. In this paper, a graph-based approach for automatic construction of domain ontology from domain corpus, named GRAONTO, has been proposed. First, each document in the collection is represented by a graph. After the generation of document graphs, random walk term weighting is employed to estimate the relevance of the information of a term to the corpus from both local and global perspectives. Next, the MCL (Markov Clustering) algorithm is used to disambiguate terms with different meanings and group similar terms to produce concepts. Next, an improved gSpan algorithm constrained by both vertices and informativeness is exploited to find arbitrary latent relations among these concepts. Finally, the domain ontology is output in the OWL format. For ontology evaluation purposes, a method for adaptive adjustment of concepts and relations with respect to its practical effectiveness is conceived. Evaluation experiments show that GRAONTO is a promising approach for domain ontology construction.

