A domain ontology for construction concepts in urban infrastructure products. abstract: Domain ontologies are the cornerstone of informatics systems. Like their philosophical counterparts, they aim at providing a shared representation (language) for the concepts within a domain of knowledge. Ontologies are related more to knowledge representation rather than reasoning. Consequently, they normally can be complemented by artificial intelligence tools to enhance their decision support capabilities. This paper presents an ontology that is an abstract (yet extendable) philosophical (yet practical) conceptualization of the essence of knowledge that relates to construction aspects of infrastructure products. A product is the outcome of any work process and includes physical products, decisions, abstract knowledge and knowledge items generated based on all of these. A set of related constraints, mechanism, actors and process are identified along with these products. Product attributes and modalities are also presented to help describe the behavior of these products and support the generation of types or classes of these products. 1. Introduction: the dawn of e-cityThe realization of e-city (cities that exploit information technology)is a necessary component for green-city. In other words, we cannotbuild 21st century cities without coherent business-savvy and humanoriented informatics solutions. One of the key domains where information technology can support e-city is the management of civilinfrastructure systems in a sustainable way. Sustainability is a multidisciplinary domain (encapsulating engineering, environmental, economic and social sciences). It relies on knowledge and human wisdomas much as it relies on data and information. Moreover, given that theconcepts of sustainability are relatively new, it is impossible to allocateall required “human” expertise in one place, hence sustainabilityanalysis is by default “networked”. In fact, the desire for sustainabledevelopment along with globalization and the emergence of knowledgeeconomies are reshaping infrastructure as a global e-industry. Soon,virtual enterprises and “knowledge products” (in the form of webservices) will flourish as valuable commodities in a new e-market forcivil infrastructure This triggers discussions (and concerns) about the suitability of ourknowledge management systems in the urban infrastructure domainto the demands of the globalized knowledge economy. Case after case,the massive and complex nature of rebuilding our urban environmentis magnifying the inefficiencies in the current approaches: focus on short term costs (in contrast to life cycle costing); lack of coordination and sporadic project developments (in contrast to coherent integrated development); limited community engagement; inadequate tools for quantifying sustainability-related costs; and lack of mechanisms to harness, use and share knowledge. Knowledge can be seen as spanning three spheres: software (means for static capture of knowledge), work processes (reflection of best practices and value generation), and human judgment (the wisdom/ intelligence of human experts). It is sad to notice that except for the software facet, knowledge management practices in the infrastructure domain have lagged almost every other industrial sector. Extensive work has been dedicated to achieve interoperability in the software sphere. For example, XML and data exchange standards (such as industry foundation classes-IFC) have been introduced to address data interoperability problems in the domain. However, there is very limited application of business process management (BPM). More alarming, communication protocols in the industry are normally based on verbal or paper-based techniques. While, the advancement of social networking and is emphasizing the role of human communication alongside computer-based systems, current information systems in the domain focus on data handling and utilize database-driven applications. Such systems are static in nature and cannot support the varying subjective nature of a decision situation [1]. Moreover, information is spread amongst several organizations. Even though a good part of such information is represented in GISbased systems, they usually lack interoperability. It is argued that what we need at this stage is the development of a fundamental model that facilitates (philosophical and linguistically) representation of infrastructure knowledge (not just data or More alarming, communication protocols in the industry are normally based on verbal or paper-based techniques. While, the advancement of social networking and is emphasizing the role of human communication alongside computer-based systems, current information systems in the domain focus on data handling and utilize database-driven applications. Such systems are static in nature and cannot support the varying subjective nature of a decision situation [1]. Moreover, information is spread amongst several organizations. Even though a good part of such information is represented in GISbased systems, they usually lack interoperability. It is argued that what we need at this stage is the development of a fundamental model that facilitates (philosophical and linguistically) representation of infrastructure knowledge (not just data or hypertext) and at the same time supports the exchange and communication of such knowledge in a machine-interpretable manner. Informatics ontologies have been proposed as means to help formalize knowledge in a manner that can integrate two complementary objectives: the representation of knowledge in a human-friendly manner to facilitate people-to-people communication and, at the same time, reasoning about such knowledge in a formal machine-interpretable manner. To this end, the proposed ontology is an attempt to present a philosophically-influenced model of infrastructure products that focuses on their functions, roles and semantic attributes to facilitate human representation of their construction knowledge. 2. Background The term ontology originates in philosophy and relates to describing the world as seen by a group of people at certain time according to a school of thought that is based on a set of fundamental proposition or view of the world (normally called epistemology). As such and contrary to common belief, ontology is temporal and, in many cases, relative to the perspective of the developer, the scope of the ontology and, of course, the limitations imposed by epistemology. In informatics systems ontology is an attempt to formalize a language to describe knowledge in a certain domain. This is not limited to a vocabulary (taxonomy of terms) but also extends to describe relationships between concepts and also the articulation of rules (axioms) to control/explain the behavior of such concepts. Of course, this language is bound by the same limitations faced with philosophical ontologies if not more. It is important then to remember that the ontology is another “model for representation of knowledge”. Like any model, there is a guaranteed minimum level of mismatch between the ontology and reality. The specific roles of an ontology in a civil informatics system include facilitating high-level representation of knowledge (not just data), capturing the engineering semantics familiar to the end users; and adequate support for multiple views. “In the context of civil engineering, an ontology will thereby be described by defining a set of representational terms in the target domain. These definitions must adequately associate the names of entities in the universe of discourse (bearing structure, wall, column, beam, joint, support, spring ...) with human-readable text describing what these names mean, and formal axioms that constrain the interpretation and the use of the introduced terms. Thus, different from a product data model, an ontology is comprised of statements of a logical theory (and not only data specifications) about the targeted abstraction of the real world” [2]. Consequently, ontology scoping is one of the most important tasks upon the development of ontology. Trying to model a large domain of discourse could increase the level of mismatch. Equally important in this regard, is to realize that an ontology has to evolve over time through constant revisions that are based not only on theoretical conceptualization but also based on actual use. To that end, one of the most important rules in ontology development is to “minimize ontological commitment” [3]. In other words, less could be better than more–especially when it comes to axioms. The “representation” aspect of domain ontologies has major consequences on their use. Unlike AI, ontology does not aim to reason about domain knowledge. It is not an attempt to optimize or find a solution to a problem. Rather, it aims to understand the underlying principals of the concepts in the domain and their interrelationships. Recently, this has led ontology research to get closer and closer to linguistic, communication and cognitive sciences. In other words, ontology is an attempt to present human mental constructs in a computer-interpretable format. Informatics ontologies are divided into three major categories: upper, domain and application. Upper ontologies have the highest level of abstraction as they attempt to present a cross-domain, holistic view of things. Domain ontologies tend to focus on the fundamental conceptualization of the knowledge in a specific domain. While less abstract than upper ontologies, domain ontologies tend to focus/ incorporate substantial philosophical views and representations. This is mainly because they aim, mainly, at capturing the essence of the accumulated knowledge in a domain. Application ontologies are at the other end of the spectrum. They are basically a detailed extension of a domain ontology to a specific sub domain. They tend to focus on axiomization. To an extent, application ontologies are the bridge between the philosophical nature of a domain ontology and the computational aspects of software and AI tools. This categorization has been used in other ontology development efforts in the gas industry [4], and bioinformatics [5,6]. Finally, we have to emphasise here that ontology is more concerned with knowledge not information or just data. The cornerstone of ontology development, then, is a sound ontological model that is simple, yet deep and flexible enough to be able to capture the multifaceted nature of human cognition and communication. This is far more critical contribution than the number of terms in the taxonomy or the complexity of the axioms proposed. 3. Scope The ontology presented here is an attempt to formalize construction concepts in infrastructure products. It can be argued that three elements are required to model any domain: processes, actors and products. Products represent the final outcomes of processes that are controlled by actors. Infrastructure products (as will be explained later) cover physical products (such as a bridge, a traffic signal, an electric cable), a decision, a knowledge (abstract mental conceptualization) and knowledge items (the representation of abstract knowledge in usable format such as reports, drawings, or codes). While ontology and semantic systems are poised and assumed to be of great help in integrating knowledge flows, it is not feasible that one ontology can address the diversity of concepts in infrastructure domain. The ontology scope was limited to construction concepts. This ontology does not cover the urban planning, environmental assessment, social aspects, policy making, or business processes of infrastructure systems. Addressing all these domains in a single ontology is not possible (if we have to assure an adequate level of details). Consequently, this ontology is a step towards realizing formal semantic representation of infrastructure knowledge. Other interoperable and complementary ontologies have to be developed to achieve this objective. Infrastructure in this ontology is limited to urban civil infrastructure. This means that irrigation infrastructure (e.g. dams, locks and canals) and facilities (e.g. schools or hospitals) are out of the scope of this ontology. The ontology covers municipal utilities: electric, telecommunication, gas, water, and wastewater along with urban street and highway elements. To this end, this ontology started by incorporating the highway engineering ontology proposed by ElDiraby and Kashif [6]. The ontology proposed in the paper is intended to serve as a component in a much larger knowledge-enabled framework for urban infrastructure management. Related work has been undertaken to develop an ontology for processes [7] and actors [8]. All ontologies share a common upper-level model to enable interoperability between systems that subscribe to the ontological model. Given that a multitude of actors are involved in the domain of infrastructure, any model for knowledge must make assumptions with respect to the perspective it takes on modeling. This assumption is imperative to the modeling effort. For example, the attributes of an ‘electricity meter’ from an operator's perspective are completely different than those of a user. Similarly, infrastructure products could have different meanings to a civil engineer and to an accountant (who is evaluating their values). As such, this ontology assumes the perspective of a civil engineer, particularly an infrastructure designer. 4. Related works Historically, initiatives to model infrastructure products have generally fell behind their counterparts in other industries and even with the AEC industry [9]. Software vendors, mainly in the GIS market, have developed specific spatial data models that focus on providing vertical integration within each industry and can be used as templates for implementing GIS projects (ArcGIS [10]). Other efforts for product data modeling were conducted by industrial coalitions or academics. The following section presents a summary of product model representation standards. SDSFIE (Spatial Data Standards and Facility Management Standards for Facilities, Infrastructure, and Environment): This standard was developed by the U.S. Army CADD/GIS Technology Center. The standard was created in response to the vast amount of geospatial and location-based data related to infrastructure and environmental features that were represented in an array of different computer systems [11]. SDSFIE/FMSFIE relies on a relational database model for managing spatial and attribute data, consistent with the data models employed by most GIS/CAD vendors. The SDSFIE standard represents 967 different entity types grouped under 28 different entity sets. LandXML is based on aecXML schemas, which can be considered to be an ‘organized briefcase’, storing packets of information required for a business transaction which are to be transferred or published via the Internet. The infrastructure working group published the LandXML schema that focuses on the exchange of data created during the Land Planning, Civil Engineering and Land Survey process [12]. The LandXML schema has been primarily driven by the needs of the land development industry. Some of the entities that are represented in the schema (in decreasing order of detail) include parcels, roads, monuments, pipes, bridges and retaining walls. The modeling paradigm is an object-attribute model consistent with the XML representation. MultiSpeak is a specification for the exchange of data among software applications commonly applied in small electric utilities. The MultiSpeak Initiative is a collaborative effort of the National Rural Electric Cooperative Association (NRECA) along with over 120 software providers and consultants that serve electric utilities. Applications involved include software to support meter reading, call handling, outage analysis, outage detection and customer billing only to mention a few. The MultiSpeak specification is intended to be used by software providers to write interfaces that will enable the interchange of information with other software that supports MultiSpeak. Software that is MultiSpeak compliant marshals the required information from their native data structure, converts those data into extensible markup language (XML), and sends those packets in the form of a predefined message that is later unbundled by the receiving application [13]. Quiroga & Pina [14] proposed a utility data model that represents both surface and subsurface utilities present in a highway ROW. Their model represents traditional subsurface utilities like ‘Gravity pipeline’, ‘Forced pipeline’, ‘Culvert’ and ‘Manhole’ as well as surface utilities like ‘Cable’, ‘Pole’, ‘Tower’ and ‘Street light”. The scope of their model encompasses those utility elements that directly interact with highways. As such, no reference is made to entities like pumping stations, treatment plants, purification plants and electric stations. Halfawy [15] proposed a STEP-based information model that was created to represent four main views that support the bridge information model namely, structural design, structural analysis, construction scheduling, and cost estimating. Their model represents objects of bridge superstructure, substructure and site. The superstructure not only includes reference to the main structural components of bridges like ‘deck’, ‘span’, ‘beam’ and ‘structural form’, secondary objects like ‘parapet’, ‘roadway surface’ and ‘drainage’ are also included. Substructure bridge objects like ‘abutment’, ‘pier’ and ‘foundation’ are included in the model. Reference is made between the bridge object and the site where it is located. The site includes objects like ‘location’, ‘grid reference’ and ‘soil conditions’. Finally, El-Diraby & Kashif [6] proposed an ontology for representing concepts relating to highway construction covering concepts that extended beyond products to include resources, actors, projects, processes, systems, and technical topics. In total around 1800 concepts/terms were defined within the ontology. Out of these, only 84 products related to highways were defined. The aforementioned research efforts had some of the following limitations: 1) The focus on data exchange standards rather than knowledge representation. This can be attributed in part to the interest to enhance CAD-based design software rather than develop knowledge management systems. 2) The focus on single-domain product models where requirements address a specific area (electric utilities, bridges, highways, etc.). This has subsequently limited the interoperability of software that subscribe to these standards. As such, this research aims to develop a domain-independent knowledge model that will overcome these shortcomings. 5. Methodology Research initiatives that focused on methodologies or methods for building ontologies started as early as the 1990's (Lenat & Guha [16], Uschold & King [17], Grüninger & Fox [18], Fernandez-Lopez et al. [19], Gómez-Pérez et al. [20]). The methodology used in this research draws mainly on METHONTOLOGY (Fernandez-Lopez et al. [19]), and the approach by Grüninger & Fox [18]. According to Gruninger & Fox [18] and based on a motivating scenario or a requirements statement, a set of competency questions are set forth that the ontology must address. An ontology must be able to represent these questions using its terminology, and be able to characterize the answers to these questions using the axioms and definitions. The questions serve as constraints on what the ontology can be, rather than determining a particular design with its corresponding ontological commitments. The proposed approach attempts to capitalize on the following strengths: 1. The identification of motivating scenarios for the Ontology [18]. 2. The use of competency questions to drive the ontology building exercise: [18]. 3. The re-use of the competency questions as a validation tool to test the coverage of the ontology against the requirements set forth by the competency questions: [18]. 4. The level of detail in describing the methods, techniques of ontology development [19]. 5. The identification of life-cycle of ontology through a series of evolving prototypes [19]. 6. The middle-out approach for identifying concepts: [18,19]. The development of the ontology included the following steps which overlapped and were conducted in an iterative manner: Requirement analysis: In addition to its vital role in scoping the ontology, requirement analysis serves as the basis for establishing competency questions. This phase of the research benchmarked similar requirement analysis for aforementioned product data models. Benchmarking: The aforementioned models (and other relevant models such as Industry Foundation Classes) were analyzed to identify the strengths and weaknesses; distill lessons learned; investigate the possibility of reuse. Design guidelines: Review of design guidelines that define the constraints and requirements for buried infrastructure was conducted. Knowledge from these guidelines represents explicit constraints. Case studies: Eight major infrastructure projects in Southern Ontario were investigated. Seven of these projects involved municipal infrastructure. The eighth involved a highway interchange reconstruction. The cases were instrumental in identifying some competency questions; identify some of the main attributes; and better understand the main issues facing designers during coordination. Industry meetings: In order to consolidate the issues faced by practitioners and to get exposed to the tacit knowledge, the second author attended regular meetings of two organizations over two years: the Ontario Regional Common Ground Alliance (ORCGA) and the Toronto Public Utility Coordination Committee (TPUCC). ORCGA is a consortium of owners, designers, excavators and utility locators that have a mandate of preventing the damage to underground utilities. TPUCC is a committee that includes all organizations that own or operate public utilities within the City of Toronto. Evaluation: four tools were used in evaluating the consistency and relevancy of the ontology: using OWL reasoners, answering competency questions, interviews of domain experts, and the development of two application ontologies to test the extendibility and reusability of the ontology. 6. The ontological model In the Infrastructure Product Ontology (IPD-Onto), entities are used to represent the core concepts within the domain of utility infrastructure. The five main entities used are Project, Process, Actor, Resource and Product. In addition to these five entity classes, three additional concepts are used to support the definition of entities. These concepts include Attributes, Constraints and Mechanisms. Finally, the concept of Modality is used to describe the various dimensions of these entities. These concepts are prevalent in system analysis and manufacturing systems [17,21]. This ontological model (shown in Fig. 1) follows the same structure as that proposed by El-Diraby et al. [22]. The proposed ontology assumes that a Project is composed of a set of Processes, where a set of Actors are involved to control/use Resources in order to produce Products. Products, being the central theme of this ontology were further broken down into four distinct groups: Knowledge: An abstract entity that describes tacit knowledge or the fundamental understandings which has been acquired by experience or study [23]. Knowledge is considered an intangible product. For example a bridge engineer is expected to possess knowledge of bridge construction principles in abstract form in addition to formal forms (the ability to master the design equations, for example). Knowledge item: A physical or symbolic manifestation of knowledge that is represented in a tangible form. For example knowledge of bridge construction procedure can be manifested in a Construction Method Statement and a Construction Schedule Report, or a design drawing. This is basically the explicit representation of tacit and formal knowledge. Physical product: Refers to tangible products that are usually the output of a construction or manufacturing process. It includes basic products (such as a beam, pipe, column, or footing) and complex products (such as a house, bridge, highway, or water distribution system). Decision: Some processes do not lead to the creation of a physical product but rather to the selection of a specific course of action. For example, the bid evaluation process leads to a selection of bid winner. In summary, the linkage between processes and products is such that a Process produces Knowledge that can be manifested in a Knowledge Item. Some Process produce Physical Products while others lead to a Decision. In all cases, Processes lead to generation or update of tacit knowledge. 6.1. Attributes Attributes are dedicated to the description of Entity features. Attributes can be indigenous features (like the diameter of a pipe, the porosity of soil) or exogenous/assigned (like the budget of a product, level of performance of a pipe, schedule of a project, or objectives of a process). 6.2. Constraints Constraints represents an umbrella for all 1) Laws, 2) Codes, 3) Specifications (including Owner specifications, Manufacturer specifications), 4) User requirements, 5) Conditions, including Physical conditions (such as Site Layout, Topography, and Weather), Logical conditions (such as Temporal restrictions) and 6) Other controls such as cultural restrictions and environmental requirements. 6.3. Mechanisms Mechanisms represent an umbrella concept for all tools and concepts that support the work of actors in all processes. They are used to support the definition of entities. This is similar to the modeling paradigm adopted in IDEF0. Mechanisms are further subdivided into: guides, methods, and measures. Guides: every aspect of work includes a set of guides that support actors in their work. Guides include: Theories: such as the theory of structures and the theories of hydraulics. Algorithms: such as scheduling algorithms and resource allocation algorithms. Principals: such as the least energy principle. Strategies: where formal theoretical representations are not available, strategies represent the fundamental mechanism that supports the handling of our work. As an example, one can think of the different impacts of adopting a sustainable strategy vs. traditional strategy. Best practice: At lower levels of our work, best practices are one important tool in forming a model of our work entities. Methods: specify a technique or way in which a process is executed. For example, a pipe installation process could be executed using traditional trenching or using directional drilling. Measures: The need for this concept stems from the requirement to measure the conformance of an entity's attribute to a pre-defined requirement. Measures include conducting tests (for physical entities) and using metrics (for the logical and tacit entities). 6.4. Modality Most concepts in a subjective domain of knowledge such as infrastructure design are multimodal. For example an infrastructure product can be classified based on its function (functional modality), its position within the infrastructure network hierarchy (hierarchical modality), and the infrastructure sector it belongs to (sector modality). Adopting a functional classification, products types can include ‘Access Products’ (provide access to other infrastructure), ‘Control Products’ (control the flow of the medium within a product), and ‘Measuring Products’ (measure an attribute of a product). Similarly from a network hierarchy standpoint, products types can include ‘Main products’ (linear carriers of the medium being supplied), ‘Supporting Structures’ (that provide auxiliary roles) and ‘Devices’. The traditional consequence of such multidimensionality is a huge top-level taxonomy where all these dimensions are listed in the first layer, and then combinations of these dimensions are created in subsequent layers. For example, we could have “electricity product”, “conveyance product”, and “superstructure product” in the first layer. Then, in the second layer, we will find combinations of these. For example, “Electricity conveyance product” and “electricity Supportstructure Product”. Then in the third layer, we find “Electricity conveyance Supportstructure product”. Then the “real” concepts (in this case, cable) will be pushed down to the fourth layer. Instead of this very rational and hierarchical structure, this ontology adopted a balanced approach in managing concept inheritance (Taivalsaari [19]). The ontology uses a multi-dimensional representation. The main dimension (called basic modality) is an easy to understand, flat and straight forward representation of concepts as “commonly used” by the industry. Orthogonal to this, additional dimensions are used to describe other relevant modalities of the concept (see Fig. 2). It should be noted, however, that this is more of a matter of view-ability. The concept is still described through all modalities in the OWL file, but is set to the basic modality as default when it is viewed. 6.5. The taxonomy The ontology presents physical products in two fundamental categories: generic and sector-specific products. A generic product is a product that is not unique to any specific utility sector (for example a generic pipe, manhole, or valve). In contrast, a sector-specific product can be considered a manifestation of a generic product in a particular utility sector (for example a gas pipe, wastewater manhole, or a wate valve). Four orthogonal modalities are further used to describe the concepts Hierarchical role, Sector, Function, and Composition (see Fig. 3). 6.5.1. Sector This dimension classify infrastructure utilities into one of five main sectors; gas, electricity, telecommunication, wastewater, and water. 6.5.2. Hierarchical role This dimension is intended to classify a generic infrastructure product based on the role it plays within the infrastructure network. A generic product is assumed to be either a main structure, supporting structure or device. Main structures are the basic linear carriers of the medium being conveyed by the infrastructure carrier (e.g. lines, pipes, or cables). Supporting structures and devices provide auxiliary roles in the network to main structures (e.g. manholes, pedestals, or fittings). Devices are mostly mechanical or electronic objects that are designed to fulfill a specific purpose whereas supporting structures are non-mechanical/electronic objects that are used to maintain the main structures (e.g. valves, meters, or sensors). 6.5.3. Function This group classifies infrastructure products based on the functions each product performs. In this regard, eight functional categories have been identified: Conveyance products: Act as the direct carriers of the medium being supplied by infrastructure systems. Examples include gas lines, water lines, electricity cables, and fiber optic cables. Control products: Regulates the medium being supplied by the infrastructure system. Examples include gas valves, electric switches, and water control valves. Protection products: Provide protection to other infrastructure products or to entities in the vicinity of the product. Examples include cathodic protections devices, concrete duct banks, electricity ground points, relief valves, and manhole covers. Access products: Provide access to other infrastructure products. Examples include various types of junction boxes, manholes, and chambers. Measuring products: Perform some sort of attribute measurement to the infrastructure product itself (e.g. corrosion sensors) or to the medium being carried by the product (e.g. electricity meters, flow measuring devices, pressure gauges, and thermometers). Storage products: Accumulate the medium being supplied by the infrastructure product. Examples include water tanks, capacitors, batteries, and gas storage facilities. Locating products: Identify the location of buried infrastructure. Examples include markers and tracer wires. 6.5.4. Composition This dimension is intended to capture the notion of aggregation and composition between products. The relationship between the three levels of products is a composed-of relationship. Component products: These products represent the lowest level of infrastructure product aggregation. In essence, these products cannot be decomposed into any further products that lie within this taxonomy. Examples include individual pipe segments, fittings, meters, manhole covers, pumps, and electricity cables. Sub-system products: This group encompasses all intermediatelevel products that do not belong to the aforementioned groups. Examples include, water lines (that are composed of pipes, fittings, and valves), electricity lines (that are composed of cables and couplings), and pump chambers (that are composed of pumps, fittings, meters, and valves). System products: This group is used to represent the top level of infrastructure product aggregation and include most high-order network entities. Examples include water distribution systems, storm water collection systems, electricity substations, and gas distribution systems. An example of how some infrastructure products fit these proposed modalities is presented in Fig. 3. The figure includes two main parts; the upper part shows the generic product model, the lower part shows the sector-based product model. The upper part illustrates how the generic products are modeled in two dimensions: “Hierarchical role” and “function”. The lower part includes two dimensions: the sector and composition. The relationship between the generic and sector specific products is shown by the arrows; a Gas Valve is–a Valve and a Wastewater Manhole is–a Manhole. Similarly, subsumption relationships between compositional products are shown. For example a Gas service line is-composed of Gas Valves, Gas Pipes and Gas Fittings. 6.5.5. Infrastructure product attributes Attributes are some of the most important concepts that are needed to describe infrastructure products. The model for infrastructure product attributes identifies two distinct dimensions of attributes; attribute types and modalities. Attribute types are clustered into 11 main groups (dimension, spatial, material, shape, cost, performance, soil, dependency, redundancy, state, and impact). Orthogonal to these 11 basic groups, additional modalities of attributes were generated (See Fig. 4). Six main modalities for attributes were found to be of relevance. Physical modality: Physical attributes are usually tangible (e.g. diameter, width, material, etc.) whereas non-physical attributes include things like cost, performance, dependency, etc. Change modality: Fixed attributes are those attributes that do not change throughout the lifecycle of a product (e.g. shape, diameter, and material). On the other hand, changeable attributes assume various values throughout the product's life-cycle (e.g. performance, state of operation, and cost). Phase modality: Used to identify the specific project life-cycle phases when a particular attribute is of relevance for example, design attributes vs. construction attributes. Perception modality: Objective attributes have values that are usually not contested. In contrast, subjective attributes that can have more than one value depending on the point of view of the actor. Domain modality: Attributes can belong to a certain domain of discourse. For example, “environmental impact” belongs to the sustainability domain, whereas “soil conditions” belongs, mainly, to the engineering domain. Of course, some attributes can belong to more than one domain. Composition modality: This modality classifies how the values of attributes are inherited from component to sub-system products. In this regard, three compositional categories exist: Unique attributes: These attributes are not inherited upwards in an aggregation relationship. For example, performance and location are not inheritable as the depth of a water pipe cannot be inherited upwards (by a water line) to indicate the depth of the entire water line. Inheritable attributes: These attributes are inherited upwards in an aggregation relationship. Examples include surrounding soil attributes and ownership attributes. For example, if a segment of a water pipe is surrounded by a sandy soil, then it can be inferred that the water line it belongs to is also surrounded by a sandy soil. Compositional attributes: These attributes are combined together in an aggregation relationship. Examples include cost and length for pipes. For example the cost of a transformer station is the sum of the cost of its constituent products and the length of a sewer line is the sum of the lengths of the pipes it is made up of. Dimensional attributes: Relate to describing a measurable dimension of a product. Examples include height, length, width, diameter, etc. Spatial attributes: Relate to describing a location-related attribute of a product. Examples include depth, longitude, latitude, x-coordinate, y-coordinate, etc. Material attributes: Relate to describing one or more attributes of the infrastructure product material. Examples include material type, strength, ductility, conductivity, etc. Shape attributes: Relate to describing the shape of an infrastructure product. Examples include cross-sectional shape and longitudinal shape. Cost attributes: Relate to the monetary value associated with a product. Two orthogonal views of costs are assumed in the ontology. The life-cycle cost view groups the costs into categories based on their life-cycle stage (e.g. design costs, construction costs, operating costs, etc.). The other view on costs examines the types of costs that are incurred during any particular life cycle stage (e.g. labor costs, equipment costs, overhead costs, etc.). Performance attributes: Describe the various engineering, operational, safety and sustainability performance a product. Performance attributes are measured using Performance Measures. Surrounding soil attributes: The interaction between buried infrastructure products and their surrounding environment is of utmost importance. These attributes are used to describe the various soil-related properties (soil reactivity, permeability, strength, etc.), soil types (the ontology adopts the UCS classification system) and ground water properties of surrounding soils. Dependency attributes: Describe the various interdependencies between infrastructure products and other products using four main groups of dependency (physical, cyber, geographic, and logical). Redundancy attributes: Used to describe the need for the particular infrastructure product within the greater infrastructure network in case a breakdown of that product occurs. State of operation attributes: Used to describe the state of operation of an infrastructure product at a particular state in time. Impact attributes: Describes the environmental, economic and social impacts of products. 6.5.6. Mechanism The ontology describes the attributes of infrastructure products via a measurement centric approach. The model assumes that any infrastructure product attribute can be identified or measured through a Measure (e.g. a construction safety index acts as a measure for construction site safety, or design efficiency index acts as measure for design efficiency). A measure is composed of one or more Indicators that in turn describes or quantifies the measure. (e.g. Lost time injuries act as an indicator for site safety index). Two main types of indicators exist depending on the attribute being measured. Indicators that assess physical product attributes are usually the result of a test. For example, concrete slump is an indicator for concrete workability and is a result of the slump test. Indicators that measure non-physical attributes are usually the result of some metric development process. For example, design schedule delay and document release commitment are indicators for design efficiency. The ontology further distinguishes between the test/metric as a mechanism, and the specific method for performing the test/metric. For example slump testing can be performed via various procedures depending on the specifying code (ASTM C143 in the U.S. and EN12350-2 in Europe). Test/Metric methods are grouped under a parent concept of Data Collection Method. The result of the test or metric (e.g. a slump of 4 inches or a 2 lost time injuries per month) is the indicator that makes up a measure that in turn describes an attribute. Guided by the competency questions, the ontology describes data collection methods as having five main attributes (see Fig. 5). Measurement unit: Describes the unit being used to qualify an indicator (e.g. centimeters, psi, number of cracks, etc.). Usability: Describes how easy-to-use the measure is. Cost: Describes the costs associated with performing the test or metric. Accuracy: Describes how reliable or accurate the collection method is. This will depend to a great extent on the measurement resource used. Quickness: Describes how quick the collection method is in identifying the indicator. 6.5.7. Constraints The generic model for constraints in IPD-Onto assumes that a Constraint will constrain or limit an infrastructure product or process. Constraints are articulated in or specified through Codes, Specifications or Know-how. Design Codes are usually drafted by professional organizations with the purpose of standardizing the design process within a particular discipline. Specifications are usually drafted by project proponents or their agents (usually designers). Their purpose is to specify the required attributes of products being designed. Finally know-how refers to an often ‘loose’ collection of tacit knowledge that designers accumulate from experience. Collectively, codes, specifications and knowhow represent the manifestation of constraints that impact the infrastructure design and construction process. The underlying purpose of constraints is to limit the Impacts caused by infrastructure products and processes. The impact produced by an infrastructure product is due to the interaction between the attributes of the product and the conditions of the surrounding area where it will be located. As such the ontology assumes that IPD Attributes and Area Conditions produce an Impact. Area conditions include things like soil conditions, traffic conditions, weather conditions, geospatial conditions, and existing infrastructure. These surrounding conditions combined with intrinsic attributes of the product itself (e.g. shape attributes, material attributes, flow attributes, etc.) produce an Impact. Take for example a constraint that limits the use of some material types in highly corrosive soils. This constraint is triggered by a situation that involves a pipe material that is susceptible to corrosion (an intrinsic attribute) and a corrosive soil (a surrounding condition that constrains the placement of the pipe in this location). Another example is minimum clearance constraints between utilities. This constraint is triggered by a situation that involves an infrastructure product with a particular location (an intrinsic attribute) and its surrounding existing infrastructure (a surrounding condition). Constraints can be classified according to their level of limitation as being either Hard Constraints, Soft Constraints or Advisory Constraints. This classification is similar to that adopted in IDEF9 for business constraints [24]. A Hard Constraint refers to a constraint that must always be followed regardless of the circumstances. For example, a pavement moratorium is a constraint on construction that cannot be overturned until the moratorium expires. A less limiting constraint is termed a Soft Constraint and refers to a constraint that must be followed but can be relaxed in case a special measure is performed. For example a municipal bylaw might stipulate that the use of heavy equipment is prohibited in residential areas unless a proper sound barrier is maintained around construction. The last grade of constraint is termed an Advisory Constraint and refers to constraints that should be followed if possible but can be relaxed at the discretion of the decision-maker. An example would be a best practice document that recommends the use of subsurface utility investigation prior to the design of a buried urban infrastructure project. Constraints are always related to one or more set of Decision Criteria. These criteria represent the underlying purpose of a constraint. Decision criteria include issues like minimizing traffic disruption, minimizing impacts on businesses, enhancing air quality, etc. These decision criteria are shaped by Policies that are drafted by policy makers reflecting the values of communities. Policies are related to the overall goals envisioned by stakeholders. Within the domain of infrastructure design, three goals are of significance. These include Infrastructure Sustainability, Public Safety, and Life-cycle cost reduction (Fig. 6). 6.6. Relationships Relationships aim to describe the associations that occur between concepts in the Ontology. This linkage of concepts enriches their definition and clarifies their context. Relationships that were found to be most relevant to the ontology were organized in a small taxonomy. These meta-relationships aim to classify the most common relationships into meaningful groupings. Hyponymy (the ‘is–a’ relationship): This category includes relationships that define the kind or type of a concept. These concepts are considered the backbone of building taxonomy trees. Some variations of the ‘is–a’ relationship include: Synonymy relations: Used to indicate relative similarities between concepts. In this regard two main relationships are used: “isEquivalentTo” indicates complete synonymy between concepts. “isSimilarTo” indicates partial synonymy between concepts. Antonymy relations: Used to define the category or type of a concept by excluding it from a certain domain (or group of concepts). In this regard two main relationships are used: “isDisjoint” indicates that two concepts are completely different from each other. Concepts that are disjoint cannot have a similar instance. For example the shape attribute “Square” is disjoint with “Circular”. “isOpposite” indicates that concept A is the complete opposite of concept B. For example the attribute “Under” is the opposite of “Over”. Meronymy (whole-part relationships): Aim to augment concepts into groupings or decompose concepts in parts. The basic relationship found under this meta-relation is the “isComposedof” relationship. Descriptive (cross-tree relationships): Used to describe the behavioral context of concepts by linking them to other concepts within the taxonomy. Six main groupings were found to be of relevance: Attributive: Used to identify the key attributes of infrastructure products. This is accomplished through a generic ‘has’ relationship that is further decomposed into more specific attributive relationships (e.g. hasDiameter, hasMaterialType, etc.). Contingency: Used to identify cause and effect relationships related to infrastructure products. The main relationship used is the ‘impacts’ relationship that is used to describe the impacts of infrastructure products on their surroundings and each other. Function: Used to relate infrastructure products to their key function(s). This is accomplished through the generic ‘hasFunction’ relationship. Conformance: Used to represent constraints that influence infrastructure products. This is accomplished through the ‘Constrains’ relationship. Temporal: Used to represent relationships to time-related concepts. In this regard four main types of temporal relationships are present; Occurrence (e.g. happens, occurs), Precedence (e.g. precedes, follows), Milestone (e.g. starts, ends), and Interruption (e.g. interrupts). Intentional: Used to specify the reason(s) for taking a certain action. This relationship is used to relate constraints to criteria for implementing a constraint. The ‘hasPurpose’ relationship is used to represent intentional relationships. 6.6.1. Fuzzy hedges of relationships Relationships play a vital role in the semantic characterization of concepts and their meaning. By linking concepts and defining what sort of link exists, they triangulate concepts and make their “meaning” more explicit (beyond the simple definition) (Table 1). To accurately reflect the variety of semantic linkages between concepts, relationships were viewed as fuzzy sets. This meant that some hedges can be applied to relationships to make them more accurate. For example, compositional relationships define how individual system or subsystem products are composed of one or more component-level products. In this regard, it was found that relying solely on one relationship would not suffice to describe the necessity (or lack thereof) of product composition. As such, two relationships are used to define product composition: must-be-composed-of: indicates that a product must be composed of another product in order for it to be properly defined. could-be-composed-of: indicated that a product might be composed of another product but it is not necessary for it to be properly defined. An example of the difference between the two relationships is illustrated by the subsystem level product Wastewater Line. A Wastewater Line must-be-composed-of at least one pipe, fitting and manhole, whereas is it could-be-composed-of pumps, grease traps, and backwater preventers. Further, based on the CQ, the ontology needed to map relationships between generic and sector products, a full-scale mapping was performed between all generic products and their sector counterparts. The modeling requirement for this mapping necessitated that a usually-is relationship be used for this mapping. This relationship indicates that a particular sector product will predominantly fall under this category of classification. The ‘is–a’ relationship is simply modeled using the ‘subclass of’ property in OWL. In order to model the ‘usually-is’ relationship, an object property is defined in OWL. For example, the following OWL abstract syntax specifies that a gas transmission main is usually a metallic pipe: Class(GasTransmissionMain partial restriction(usually_is someValuesFrom(MetallicPipe)) 6.7. Axioms Unlike pure AI approaches, axioms are needed to specify concept behavior not to reason about problem solving or optimization. The following are some examples of axioms used in the ontology. 6.7.1. Attribute control These axioms control how unique, inheritable and compositional attributes affect the value of the attributes themselves. Inheritable attributes: “If Attr is an inheritable attribute of product B that has a value of val, and Attr is a inheritable attribute of A, then the value of attribute Attr for product A is val.” nheritable attributes include material type, diameter, use, etc. So, unless otherwise stated in any application built on top of this ontology, if a water pipe is 200 mm steel pipe that is in use then it can be concluded that the water line that is composed of this pipe is a 200 mm steel water line that is in use. Compositional attributes (generic): “If Attr is a compositional attribute of product A,B and C and the value of Attr for product B is val1 and the value of Attr for product C is val2 then the value of Attr for product A is a function of (val1, val2).” Compositional attributes include performance, redundancy and dependency. For example the performance of an entire sewer line if a function of the performance of its constituent pipes. Compositional attributes (additive): “If Attr is a compositional attribute of product A, B and C and the value of Attr for product B is val1 and the value of Attr for product C is val2 then the value of Attr for product A is val1 + val2” Compositional additive attributes include cost and length. For example the total length (or cost) of a gas line is the summation of the lengths or costs) of its constituent pipes. Unique attributes: “If Attr is a unique attribute of product B that has a value of val, and Attr is a unique attribute of A, then nothing can be concluded about the value of attribute Attr for product A. An example of a unique attribute is soil characteristics. Knowledge of the soil type surrounding a pipe segment does not allow the inference of soil type for the pipe line as a whole or other adjacent pipe segments. 6.7.2. Infrastructure product cost axioms The following axioms aim at describing how individual cost types and life cycle costs are related. “The total cost of a product is the sum of all the Cost Types of that product”. Examples of cost types include direct cost, indirect cost, overhead cost, labor cost, etc. “The total life cycle cost of a product is the sum of total cost of the product over its various life cycle stages.” Examples of life cycle costs include design costs, construction costs, operations costs, rehabilitation costs and decommissioning costs. 7. Ontology evaluation One of the main challenges in evaluating informatics ontologies is their name. Many associate ontology with universal standard of knowledge (as was the typical case with philosophical ontologies). Universality should not be the objective of domain informatics ontologies. This is almost un-achievable given the typical difficulty of building consensus on knowledge representation (which is subjective by its very nature). Even upper ontologies (which by their name have a claim to universality) could not reach the stage where there is consensus about them. Consequently, the development of widely-accepted ontologies is an evolutionary problem. i.e. such ontologies have to be used, criticized and updated by researchers over many years. This is true in philosophy and in informatics. According to Grunninger and Fox [18], an informatics ontology is one of many possible knowledge model. To evaluate such models, researchers need to establish a set of measures (competency questions) before the model development and then test the achievement of these benchmarks after the development. According to them, answering CQ is sufficient means to evaluate the ontology. However, the CQ approach tends to be inspired by AI methodology. Given the increasing role of ontologies in semantic web technology, we believe that ontology has to be sensitive to the linguistic and communication aspects too. In addition to CQ, the following tools were used to test the consistency of the ontology, examine the relevance of the terms used, and test the usability of the ontology in supporting the development of other applicationlevel ontologies. 7.1. Protégé consistency checking The ontology was represented in OWL using the Protégé Ontology Editor (Protégé, 2006). Reasoning is performed in Protégé by utilizing a Description Logic Reasoner. Several description logic reasoning tools (such as Racer, Pellet, and FACT++ ) can perform intelligent reasoning on OWL ontologies. This helps in assessing the overall consistency of the ontology. Racer was used to evaluate IPD-Onto due to its relatively easy interface with Protégé. Racer provides the following services for OWL ontologies and RDF data descriptions: Check the consistency of an OWL ontology and a set of data descriptions. Find implicit subclass relationships induced by the declaration in the ontology. 7.2. Interview with Domain Experts To assess the relevance of the terms used, seven industry experts were interviewed. Interviewees are selected based on the following criteria (see Table 2); 1) Years of experience within a particular sector of utility infrastructure, 2) Thorough knowledge of utility design issues, and, 3) Familiarity with issues associated with design coordination among various utilities. In some instances, interviews were attended by a small panel of experts (2 to 4 experts at most) that possessed the required experience. It should be noted that the aim of these interviews is not to prove the universality of the ontology (if this is ever possible). Rather, to test the adequacy of the semantics and the ease of use of the ontology. On average each interview took 80 min to complete. The experts were briefed for 30 min about the sources of gathering different concepts and how they are structured to form hierarchies. The survey included 12 questions that were designed to serve the following objectives: Navigational ease through locating concepts: Navigational ease is important to ensure knowledge access, retrieval, re-use, and maintenance. An easy-to navigate taxonomy means that it is not too difficult to locate concepts in the hierarchy structure. Experts were asked to find 10 concepts in the taxonomy. Definitions of those concepts were given in the questionnaire to avoid any ambiguity. A six-point scale was used to record the experts’ responses, with 1 being the easiest navigation and 6 being the most difficult navigation. Categorizing concepts: Respondents were asked to rate their consensus with 10 concepts with respect to the classification in the ontology. The respondent was subsequently presented with another set of 10 concepts. These concepts were intentionally removed from the ontology. The expert was then asked to categorize these concepts according to the ontological model using the same 6-point scale. Overall assessment: finally, as the experts became fully aware of the ontology and the conflicting needs of categorization, they were asked to make a general assessment about the taxonomy. Interviewees provided their feedback to these questions on a sixpoint scale (with 1 being the most favorable in each case). Table 3, shows the results of the assessment for navigational ease. The ease of finding concepts varied from 1.89 (for finding Infrastructure Products) to 2.57 (for finding Product Attributes) to 3.10 (for finding Other Concepts). This is expected due to the product-centric nature of the ontology (note, lower scores indicate easier navigation assessment). Using the standard deviation (SD) we could assess the level of agreement by experts: Products (SD = 0.79), Other Concepts (SD = 1.79), Attributes (SD = 1.12). Categorization effectiveness (see Table 4) decreased from Infrastructure Products (Score 2.18) to Product Attributes (Score 2.46) to Other Concepts (Score 2.54). This is similar to responses about navigational ease. Agreement among respondents on concept navigational ease also varied. Overall respondents tended to have more agreements on Products (SD = 0.86), followed by Attributes (SD = 0.92), followed by Other Concepts (SD = 0.96). The overall standard deviation for concept categorization was less than its counterpart for navigational ease. This can be explained by the difficulties experienced by some of the respondents in using the Protégé software to navigate the ontology (mainly due to lack of familiarity). Section 5 of the interview asked respondents to provide an overall evaluation of the ontology. Results of this section are summarized in Table 5. 7.3. Application ontologies One of the key requirements of a domain-ontology is to be a generalized knowledge representation of a domain [3]. In order to verify the claim that the domain-ontology is generic in nature and not influenced by the requirements of the application-level, two verification application-ontologies were created. The first verification ontology was created to represent routing problems and conflicts. The second was created to represent knowledge pertaining to trenchless construction method selection. The two ontologies were created based solely on the concepts defined in the domain-level ontology. Through collaboration of few industry experts, relevant concept lists were developed and then categorized into the domain ontology main concepts. This showed how the ontology can adapt to the sub domain level needs. The Trenchless Construction Method Ontology (TCM-Onto) is an application ontology intended to encapsulate the knowledge needed to select the most suitable trenchless construction method for a pipe based on its attributes and that of its surroundings. TCM-Onto is composed of 35 concepts and 16 relationships. The ontology is populated by 12 instances of proven trenchless construction methods and 14 instances of impacts that link pipe and TCM attributes. Of the 35 concepts defined in TCM-Onto, 29 concepts were created based on concepts already defined in IPD-Onto. Table 6 lists these concepts. Many concepts requiring definition in TCM-Onto were either already defined (e.g. Cost), or were specified as a child of a parent concept (e.g. ExpectedAccuracy is a sub-concept of Accuracy). TCM-Onto required the definition of four concepts and relationships that were not found to be directly related to a concrete concept in IPD-Onto. These concepts all related to TCM attributes and included: 1) Surface subsidence, 2) Maneuverability, 3) Obstacle detection, 4) Spoil removal requirements. These concepts were represented as sub-concepts of the abstract concept of ‘Attribute’ that is defined in the root ontology. In summary, of the 35 concepts and 12 relationships defined in TCM-Onto only 4 concepts and 4 relationships were defined as direct children of abstract concepts/relationships defined in the root ontology. The remaining 31 concepts and 12 relationships were defined as direct children of the concrete concepts defined in IPD-Onto. This demonstrates that IPD-Onto can serve as a robust domain ontology for representing applications that were not defined ‘a-priori’. This highlights an important trait of ontologies, that of knowledge reusability (Fig. 7). 8. Conclusions The domain ontology is considered a core model for productcentered knowledge in the five sectors of utility infrastructure. The core contributions of the ontology are two-fold. The first is shifting the focus from data exchange and product modeling to knowledge representation, sharing and reuse in the domain of infrastructure products. The ontology's generic framework (owed to the top-down modeling approach used), will enable knowledge to be seamlessly added without the need to re-create a model. The developed Ontology provides a conceptualization for knowledge in civil infrastructure. Application ontologies (and subsequent software systems) that subscribe to the ontology will need to add a minimal amount of information. Examples of possible application ontologies include ontologies relating to asset management, infrastructure deterioration modeling, and sustainable infrastructure design to name of a few. 2) Sector-independent representation of products and their related attributes. This will serve as an enabler of information and knowledge interoperability across utility infrastructure sectors. Most information/data modeling initiatives have, normally, focused on only one utility infrastructure sector. This is one of the main impediments to interoperability between various utilities. The ontology presented in this research is intended to bridge this interoperability gap. Possible applications that would serve from improved operability include coordinated design and construction of urban infrastructure, e-society applications in infrastructure decision-making, and coordinated disaster management of urban areas. 
A methodology for the semi-automatic creation of data-driven detailed business ontologies. In the context of technological expansion and development, companies feel the need to renew and optimize their information systems as they search for the best way to manage knowledge. Business ontologies within the semantic web are an excellent tool for managing knowledge within this space. The proposal in this article consists of a methodology for integrating information in companies. The application of this methodology results in the creation of a specific business ontology capable of semantic interoperability. The resulting ontology, developed from the information system of specific companies, represents the fundamental business concepts, thus making it a highly appropriate information integration tool. Its level of semantic expressivity improves on that of its own sources, and its solidity and consistency are guaranteed by means of checking by current reasoning tools. An ontology created in this way could drive the renewal processes of companies’ information systems. A comparison is also made with a number of well-known business ontologies, and similarities and differences are drawn, highlighting the difficulty in aligning general ontologies to specific ones, such as the one we present. 1. Introduction With the first phase of Web connectivity infrastructure consolidation over, electronic commerce is now immersed in a process of vast expansion. Many companies seek to justify investment in e-commerce, and they demand improved information integration processes. The need for process optimization is urgent. More information is required, and more rapidly, to gain better quality knowledge. That is, we are in a phase that requires closer integration of business information systems to make better use of the enormous quantities of data, by means of an improved and more accurate interpretation of those data. Undoubtedly, this will bring a stark improvement in companies’ knowledge generation capabilities ([1]). The work we will present in this paper consolidates years of experience in and around commercial enterprises, with a focus on databases, as well as on the analysis of how companies in different branches of industry use the Internet (design, electronic commerce, production, electronic administration, integration with clients, suppliers, employees, etc). In recent years, two facts or situations of great importance have become clear. One, companies increasingly want their information systems to perform better (e.g.: [2]). Two, these systems are prone to error, redundancies and semantic failures, which result in lower quality knowledge (e.g.: [3]). Company data are often scattered over different areas, formats and systems. Such data must be managed by means of processes that are more centralized and sophisticated to exploit their information more effectively and profitably ([4]). Therefore, a transition is needed towards systems that are more efficient in their knowledge management and which can better integrate data originating in multiple places. Many companies that previously used commercially available database management systems as standalone software applications have come to manage their information resources via their local networks or the Internet by means of shared environment Intranets, some even sharing information with other companies (associates, clients, suppliers) through Extranets. Today, companies want to grow towards more powerful database management systems able to manage information from the Web, within centralized environments and with the need to integrate along their supply chains as much as possible ([5,6]). It is also known that companies see information as a highly valuable asset. To keep this information from its various sources fresh and integrated is of the utmost importance today. Therefore, applications created especially for working in these environments are necessary, as is a data vision that is new and integrative. Likewise, companies need to optimize the internal management processes of their resources, which normally leads to changes in the structures of information and the applications used. This involves making databases, tables, attributes, restrictions, etc., compatible. The need for the integration and reorganization of data sources is also evident in the case of the adoption of solutions developed by third parties, for example, SAP1 or Navision2 ([7]). The main contribution of this paper is a methodology for the development of an ontology from a set of company databases to integrate information sources for the companies and to contribute to the logical treatment and strengthening of current databases. The resulting ontology meets both commercial and managerial needs. An ontology produced in this way has a series of characteristics that make it highly appropriate for solving current problems of homogenization and integration revealed by the Semantic Web initiative. The ontology contributes solidity to the renewal processes through which the company modernizes its information systems at the same time that it integrates its various sources into an information model that is coherent, consistent and shared with the other associated companies and with clients and suppliers. The paper is structured as follows. First, the article presents a background of the Semantic Web from a variety of perspectives, and the semantic interoperability needed for the integration of information between systems, as well as the role of ontology in this process. Then, Section 3 explains our methodology for constructing a business ontology based on existing database systems, and shows an example of a complete ontology developed following our method. Section 4 compares the resulting ontology with other well-known business ontologies and discusses the strengths and weaknesses of our methodology. Finally, present directions for future research and conclusions are presented. 2. Background 2.1. Starting considerations Today’s business seeks to optimize information resources by making the most of new technologies and languages rooted in HTML (e.g.: XML, eXtensible Markup Language; RDF, Resource Description Framework; and OWL, Ontology Web Language) to extract the most refined and accurate information possible from its systems. This information then acts as the basis for correct and effective decision-making, with minimal risk. Alongside the new technological environment, we see the unstoppable process of globalization; in business terms, this process is of vital importance. Companies undergo association and absorption processes, businesses jump across national and continental barriers. All these, together with greater profit demands, makes them very aware of the added value of the correct integration of information and its sharing with partners, shareholders, clients and suppliers [5,8]. The recent and promising research field of ontology is now of vital importance in terms of the business problems discussed above. Philosophical ontology informs a vital approach to philosophical inquiry by addressing the metaphysical aspects of the nature of existence, including meanings, relationships and instances of the abstract, the concrete, the general, and the specific. It describes the basic categories and relationships of existing beings or entities. These existing entities or beings encompass all objects, people, concepts, and ideas. In an applied sense, a business ontology models what we know about a domain or part of reality by means of concepts and relationships. The ontologies are represented by classes (categories), properties (roles) and class attributes. For a deeper understanding of the term ontology, and a general sense of the ontological aspect of information technology, the reader is referred to [9]. [10] Defines ontology as: ‘‘an explicit specification of conceptualization’’. Borst et al. [11] extend that definition as ‘‘a formal specification of a shared conceptualization’’. In addition to the original definition, two important notions are used – formal and shared. Formal – indicates that ontology should be specified in a standard way in order to be comprehended by a machine. Shared – denotes that the conceptualization is in consensus rather than relating to an individual scenario. Our goal is to produce a formal shared ontology. An operational definition of a formal shared ontology refers to a document or file that contains formal definitions of the concepts and relationships between them in a specific knowledge domain which can then be applied in a computational setting. A formal shared ontology contains a set of classes (relational taxonomy of concepts and roles) and a group of axioms that enable new knowledge to be deduced. 2.2. Semantic interoperability: the new challenge for firms’ 2.3. Previous projects on information integration – A short integration of information review. The time when a company’s ability to compete was  based exclusively on its individual strengths is over [12].  On the contrary, sectors are now made up of value  constellations that compete and cooperate among each  other in the form of coopetition [13].  Understanding between companies that collaborate  within the same specific value network is critical, as its  non-existence would lead to competitive inefficiency (see  [14]). The diversity of economic agents in the global  business environment extends beyond value systems and  is increasingly interconnected thanks to Internet and the  generalized use of e-business applications. This means that  firms must find a way to work with Information Systems  that use languages that are compatible, homogenous and  universal in meaning. This involves resolving interoper-  ability between systems and, specifically, semantic inter-  operability (see [5]). In fact today, the search for systems’  semantic interoperability is so crucial as to be a key aspect  of investigation and technical challenge (see [15]).  The problems of interoperability are not new, neither  for systems technicians nor for users. Totally solving  interoperability involves crossing three dimensions of  lesser to greater complexity: technical, syntactical and  semantic. Today, technical interoperability associated to  the systems’ capacity to exchange signals is solved. The  same cannot be said of the two other dimensions,  especially the semantic. The existence of syntactic inter-  operability supposes the capability of diverse systems or  software components to interpret the syntax of data with  the same form. This interoperability definitively enables  cooperation between systems even when they differ in  language, interface and execution platform [16]; i.e.  syntactic compatibility between systems. Nevertheless,  as we have pointed out, complete interoperability will not  exist until the semantics are guaranteed; that is, the  scenario in which semantic meanings are shared for each  terminology used [17]. Currently, semantic heterogeneity  is broad, which assumes that considerable effort is needed  to integrate information. So, the development of business  environments that enable the semantic management of  information is a necessity [5,7,18,19].  Diverse types of approaches are evident with regard to  the semantic information integration (see, as e.g., for more  detail: [20]): (1) the mapping-based approach. This is  based on correspondence or mapping from local data  sources onto a global diagram (see: [21,22]); (2) the  intermediary-based approach, based on the use of inter-  mediary mechanisms (mediators, agents, ontologies, etc.)  (see: [23–25]), and (3) the query-oriented approach,  based on interoperable languages (SQL3), logic-based  languages (see: [26,27]).  As is detailed in Section 3, we pose a hybrid approach  that, although based mainly on the use of ontologies, is  particularly interesting for the resolution of the problem  of semantic interoperability (see, as e.g.: [28–30]) 2.3. Previous projects on information integration – A short review. Many studies investigate the integration, mixing and articulation of information through ontologies. What follows is a brief description of the most important. The projects whose methodology most closely fits the work in this paper are those of Fensel et al. [31] and Schwartz and Schreiber [32]. This work documents the Corporate Ontology Grid (COG) project undertaken by Fiat (Italy), Unicorn (Israel) and LogicDIS (Greece), with the Institute of Computational Sciences of the University of Innsbruck (Austria) acting as consultant. The COG project investigated the problem of semantic heterogeneity among the data sources of the Fiat automobile company, and how it could be overcome by integrating these sources using a central information model, that is, an ontology. The problems of integration were solved by the application of the information semantic via a Unicorn Workbench tool. This tool was used to create an ontology based on schemas collected from the sources under study. These schemes were mapped onto an information model (ontology) to make the meaning of the concepts explicit and to relate them, thus creating an information architecture that made for a unified vision of the organization’s data  sources. The methodology presented here, although it  necessarily operates in the same environment and problem area, differs markedly and achieves superior results. Given the aforementioned proximity to COG, we carry out a more detailed treatment than the rest of the projects reviewed in this section. Although the detailed presentation of our methodology comes in Section 3, certain questions are anticipated. In what follows, we comment on some fundamental differences. Essentially, our approach describes a real case based on the information systems of companies that trade in varied energy-related products. The data sources we study are all MSAccess formatted, but we apply the information semantic as follows: Phase 1: We thoroughly analyse the metadata or schemas of the databases as well as the studies undertaken by technicians and experts of the company/companies in question. With the knowledge acquired, we set a partial map of concepts onto a newly created ontology. In this phase, we generate the hierarchy of classes, properties and restrictions from the semantic structure taken from the schemas and company rules regarding mapped sources. We call it partial mapping due to the fact that we had to reinterpret many of the schemas to give the ontology a logical and well organized structure. Phase 2: Contrary to the COG project, we fill the ontology with real database requests. We use our own ad-hoc created tool (GOWL). This tool consists fundamentally of an SQL query manager that produces code in the OWL format, containing the individuals as well as the relationships with other individuals via its properties and restrictions. During the construction and refining process, the ontology was checked by using logical reasoners in order to obtain a coherent and consistent information model.4 MOMIS (Mediator environment for Multiple Information  ́ Sources) [33], developed at the universities of Modena, Reggio Emilia and Milano, is a framework project that pursues the extraction and integration of information from structured and semi-structured sources. InfoSleuth [34] is a multi-agent system for semantic interoperability on heterogonous data sources. It supports the construction of complex ontologies from small ontologies. OBSERVER [35] is defined as a system based on ontologies perfected by relationships for the resolution of the heterogeneity of vocabulary. KRAFT (Knowledge Reuse & Fusion/Transformation), [36] was set up by the universities of Aberdeen, Cardiff and Liverpool in collaboration with British Telecom. Its main proposal is to research the possibility of sharing and reusing information contained in databases and heterogeneous knowledge systems. Chimaera [37,38] is a Web-based tool used to mix and diagnose ontologies. It was developed by Stanford University’s Knowledge Systems Laboratory (KSL). ONION (ONtology CompositION), [19,39] is a system centred on a secure formalism that allows the support of a working framework capable of scaling the interoperability of ontologies. ONION uses rules that cover the semantic voids by creating an articulation between systems. In Table 1, we show differences and similarities between our methodology and those mentioned previously with regard to the following comparative criteria: the integration paradigm, which indicates whether it is mixing, aligning or combining ontologies (for example, having a central ontology while the source ontologies are preserved and mapped by the central ontology); a mapping pattern, the correspondence between data sources and the ontology (one-to-one, single-shared, or ontology clustering), and depending on the paradigm chosen; mapping types supported, the different types of mapping that the methodology supports: classes, properties (relationships), queries (individuals), axioms, restrictions, logic rules, etc.; interoperability, concerning above all the import/export languages supported by the methodologies; and finally, the experience acquired via the use of the tools in different projects gives us an index of their usability and utility. Generally, there is a difference in maturity (and, consequently, quality) between the tools used in the industry and those developed as academic prototypes. 3. A new business ontology methodology with semantic inter-operability capabilities The methodology we have developed enables the construction of a business ontology to consolidate business processes, modernize information systems and at the same time integrate varied sources into an information model that is coherent, consistent and can be shared with other companies. It results in a specific ontology that is highly applicable to medium-sized and large commercial companies. We present a case study of companies that trade energy-related products in their various forms. Through this case we present and test our methodology by deriving the ontology from the companies’ business information systems. These systems contain subsystems for offers, sales, warehousing, invoicing, accounts and treasury. Their sources follow the relational model. The comprehensive information model has tables, queries and sets of logical rules as well as norms and restrictions specific to the companies (see Figs. 2 and 3). In terms of semantic interoperability, our approach applies the information semantic in the following way. In the first phase, we analyse the metadata and schemas of the databases in the case study. With this knowledge, we fix a partial map of concepts to a newly created ontology. This first phase sees the generation of a hierarchy of classes, properties and restrictions out of the semantic structure of the company’s schemas and norms in the mapped sources. The map is partial because it was necessary to reinterpret many schemas in order to provide the ontology with a well-organized logical structure. In addition to mapping from the schema sources, we also drew on the help of experts in the domain during the creation of the taxonomy. In the second phase, we filled the ontology with real requests from the databases. We used our own tool, (GOWL: ‘‘Generator OWL code’’). This tool consists of an SQL query manager that produces OWL format code containing both the data fields (the field of a database table referenced to an entity attribute or database register) and the relationships with other data fields via their properties and restrictions. Our methodology is centred on the three aforementioned approaches – mapping the databases, the construction of a global ontology and the use of logic-based languages in its development. Unlike integration projects based solely on a single approach, this hybrid formula allows the exploitation of the advantages of the approach based on ontologies, offsetting some of their inconveniences with the strengths of other approaches (for a deeper discussion, see: [20]). The ontology incorporates all the characteristics of the conceptual model of the data, both those related to the concepts, or TBox (schemas, integrity restrictions and company rules) and those related to the data fields with their relationships (ABox), as the data are imported from the tables. More specifically, on the one hand, TBox in Description Logic is a ‘‘terminological component’’, a ‘‘Concept definition’’. In other words, it is the definition of a new concept in terms of other previously defined concepts. It is used to describe statements in terms of controlled vocabularies in ontologies (see [40], pp. 17– 19). On the other hand, ABox is an ‘‘assertion component’’. The ABox contains extensional knowledge about the domain of interest, i.e. assertions about individuals (see [40], pp. 19 ss). TBox and ABox statements together form a knowledge base. Likewise, the ontology expands and improves the expressivity of the firms’ databases. This improvement, together with the solidity that the logic provides, converts the ontology into a useful platform and a starting-off point for the development of future applications to new information models. This (future) process of returning to a new model of information from the ontology will see databases treated and enriched semantically thanks to the fundamental contribution of the ontology. We have used the OWL DL (Ontology Web Language Description Logic), based on descriptive logic (see [40]). Descriptive logics are a family of knowledge representation languages. They have a formal semantic based on the logic of predicates and they represent an excellent jumping-off point for defining languages that can construct ontologies. These logics aid the reasoning tasks needed to support the construction, integration and evolution of ontologies. The ontology has been edited by  ́  ́ Protege5 [41] and Swoop6, and check for consistency by the RacerPro7 and Pellet8 reasoning services. 3.1. Stages of the methodology The methodology9 we present for the development of the ontology has six stages (see Fig. 1). This structure is based on a review practised to the set of methodologies supporting significant projects on integration information (see Section 2.3), although there is a closer relationship, especially in the first four stages, to the methodological process posed by the Semantic Information Management (see [42]), and used in the COG project. These are our methodological stages: 1 Requirements analysis: Shared analysis of requirements. Fixing the range of the project, its starting-off point, aims to be fulfilled and method used for the construction. 2 Metadata collection: Collection and cataloguing of metadata from an exhaustive analysis of the relevant sources: schemas, restrictions, company rules or norms, procedures, uses or performance modes when faced with the varied circumstances associated to specific practice, etc. 3 Construction: Construction of the ontology which includes the development of a new information model from the existing ones, following these two steps: a Modelling the taxonomy of classes, properties and restrictions (TBox). For instance, in the case study used to present our methodology, Fig. 6 shows ‘‘trading.owl’’, the ontology’s collapsed tree. Its class root is ‘‘Trade Handling’’, from which all other branches or classes derive. b Populating the ontology with class queries (ABox). This is done by means of a tool created ad hoc that makes the process automatic, importing data from the table registers. 4 Refinement: Rationalizing and refining the ontology, extracting the semantic from the sources and mapping it onto the ontology. The steps are: a Checking the taxonomy: to find the ontology’s incoherencies or contradictions via the most upto-date automatic reasoners already mentioned. b Checking and treating of the data items to find axiomatic inconsistencies and make the necessary corrections to achieve consistency. 5 Testing: Comparing the resulting ontology with other relevant ontologies, which in our test case are TOVE, REA, EO, BMO and e3 value that already exists on the Web, highlighting differences, mutual deficiencies and contributions. 6 Feedback: Collection of teaching experiences and selection of the best practices in integration, correction of possible errors and projections on future lines of investigation in order to make the most of our research. By checking the ontology with logical reasoners, we arrive at a solid consistent ontology. The model created can then be used as the basis for new applications. That is to say, as suggested in stage 6, once the methodology is applied, and an ontology is created and checked, it can then be used to create new information systems (i.e. coherent ontologies or databases). The most important aspects of the ontology are: (1) The ontology as repository of business knowledge. That is, we have a specific ontology that describes the commercial concepts used by the companies, both in terms of the classes and the properties or relationships between them. Also described are the restrictions imposed not only from the metadata but also from other data arising from daily use and practice that report what we could call ‘‘the company’s behavioural philosophy’’ and (2) The ontology as basis for new information systems, has all the expressivity of its sources. In addition, it is enriched by the incorporation of company rules and restrictions and, given its basis in logic, we can assure its consistency when used as a basis for modifying and updating information systems. Finally, the methodology we propose for the particular field of application we tackle (i.e. business ontologies) is very coherent with the Design Science Research Methodology (DSRM) for Information Systems Research, recently proposed by Peffers et al. [43]; This work has significant antecedents in [44], March and Smith [45,46]. Due to the potential relevance of this work in becoming a common framework for conducting Design Science Research in Information Systems, we will now make some comparative comments. In [43] there appears a general methodology that is structured in the six following steps or stages: (1) Identify problem and motivate. A specific research problem is defined and the value of the solution is justified. (2) Define objectives of a solution. The aims of the solution are inferred from the problem definition and the knowledge of what is possible and feasible. (3) Design and development. This consists of the creation of the artifact10. Such artifacts can be constructs, models, methods or instantiations. (4) Demonstration. This means demonstrating the use of the artifact in order to resolve one or more cases of the problem. (5) Evaluation. This deals with observation and measurement of how the artifact finds a solution to the problem. (6) Communication. Communicating the problem and its importance, the artifact, its use and novelty, the rigour of its design, and its efficiency for the researchers and other relevant parties. These steps or stages are normally structured in sequential order (1–6), although researchers do not always follow this sequence according to the problem they are trying to solve and the condition of situation of the problem. In what follows, we synthetically describe some of the differences and similarities between The DSRM methodology and the approach we have taken as presented herein. DSRM incorporates the principles, practices and procedures required to make information systems work. It provides a model for investigation in Design Science and a mental model for presenting and evaluating such research. In effect, it is a generic methodology theoretically applicable to any field of investigation. However, the methodology we present in this paper has the analysis of a specific business reality as its starting-off point. In particular, it is a series of research problems within this reality, the most important of which is the integration of information. On the other hand, the purpose of our methodology is the development of a business ontology as an efficient medium for the solution of the problem of information integration as well as a model for creating new information systems or the renewal of existing ones. All this, as previously discussed, aimed at business management. However, despite the differences in the starting-off point and in the objectives, we can appreciate significant common ground at various stages of both methodologies. We will now comment on some questions related to this issue. Given the generality and, thus, the applicability of the DSRM methodology, our methodology could be a case of DSRM application. In particular, we note that the third stage (design and development vs. construction of the ontology) is practically identical. Indeed, our computer application (GOWL) would be considered as described as a DSRM artifact. The two final stages, (‘‘problem identification and motivation, definition of the objectives for a solution’’ vs. ‘‘requirements analysis and metadata collection’’) undoubtedly specify the first phases of any project: the analysis of a real domain, its problems, and the fixing of the aims of the solution. The second stage of our methodology is very specific, since its starting-off point is real companies, using metadata not only to identify the problems but also to define the solution. Our fourth stage, which deals with the improvement and refining of the ontology, is a process that involves possible repetitions of previous stages. This stage is provided for in the fifth stage of Peffers et al.’s DSRM, when it states that researchers can decide to make repetitions with activity three in order to improve efficiency. Likewise, our methodology proposes stages five and six, which describe the testing of the methodology with other similar methodologies, and the possible feedback based on good practices and external experiences. These stages correspond to the fourth, fifth and sixth stages of our work, as both methodologies provide for the need to demonstrate or test the investigation as well as publishing it so that it can be shared and improved. Therefore, apart from a few minor differences of meaning, both methodologies agree on the fundamentals; the only differences being in perspectives and levels of generality on which both are based. This fact, also, talks about the suitability of our methodology. Finally, Table 2 shows a side-by-side comparison of the stages followed in each methodology. 3.2. Demonstration by empirical application: A case study Our case study is based on the information system of a company involved in the large-scale trade of energy products in their various forms and which belongs to an industrial group that is leader in the field of new technologies. Its trading volume is over 500h million per year. It sells products in telecommunications to manage task developed in the energy industry, energy transport, solar energy and supplies’ management. Fig. 2 illustrates the structure of its commercial information system. All the commercial information make up the universe of discourse, subdivided into domains (semantic classes or tables) in which are defined the relationships whose extension takes in subsets of explicit elemental facts (literal, records, registers, queries) and general laws normally represented by rules of integrity or inference. 3.3. Modelling the ontology (TBox) The semantic management of information gives us a methodology for modelling of the ontology, based on the semantics of the data sources. We have taken into account the various integration methods of the projects previously mentioned, comparing their methods with the ones applied in our investigation, but with this criterion uppermost: applicability to the integration of data schemas. The main characteristics of our methodology (see Fig. 4) are the following: (1) The integration paradigm consists of the construction of a central ontology (with possible future links to more generic ontologies) starting from an exhaustive analysis of the metadata collected from business databases, and setting up a correspondence between schemas towards the concept hierarchy. (2) The resulting correspondence pattern (mapping) between sources and ontology is very close to oneto-one correspondence. However although the ontology’s concepts are coherent, this is not the case with all its corresponding concepts in the databases. (3) Our methodology supports a partial mapping at class level, and a total mapping at property, individual, axiom and restriction levels. (4) Our methodology’s degree of automation fits between semi-automatic and interactive. The taxonomy’s creation  ́ge process is manual, with the use of the Prote  ́ tool, but the integration process of queries is automated by our own GOWL application, in which a series of SQL enquiries integrated into codified events in the application is defined. When these events are executed automatically, they generate OWL code files that are totally compatible  ́ge and integrable with the ontology in Prote  ́. (5) The ontology supports semantic interoperability, being implemented by the standard OWL language based on descriptive logic and having past the logical reasonability test (RacerPro, Pellet). (6) The mapping tool’s visualizing interface is made up of graphic forms in VBasic on Access. The ontology can be seen and edited using various tools, for example,  ́  ́ Protege, Swoop, Eclipse, etc. (7) The maturity of this methodology and its accompanying tools makes it a solid basis for new ontology generation and applications based on those ontologies. It is currently being used for building new business databases on the SQL Server (see Future Research). The semantic management of information as reported by our methodology contains the layers shown in Fig. 4. The external data sources’ layer lies within the block at the bottom of the figure. It describes the place and origin of the data and metadata. In our case study, these sources are databases in a relational format (RDDBB). This layer includes correspondences between the ontology’s sources and concepts. The block at the top contains the tools and ontology construction layers. The latter describes the meaning of the data, that is, the information model. The ontology built following this methodology is made up of the following elements: 1. The hierarchy of classes. The concepts are set in a hierarchy of classes. The classes, as mentioned before, come from metadata schemas collected from databases. In the ontology, each element (classes, properties, data items) is given its own name that should be as expressive as possible. The names of the subclasses and subproperties have prefixes that indicate their origin immediately above them. The names of data items are imported from the corresponding tables and, to avoid duplication, they are given a prefix indicating the class to which they belong. Fig. 5 shows an example . 2. The properties of the classes. The roles or properties of the classes, that is, their relationships, are taken and organized into a hierarchy. These can be of two types: a. Object property: are those that relate an individual of a class to an individual of another class, or, whose domain and range are classes. In the methodology, different types of object property are considered (functional, inverse functional, symmetric and transitive), and these are applied to each property that forms part of the ontology’s hierarchy. For example, the ‘‘have_payment’’ property means: The payment of invoices to the supplier is done by a payment order at a bank or financial company. The domain of the property is ‘‘supplier_invoices’’, and its range is ‘‘payment_order’’. b. Data-type Property: those that relate data items of a class to some type of value; their domains are the classes but their range is a type of fixed value data. The ontology only accepts the creation of object properties. The technical reason is that the automated reasoners do not currently consider this type of property, which makes it impossible to check the ontology. To get around this problem, the values of number and date are represented by a chain format in the ontology (Fig. 6). 3. Company rules and restrictions. Next, the specific conditions imposed on the classes are generated, with the aim of modelling their meaning within the company environment (for example, suppliers’ invoices are to be paid after a certain number of days, or the profit made on each sale cannot be less than a set percentage). The conditions restricting the classes can be either necessary or necessary and sufficient. a. Descriptions and notes. In addition to their formal description, notes or comments can be added to the classes and properties in natural language in order to make their meaning more intelligible. 3.4. Modelling the ontology (ABox) Once the taxonomy and all its elements are created, the entities or data items are inserted and each one matched to its corresponding class with its properties. Then the relationships between data items are generated and the properties that affect them are applied to each one. In order to automate the process of mapping and integration of the sources’ data items, we have constructed a software tool, codified in Vbasic, whose main content is a series of SQL enquiries capable of translating the data in the MsAccess registers into OWL and integrating them into the body of the ontology. The following SQL query extracts each client from the database with the ‘‘have_country’’ attribute and converts them into OWL code The ontology constructed with this methodology possesses the characteristics of the domains and subdomains of the sources in the case study. In this context, we could say that ours is a standard ontology as it aims to take in the entire terminology used by companies trading products related to energy, engineering and telecommunications. Therefore, we have taken into account the extensive experience and knowledge inherent in the development of the databases, the applications and their maintenance in working companies. 3.5. Treating the ontology In order to guarantee the logical coherence and solidity of the ontology, our methodology incorporates treatment and repairing processes of possible errors [47]. The RacerPro 1.9 and Pellet 1.5. logical reasoners are used for this in the computational logic field. These carry out a complete check of the class and property hierarchy, and of the data items and their relationships. It is important to point out that the resulting ontology is totally coherent from the logic viewpoint, which means that it is free of contradictions. This coherence is necessary when recovering information or when serving as a base for new databases or applications. Regarding expressivity, the ontology has all the expressive power of the OWL-DL language based on descriptive logic. It formalizes the data source elements and even takes on new elements drawn from the experience and reflections of experts and users. The level of expressivity it reaches is ‘‘SHOIN(D)’’ in terms of descriptive logic. The descriptive logic expressivity SHOIN(D) of this ontology is: the ‘‘S’’ symbol is the abbreviation for ALC with a transitive role. ALC allows concept intersection, full negation, full universal quantification, full existential quantification and concept disjunction. ‘‘H’’ means the role hierarchy. ‘‘O’’ is for nominals (Singleton sets, One of, for example a. ‘‘I’’ means inverse roles (properties which have inverses specified, or properties that are symmetric)). ‘‘N’’ means number restrictions (cardinality restrictions and functional properties). ‘‘D’’ is for datatypes (see, for an in-depth review: [40]). In sum, this assumes that the ontology has most of the features of descriptive logic, of which we emphasize its capacity to: formalize concepts, attributes, roles and complements, and structure them in a hierarchy; integrate database queries (individuals); incorporate negative properties and qualified restrictions; and admit various types of data. 4. Related work: A comparative analysis with other business ontologies. 4.1. An introduction to the most significant business ontologies. Following the construction of our ontology, it is a useful and interesting exercise to match it to other existing business ontologies whose domains are similar or close to ours, contrast focus and components, and reveal similarities and differences. We have selected the business world’s most representative ontologies currently available, and we describe the main ideas put forward by their investigators: BMO (Business Management Ontology). The BMO ([48] ontology, or set of ontologies, represents an integrated information model which, along with the design of the business processes, incorporates project management, requirements and performances to make up the fundamentals of a company management knowledge base. Its main users are business analysts, but it has also been taken up by information technology (IT) experts to establish correspondences with related software deficiencies such as business focus and the description of Web services. It enables the definition of the private and public processes of companies, entities and business focus, as well as the services implemented by the processes of activity. It follows the European Union’s UN/CEFACT methodology. The BMO currently contains 40 ontologies that define some 650 classes belonging to various domains within the business environment. BMO characteristics include: (a) linking ontologies to produce a single general vision; (b) multiple ontologies can exist in parallel to supply many organizations or industries. Each ontology specific to an industry integrates and extends the ontologies of the companies’ generic domain ontologies; (c) it associates the company’s fundamental process concepts to generic business concepts such as properties, company rules and documents, etc, and d) it contains technically oriented ontologies like the ontology of business focuses. REA (The ResourceEventAgent). This is a business domain ontology originally created to define and develop accounting systems. Like its data semantic model, the REA business model is based on the Entity-Relationship metamodel but contains additional primitive ontologies, axioms and guidelines associated to the model that help construct and validate the conceptual models of the information systems belonging to the accounting systems. The REA was initially created by [49,50] many to model accounting systems. However, it was found to be very useful and intuitive as it brought a greater understanding of company processes, and became one of the best frameworks both for traditional companies and electronic commerce systems. It has been extended in order to provide concepts of greater use in understanding aspects of processing as well as economic aspects (economic exchanges). For almost 20 years, the REA model has been widely used as an instrument for teaching business students how to design accounting databases [51]. However, few companies use it for systems development practice. Recently, researchers and professionals have renewed their interest in this model for two reasons (see [52]) (a) The developers of the REA model have signed up to the ISO OpenEDI initiative, UN/CEFACT, OAG, eBTWG. Their participation has resulted in the adoption of the REA model as a business ontology in the UMM [UN/ 03] methodology and the ECIMF system, as indicated in the [53]. (b) The REA model has been proposed as the theoretical foundation for the basic models of ERP (Enterprise Resource Planning) systems. The architecture of the REA business ontology has three levels or layers representing economic activity, three classes of elements that can be identified in each economic exchange or conversion process: economic resources, economic events and economic agents. Each economic resource is linked to an economic event that causes its entry and exit flows (stock flow). In addition, each economic event that results in a resources entry (a purchase, for example) is necessarily paired to an exit event (a payment), and vice versa (duality). The participative relationship describes the agents involved in an economic event. This simple ontological pattern is the basis of the REA ontology, and it comes from McCarthy’s original model [54]. The economic events are performed within a chain or ordered sequence of actions called business processes. We refer to the architecture on three levels (chain value, business processes and economic events). With advances in technology and specification methods, the applicability of the REA conceptual model has also changed. Works by Geerts and McCarthy [55] have widened its basic framework several times over recent years. The REA changes the direction of the transaction model in two ways: removing it from the traditional vision inherited from the subject matter based on finance and the single company, and directing it towards big business with the new perspective of the modern ERPs and the information systems of the varied types of electronic commerce. With the entry of commercial partners and long-term relationships, there is a need for more reliable and predictable structures in which both parts formalize their exchange contracts. The REA ontology answers this need by adding classes such as economic commitment, economic contract, agreement, etc. The REA establishes correspondences with one of the ontologies from the IEEE Standard Upper Ontology Working Group, called SUMO (Suggested Upper Merged Ontology), which is on a superior level. EO (Enterprise Ontology): The Enterprise Ontology [56] has been developed as part of the Enterprise project by the University of Edinburgh’s Institute of Artificial Intelligence Applications, together with IBM, Lloyd’s Register, Logica UK Limited and Unilever. The project has been backed by the UK’s Department of Trade and Industry within its Intelligent Systems Integration programme (IED4/8032). Its business modelling aims to achieve a global vision of the organization such that it can be used as a basis for decision-making. It is not a traditional organizational vision but a vision that comprises the subject matter or domain of the organization’s functioning. To achieve this aim, strong flexible tools are needed to support integration and communication. During the EO development process, the aim was to embrace those concepts that were widely accepted in the business world, presenting their definitions in natural language. It begins with basic concepts (activity, relationship, actor, for example). These are used to define the main body of the terms, which is divided into the following areas or sections: activities and processes, organization, strategy and marketing. TOVE (TOronto Virtual Enterprise). The TOVE project aims to develop an integrated set of ontologies for public and private company modelling, with the following features [57]: (a) To provide a terminology that company can share so that each agent can use and understand it. (b) To define the meaning of each (semantic) term as precisely as possible. Implementing the semantic within a set of axioms will give TOVE the ability to automatically deduce most common-sense questions about the company. (c) To define a group of symbols to graphically represent a term or the concept built from it. A series of ontologies have been developed in the TOVE [58] framework that comprises various aspects of the business environment. These ontologies have recently been structured on three levels: (a) Foundation ontologies, which among others include ontologies on activities [59], resources [57], and organization [60]. (b) Derived ontologies, which include among others ontologies on quality management [61], product design [62], and costs [63]. (c) Business ontologies, which include ontologies on company design, flow of materials, projects and business processes. e3_value. With the new information technologies, business models based on e-business have an increasingly wide field of development and their share of all business transactions grows daily. This ontology edges closer to a more rigorous conceptualization of business modelling. The e3_value ontology’s main feature is its basis in value (see [64]). The e3_value ontology consists of a conceptual approach whose objective is the representation of business models. The notion of value, and how the objects are created, exchanged and consumed in a multi-agent network, is the central theme of this ontology. It enables the representation and analysis of many non-trivial ideas on business domains, even processes and mechanisms that are important to the company such as the causality of income flows, client property, the ability to fix prices and chose alternative agents to deliver objects of value and company business. Based on the notion of value, as a foundation of electronic commerce modelling, the concepts are defined as: Agent, Activity of the Value, Object of the Value, Port of the Value, Interface of the Value, Exchange of the Value and Offer of the Value. The benefits of this ontology oriented towards value are two ([64]): better communication between those who must make decisions on the essential points of the model and the shareholders; a greater and more complete understanding of the operations of electronic commerce and its requirements. 4.2. Comparison To compare our ontology with the others we follow Pateli and Giaglis [65] and Jasper and Uschold [66], providing a fixed comparative framework whose main elements are as follows: (1) Purpose of the ontology. Motives or reasons that justify the existence of the ontology (achieving better communication, interoperability, reliability, specification, representation and knowledge, etc). (2) Focus of the ontology. The focus of attention differs from model to model. Some centre on one company, others look to a variety of companies. Some are based on strategy, others on operational aspects. Some focus specifically on technology, while others turn their attention to innovation or work on both simultaneously. (3) Components of the ontology. This refers to the concepts, relationships or properties, rules and axioms that the ontology uses to represent the business model. (4) Role of the ontology. Whether it contains operational data or is made up of concepts, relationships and axioms in order to contain operational data, or whether it is a language that expresses ontologies on the two previous levels. (5) Ontological representation. The degree of meaning that an ontology represents. This varies greatly from one to another. The simplest ontologies are a simple collection of terms. The meaning grasped by an ontology varies according to the quantity of the data represented and the representation’s degree of formality. According to the quantity of data, there are lightweight ontologies (a limited number of concepts, relationships and axioms) and heavyweight ontologies (a greater number of concepts, relationships and axioms). According to the degree of formality, they move between ‘‘highly informal’’ and ‘‘strictly formal’’. 5. Final remarks 5.1. Limitations and future research The methodology has so far been tested in a single domain that of a commercial energy trading company. While we believe that the results can be generalized across other domains, this still needs to be tested. While our ontology can be expressed in OWL, it has not yet been integrated with other existing ontologies. Future research should examine the ease of any such integration. Given that the ontology is created from local sources, the quality of the global model depends mainly on the quality of the local model. In our case, the ontology has been refined in two ways: avoiding inclusion of the databases’ errors and inconsistencies; and not adding concepts that were not found in the databases. Another quality element has been the interaction with experts and users drawing lessons from their work in practice. Thus we can state that the ontology reflects the knowledge of the company. Other directions for future research include: The possibilities of alignment or mixing of specific  ontologies such as the one we have presented with others which are more general. The development of reasoners with greater checking capacity would be of great importance, not only in terms of memory use but above all for their capacity to reason and explain errors. We also believe that it is vitally important to develop projects that investigate and produce software applications capable of using ontologies in a fully automated manner in order to convert their enormous quantity of data into information and productive knowledge. 5.2. Conclusions We have presented a methodology to create business ontologies based on the existing database structures and information systems of an organization. We have applied that methodology to a specific case study to produce a real ontology based on the target organizations information systems. Finally we compared the resulting ontology with other more established business ontologies. This comparison of our case study ontology to those that have preceded it leads us to the following conclusions regarding our methodology. The Trading.owl ontology that was produced using our methodology is applicable to companies belonging to the energy trading sector. It is quite complete for this field as, apart from the metadata, it integrates the entire semantic support of the uses, habits, norms and ways of understanding a specific domain. Using our methodology results in an ontological model that is far better suited to the subject company than the generic models represented by the other leading business ontologies. In ours, concepts exist that could be easily integrated within the models represented by the majority of ontologies described. However, we believe it would be very difficult to align them. Without doubt, these ontologies would provide Trading.owl with many enriching concepts, in particular those economic and business concepts whose rigorous definitions could complete the meaning of Trading.owl’s concepts. On the other hand, an ontology produced following our methodology could provide all these business ontologies with the richness offered by specific knowledge directly extracted from real companies and fully treated from the formal logic point of view.
Automated ontology construction for unstructured text documents. Ontology is playing an increasingly important role in knowledge management and the Semantic Web. This study presents a novel episode-based ontology construction mechanism to extract domain ontology from unstructured text documents. Additionally, fuzzy numbers for conceptual similarity computing are presented for concept clustering and taxonomic relation definitions. Moreover, concept attributes and operations can be extracted from episodes to construct a domain ontology, while non-taxonomic relations can be generated from episodes. The fuzzy inference mechanism is also applied to obtain new instances for ontology learning. Experimental results show that the proposed approach can effectively construct a Chinese domain ontology from unstructured text documents. Ó 2006 Elsevier B.V. All rights reserved. 1. Introduction Ontology is an essential part of many applications. Supported by an ontology, both the user and the system can communicate with each other using a common understanding of a domain [4,19]. Although ontology has been proposed as an important and natural means of representing real-world knowledge for the development of database designs, most ontology constructions are not performed either systematically or automatically [17]. Information systems increasingly depend on ontology to structure data in a machine-readable format and ensure satisfactory performance. Some generic ontologies, like WordNet [13] and Cyc [9], are available, but most applications need a specific domain ontology to describe concepts and relations in that domain. Automatic ontology construction is a difficult task owing to the lack of a structured knowledge base or domain thesaurus. Ontology construction traditionally depends on domain experts, but it is lengthy, costly and controversial [15]. While many ontology tools, like OntoEdit [18], Protege-2000 [16] and Ontolingua [2], are available to aid the construction of ontologies, ontology construction still needs human effort. Most tudies of ontology construction and application assume manual construction, and only a few have proposed automatic methods [5]. Various ontology construction approaches have been presented recently. For instance, Khan and Luo [5] constructed an ontology using a modified Self-Organization Map (SOM) clustering algorithm in a bottomup fashion. Widyantoro and Yen [20] presented a fuzzy ontology based on fuzzy narrower term relationships and a fuzzy broader term relation for query refinement in an abstract search engine. Yoshinaga et al. [22] automatically constructed an ontology including keywords and relations. Zhou et al. [23] proposed a customizable collaborative system to construct the domain ontology. Maedche and Staab [11] presented an ontology learning framework encompassing ontology import, extraction, pruning, refinement and evaluation. MissiKoff et al. [14] proposed an integrated approach to web ontology learning and engineering, which can construct and access the domain ontology to integrate information intelligently within a virtual user community. Navigli et al. [15] utilized the WordNet and SemCor to interpret complex terms semantically, which it can save the problem of the semantic disambiguation. OntoSeek [31] combined an ontology-driven content-matching mechanism with moderately expressive representation formalism. Lammari and Metais [29] presented a set of algorithms to construct and maintain ontologies. Andreasen et al. [27] described a method and a system for content-based querying of texts based on the ontology. Elliman et al. [34] proposed a method for constructing the ontology to represent a set of web pages on a specified site, using the SOM to construct the hierarchy. Hotho et al. [35] proposed various clustering techniques to view text documents with the help of an ontology. Lee et al. [26] presented a meeting scheduling system based on the personal ontology and the fuzzy meeting scheduling ontology. Furthermore, Lee et al. also presented some approaches for Chinese text processing, for example, an ontology-based fuzzy event extraction agent for Chinese news summarization [8,30], a fuzzy ontology for applying to text summarization [28], and a Chinese term clustering mechanism for generating semantic concepts for a news ontology [25]. The most recent development in standard ontology languages is OWL from the World Wide Web Consortium (W3C). Like Protege-OWL plugin, it not only allows concepts to be described, but also provides new facilities. In addition, OWL has a richer set of operators, including and, or and negation, than other standard ontology languages. Therefore, complex concepts can be built in definitions from simpler concepts. Furthermore, the logical model allows the use of a reasoner to check whether the statements and definitions in the ontology are mutually consistent, and to recognize which concepts fit under which definitions. The reasoner can therefore help maintain the hierarchy correctly. This feature is particularly helpful when handling cases of classes with more than one parent. OWL ontologies may be categorized into three species or sub-languages, namely OWL-Lite, OWL-DL and OWL-Full. A defining feature of each sublanguage is its expressiveness. OWL-Lite is the least expressive sub-language, while OWL-Full is the most expressive, with OWL-DL between the other two sub-languages. The expressiveness of OWL-DL falls between that of OWL-Lite and OWL-Full. OWL-DL may be considered as an extension of OWL-Lite and OWL-Full an extension of OWL-DL [36]. This study presents an episode-based fuzzy inference mechanism to extract the domain ontology from unstructured text documents. The proposed approach is an original synthesis of previously reported approaches, including the concept of the episode, the concept clustering for Chinese text documents, and the fuzzy inference mechanism. No effective and efficient approaches to fully automated ontology construction from unstructured Chinese text documents have yet been found. Additionally, the challenge of solving the conceptual meaning of the Chinese term in a sentence is that a Chinese term may comprise many words and that a combination of words in a Chinese term may have different meanings. Take the Chinese terms ‘‘ (Government Information Office of Executive Yuan)’’ and ‘‘ (computer science)’’ for instance. The Chinese term ‘‘ (Government Information Office of Executive Yuan)’’ has six words, namely ‘‘ (walk)’’, ‘‘ (policy)’’, ‘‘ (yuan)’’, ‘‘ (new)’’, ‘‘ (smell)’’, and ‘‘ (bureau)’’, and can be split in various ways, such as {‘‘ ’’, ‘‘ ’’, ‘‘ ’’, ‘‘ ’’, ‘‘ ’’, ‘‘ ’’}, {‘‘ (administrator)’’, ‘‘ (yuan)’’, ‘‘ (news)’’, ‘‘ (bureau)’’}, and {‘‘ (Executive Yuan)’’, ‘‘ (Government Information Office)’’}. The Chinese term ‘‘ (computer science)’’ has four words, namely ‘‘ (electricity)’’, ‘‘ (brain)’’, ‘‘ (family)’’ and ‘‘ (learn)’’, and can be split into {‘‘ ’’, ‘‘ ’’, ‘‘ ’’, ‘‘ ’’}, {‘‘ (computer)’’, ‘‘ (science)’’}, {‘‘ (electricity)’’, ‘‘ (department of the brain)’’, ‘‘ (learn)’’}, and {‘‘ (department of the computer)’’, ‘‘ (learn)’’}. However, challenge of processing unstructured text documents is to extract the desired Chinese terms, such as ‘‘ (Government Information Office of Executive Yuan)’’ or ‘‘ (computer science)’’, from a sentence. Therefore, natural language processing in the Chinese language is very different from that in English. The fuzzy numbers for the conceptual similarity computing are presented for concept clustering and defining taxonomic relationships. Moreover, the attributes and operations of concepts can be extracted from episodes for the ontology construction. Non-taxonomic relationships are generated based on episodes. The fuzzy inference mechanism is further adopted to obtain new ontology learning instances. Experimental results indicate that the proposed approach can effectively construct the Chinese domain ontology from unstructured text documents. This study is organized as follows. Section 2 presents the episode-based ontology construction mechanism. Section 3 proposes the fuzzy inference mechanism for Chinese text ontology learning. The experimental results are shown in Section 4. Finally, conclusions are drawn in Section 5. 2. Episode-based ontology construction mechanism This section describes the episode-based ontology construction mechanism. Section 2.1 briefly introduces the concept of the episode. The domain ontology is defined in Section 1. Section 2.3 describes the automatic construction process for Chinese domain ontology from unstructured text documents. 2.1. The concept of the episode The concept of the episode was proposed by Ahonen et al. [1] and Mannila et al. [12]. An episode e is formally defined as a triple (V, 6, g), where V denotes a set of nodes; 6 denotes a partial order on V, and g : V ! E denotes a mapping that associates each node with an event type E. The interpretation of an episode is that the events in g(V) have to occur in the order described by 6. An episode e is parallel if the partial order 6 is a trivial order (i.e., x not 6 y for all x, y 2 V such that x 5 y). Conversely, an episode e is serial if the partial order 6 is a total order (i.e., x 6 y or y 6 x for all x, y 2 V). Informally, an episode is a partially ordered collection of events occurring together. Episodes can be directed acyclic graphs. For instance, consider episodes a, b, and c in Fig. 1. Episode a is a serial episode, and can only occur in a sequence that includes events of type a and b in that order. Episode b is a parallel episode that does not impose constraints on the order of d and e. Episode c is a non-serial and non-parallel episode, and can occur in a sequence in which occurrences of c precede occurrences of d and e, which may occur in any order [12]. 2.2. The definition of domain ontology This section briefly describes the graph-based definition of the domain ontology. Fig. 2 illustrates the structure of the graph-based domain ontology. Definition 1 (Domain Ontology [28]). A domain ontology defines a set of representational terms called concepts. Inter-relationships among these concepts describe a target world [29]. A domain ontology has four layers, called the domain layer, category layer, class layer and instance layer [30]. The domain layer denotes the domain name of an ontology, and comprises various categories defined by domain experts. The category layer has many categories, termed category 1, category 2, . . ., and category k. Each concept in the class layer contains a concept name Ci, an attribute set fACi 1 ; . . . ; ACi qi g and an operation set fOCi 1 ; . . . ; OCi qi g for an application domain. In the instance layer, each concept contains a concept name Ci, an attribute set 2.3. Automatic construction process for domain ontology This section applies the concept of the episodes to assist in the construction of Chinese domain ontology from unstructured text documents. Additionally, the SOM algorithm [5,32] is adopted to cluster the concepts of Chinese terms. Fig. 3 displays the flowchart of the episode-based Chinese domain ontology construction, which includes four processes, namely Document Pre-processing, Concept Clustering, Episode Extraction, and Attributes-Operations-Associations Extraction, which are described below. A. Document pre-processing The CKIP [24] utilized in this study includes a Chinese Part-of-Speech (POS) Tagger and a Chinese news corpus developed by the CKIP (Chinese Knowledge Information Processing) Group, a research team in Taiwan formed by the Institute of Information Science and the Institute of Linguistics of Academia Sinica in 1986. The aim of the CKIP is to create a fundamental research environment for Chinese natural language processing (http://ckip.iis.sinica.edu.tw/CKIP/engversion/index.htm). The corpus and dictionary provide adequate Chinese POS knowledge to analyze the features of the terms for semantic concept clustering. The Stop Word Filter is used to reduce significantly the number of Chinese terms, while preserving the terms with partial noun tags or verb tags and filtering the terms with other POS tags. The preserved terms used in this study are Na (common noun), Nb (proper noun), Nc (location noun), Nd (time noun) and various classes of verbs (VA, VB, VC, VD, VE, VF, VG, VH, VI, VJ, VK, VL). The filtered terms are Ne (stable noun), Nf (quantity noun), Ng (direction noun), Nh (pronoun), adjective, adverb, preposition, conjunction, particle, and interjection. However, whether a term is preserved or filtered depends on the domain and applications. B. Concept clustering This process aims to cluster concepts and instances from documents. To select important terms for Concept Clustering, the nouns with the highest tf · idf values are preserved and adopted in Term Analysis, where tf is the term frequency and idf is inverse document frequency [3]. In Term Analysis process of this study, the three factors, namely POS, Term-Vocabulary (TV), and Term-Concept (TC) were selected as the conceptual similarity factors for analyzing the Chinese terms and calculating the conceptual similarity between any two Chinese terms based on the features of the Chinese language and the definitions of the CKIP. These terms are briefly described as follows. (1) POS: Each node of the tagging tree denotes a Chinese POS tag defined by CKIP. The path length between two nodes is adopted to calculate the conceptual similarity in POS between any two Chinese terms. (2) TV: The value of the conceptual similarity in TV between any two Chinese terms is calculated, according to the characteristics of Chinese language. (3) TC: The conceptual structure provided by CKIP is adopted to obtain the concept of nouns and calculate the conceptual similarity in TC between any two Chinese terms. These factors are described below. Other relevant factors for analyzing Chinese terms are left for future study. B.1. The conceptual similarity in pos between any two terms. Each node of the tagging tree in Fig. 4 denotes a Chinese POS tag defined by CKIP. The path length between two nodes is used to calculate the conceptual similarity wpos in POS between any two Chinese terms. The path length for any two nodes in the tagging tree is bounded in the interval [0, 6]. The value of wpos is large when the path distance of any two Chinese terms is short. For instance, the two terms ‘‘ (computer)’’ and ‘‘ (software)’’ have POS values of ‘‘Nab’’ and ‘‘Nac’’, respectively, hence the path distance between them is 2 (Nab ! Na ! Nac). The conceptual similarity wpos is calculated as follows. B.2. The conceptual similarity in term-vocabulary between any two terms. Almost every word in the Chinese language is a morpheme with its own meaning. This study considers three characteristics of Chinese terms to analyze the conceptual similarity in term-vocabulary between any two terms based on the definition in the CKIP [25,28,30]. The three characteristics are: (1) the more identical words in both terms in the pair, the more similar the terms are to each other in semantic meaning; (2) terms in a pair with both identical and continuous words have much greater semantic similarity than those in a pair without identical or continuous words, and (3) terms in a pair with identical starting or ending words have a strong semantic similarity. The conceptual similarity value wTV in TV between any pair of Chinese terms is calculated according to these three characteristics of Chinese language. The algorithm for calculating the conceptual similarity wTV is given below. B.3. The conceptual similarity in term-concept between any two terms. The conceptual structure provided by the CKIP for Chinese terms is like the hierarchical tree shown in the Appendix [24]. The conceptual structure is adopted to obtain the concept of nouns and calculate the conceptual similarity wTC in TC between any two Chinese terms. The path length for any two nodes in the conceptual structure is bounded in the interval [0, 12]. The algorithm for calculating the conceptual similarity wTC is now described as follows. Next, a well-known clustering method, the SOM [6,37], is adopted for concept clustering. An SOM is an unsupervised neural network, which maps high-dimensional input data onto a two-dimensional output topological space. Moreover, the SOM can be regarded as a ‘‘nonlinear projection’’ of the high-dimensional input data vector onto the two-dimensional display, making SOM optimally suitable for visualizing and clustering complex data [5]. Self-organizing neural network-based clustering algorithms have been widely studied for the last decade, but like hierarchical clustering algorithms, have no predefined number of clusters in self-organizing neural networks. The clusters can be distinguished by visualization with manual help. The network size is always greater than the optimal number of clusters in the underlying dataset [32]. A number of self-organizing neural network-based hierarchical clustering algorithms have been presented. The neural network mechanism makes these algorithms robust in terms of noisy data. Additionally, these algorithms inherit the advantages of the original self-organizing map, which easily visualizes the clustering result [32]. In SOM Clustering, each input term is expressed as a vector {di1, di2, . . . , din}, where dij denotes the conceptual similarity between any two Chinese terms ti and tj, and dij is bounded in the interval [0, 1]. The conceptual similarity values wpos, wTV, and wTC in POS, TV, and TC, respectively, can be obtained according to the Term Analysis discussion. Eq. (1) is used to compute the conceptual similarity between two Chinese terms. This study sets w1 = 0.3, w2 = 0.3 and w3 = 0.4, because the authors regard TC as more important than POS and TV. An input term vector {di1, di2, . . . , din} can be mapped to input neurons. Terms with a certain degree of conceptual similarity in word meaning are gathered in the neighboring output neurons. Take Chinese terms ‘‘ (car)’’, ‘‘ (taxi)’’, ‘‘ (bus)’’, and ‘‘ (train)’’ for example. All of them have the same Chinese word ‘‘ ’’. Through the domain expert and term analysis process of three conceptual clustering factors, POS, TV, and TC, ‘‘ (car)’’, ‘‘ (taxi)’’, ‘‘ (bus)’’, and ‘‘ (train)’’ are determined to be subclass concepts of ‘‘ (vehicle)’’. C. Episode extraction The Document Pre-processing process separates the text into sentences, including nouns and verbs, which are then fed into the Episode Extraction process to obtain the episodes. This study denotes a term as a triple (term, POS, index), where index is the position of this term in the sentence. An episode is extracted if the episode occurs within an interval of a given window size, and the episode’s occurring frequency of the text document set is larger than the defined minimal occurrence value. To increase the accuracy of the episodes, the punctuation is filtered and the POS of terms with Na, Nb, Nc, Nd and verbs are retained in the sentence. An example of a sentence in a Chinese news document is shown below By the Document Pre-processing with the CKIP process, the sentence with the terms and POS is created as follows: By the Stop Word Filter process, the terms with triple (term, POS, index) representation are shown below: Finally, the episode extraction process generates the episodes with window size 6 as follows: Yen et al. [21] presented an algorithm to mine the sequential patterns to discover knowledge from large databases. This study extends the algorithm to extract term episodes from Chinese news documents. The episode extraction algorithm is stopped when large 3-sequences are found. The notation for the episode extraction algorithm is given below: I < t1 ; t2 ; . . . ; tk >: This set stores the term sequence t1, t2, . . . , tk occurring in a given sentence. I < t1 ; t2 ; . . . ; tk > :cardinality: This variable denotes the number of item in I < t1 ; t2 ; . . . ; tk >, and the number of occurrences of the term sequence t1, t2, . . . , tk. ti.position: denotes the position of ti in a sentence. sentence_num: The sequence number of a sentence. Episode Extraction Algorithm /* Extract episodes that appear within the given window size with occurrence frequencies above given minimum occurrence from sentences */ D. Attribute_Operation_Association Extraction After obtaining the episodes, the terms are mapped to the result of the Concept Clustering to tag the concept name, as in the following example. Herein, SOM approach is used to cluster concepts and instances, and the experts carry out the refining clustering so that the Concept Clustering result indicates that ‘‘ (Korea)’’, ‘‘ (Italy)’’, ‘‘ (Brazil)’’, and ‘‘ (England)’’ are determined to be an instance while ‘‘ (team)’’ is determined to be a concept with both as nouns, ‘‘ (champion)’’ is an instance of the concept ‘‘ (award)’’, and ‘‘ (Beckham)’’ and ‘‘ (Rivaldo)’’ are instances of concept ‘‘ (team member)’’. It is finished using a semi-automatic way and automatic concept clustering is the further study. The algorithm for mapping instances and concepts is shown below. The notation for the mapping instances and the concept algorithm is as follows: ei: denotes an episode, where 1 6 i 6 n, and n denotes the number of all episodes. tj: denotes a term, where 1 6 j 6 m, and m denotes the number of all terms in an episode. The morphological features of Chinese terms are now described. The Attributes, operations and associations are extracted from episodes according to the morphological information of the Chinese term and the Chinese syntax. For ontology construction, patterns such as ‘‘concept-attribute-value’’, ‘‘concept-association-concept’’ or ‘‘concept-operation’’ are extracted from the domain data. These patterns are treated as sentence patterns, such as ‘‘subject-verb-object’’ or ‘‘subject-modifier’’. However, the subject, object, and modifier are hard to extract from Chinese documents, because Chinese grammar is very complex. Therefore, the morphological features of Chinese terms were analyzed to assist the extraction of attributes, operations, and associations from episodes. The CKIP of Academia Sinica classifies verbs into 12 categories. In the proposed morphological analysis, these 12 categories of verbs are classified into five groups according to their meanings and syntaxes. These five groups of verbs are treated as operations or associations by their morphological features, listed in Table 1. Operations describe actions of a concept, so verbs that only need a subject, are selected as operations. Associations describe relationship between two concepts, so verbs needing a subject and object, are selected as associations. Similarly, nouns are treated as concepts or properties, and are given attributes and associations according to their morphological features, as listed in Table 2. To reduce the complexity of the automated ontology construction, this study just discusses the similarities between concepts automatically created but validated by domain experts, and those between manual-created attributes and those between manual-created operations. That is, the attributes, operations, and associations are not totally extracted automatically using the proposed algorithm, but it needs the validation of the domain experts. Future study will discuss the similarities between automatic-constructed attributes or between automatic-constructed operations. Finally, the Attributes_Operations_Associations Extraction algorithm is shown as follows: The notation for the Attributes_Operations_Associations Extraction algorithm is given below. ei: denotes an episode, where 1 6 i 6 n and n denotes the total number of episodes. 3. A fuzzy inference mechanism for Chinese text ontology learning This section introduces a parallel fuzzy inference mechanism to infer a new instance belonging to which one existing concept. Fuzzy logic is intended to alleviate difficulties in developing and analyzing complex systems encountered by conventional mathematical tools, based on the observation that human reasoning can adopt the concept and knowledge without well-defined, sharp boundaries (i.e., vague concepts) [33]. Therefore, fuzzy logic is suitable for natural language processing. The fuzzy inference mechanism is one of many methods to solve the natural language processing problem. However, previous studies [25,28,30] indicate that a new instance’s concept can be easily inferred by parallel fuzzy inference. Therefore, this study adopts the fuzzy inference mechanism to infer a new instance. Future studies will try to infer a new instance using other methods. A new instance may contain many different attributes, operations and associations, which can be added to the concept to update the domain ontology. Fig. 5 shows the concept update process. Fig. 5(a) shows an existed concept in the domain ontology; Fig. 5(b) shows a new instance discovered from new documents, and Fig. 5(c) shows the updated concept of Fig. 5(a) and (b). Section 3.1 describes the conceptual resonance between a concept and a new instance. Section 3.2 presents a parallel fuzzy inference mechanism for conceptual resonance computing. 3.1. Conceptual resonance between a concept and a new instance The conceptual resonance is defined as a degree of belonging between a concept and a new instance. Hence, a new instance is likely to belong to a particular concept if the conceptual resonance between them is high. The conceptual resonance determines whether a new instance belongs to an existing or a new concept. The concept describes a group of instances with identical attributes, operations and associations to other instances. Therefore, if a new instance belongs to a new concept, then the instance and the concept have a strong conceptual resonance, and they probably have some identical attributes, operations and associations. If a new instance does not belong to any existing concepts in the domain ontology, then the instance and the concepts have a low mutual conceptual resonance, and they do not have many identical attributes, operations or associations. The conceptual resonance is inferred from existed concepts in the domain ontology. This study adopts four fuzzy variables, resonance strength in attribute xA, resonance strength in operation xO, resonance strength in association domain xD and resonance strength in association range xR, to calculate the conceptual resonance strength between a concept and a new instance, which are described them as follows. A. Resonance strength in attribute xA The term xA denotes the ratio of identical attributes between an existing concept C of the domain ontology and a new instance I, and is calculated by Eq. (2): xA 1⁄4 the number of identical attributes in C and I the number of attributes in C ð2Þ This fuzzy variable defines two linguistic terms, A_Low and A_High. In this study, the trapezoidal function, shown in Eq. (3), is adopted as the membership function of linguistic terms and can be expressed as the parameter set [a, b, c, d]. For instance, the membership functions A_Low and A_High, can be denoted as [0, 0, 0, 0.6] and [0, 0.6, 1, 1], respectively. or instance, in Fig. 5(a), the concept ‘‘ (typhoon)’’ has four attributes in the domain ontology, namely ‘‘ (position)’’, ‘‘ (direction)’’, ‘‘ (speed)’’ and ‘‘ (rainfall)’’. Fig. 5(b) indicates that the number of attributes of the new instance ‘‘ (Zimkulu)’’ is 6, and the attributes are ‘‘ (position)’’, ‘‘ (direction)’’, ‘‘ (speed)’’, ‘‘ (rainfall)’’, ‘‘ (path)’’, and ‘‘ (strength)’’. The concept ‘‘ (typhoon)’’ and the new instance ‘‘ (Zimkulu)’’ thus has four identical attributes between them, namely ‘‘ (position)’’, ‘‘ (direction)’’, ‘‘ (speed)’’ and ‘‘ (rainfall).’’ Hence, the value of this fuzzy variable xA 1⁄4 4 1⁄4 1. 4 B. Resonance strength in operation xO The term xO denotes the ratio of identical operations between an existing concept C of the domain ontology and a new instance I, and is calculated by Eq. (4): the number of identical operations in C and I ð4Þ xO 1⁄4 the number of operations in C The membership functions of xO are the same as the fuzzy variable xA, and the two linguistic terms O_Low and O_High, can be represented as [0, 0, 0, 0.6] and [0, 0.6, 1, 1], respectively. Fig. 5 indicates that the number of operations of the concept ‘‘ (typhoon)’’ in the domain ontology is 2. The number of identical operations between the concept ‘‘ (typhoon)’’ and the new instance ‘‘ this fuzzy variable xO is 1 1⁄4 0:5. 2 559 (Zimkulu)’’ is 1. Therefore, the value of C. Resonance strength in association domain xD The term xD denotes the ratio of identical association domain between an existing concept C of the domain ontology and a new instance I, and is calculated by Eq. (5): xD 1⁄4 the number of identical association domain in C and I the number of association domain in C ð5Þ An association domain is one in which the arrowhead of associations is toward other concepts or instances, such as the association among ‘‘ (bring),’’ ‘‘ (influence)’’ and ‘‘ (approach)’’ in Fig. 5. This fuzzy variable defines three linguistic terms, namely D_Low, D_Medium and D_High, whose membership functions can be represented as [0, 0, 0, 0.3], [0, 0.3, 0.3, 0.5], and [0.3, 0.5, 1, 1], respectively. As shown in Fig. 5, the number of the association domain of the concept ‘‘ (typhoon)’’ in the domain ontology is 3. The number of the identical association domain between the concept ‘‘ (typhoon)’’ and the instance ‘‘ (Zimkulu)’’ is 2. Therefore, the value of this fuzzy variable xD is 2 1⁄4 0:67. 3 D. Resonance strength in association range xR Term xR denotes the ratio of identical association range between an existing concept C of domain ontology and a new instance I, and is calculated by Eq. (6): xR 1⁄4 the number of identical association range in C and I the number of association range in C ð6Þ The association range indicates that the arrowhead of associations is toward its own concept, such as associations ‘‘ (become)’’ and ‘‘ (meet)’’ in Fig. 5. The fuzzy variables xR and xD have the same membership functions, and also have three linguistic terms, R_Low, R_Medium and R_High, which are represented as [0, 0, 0, 0.3], [0, 0.3, 0.3, 0.5] and [0.3, 0.5, 1, 1], respectively. As shown in Fig. 5, the number of association range of the concept ‘‘ (typhoon)’’ in the domain ontology is 2. The number of the identical association range between the concept ‘‘ (typhoon)’’ and the instance ‘‘ (Zimkulu)’’ is 1. Therefore, the value of this fuzzy variable xR 1⁄4 1 1⁄4 0:5. 2 3.2. A parallel fuzzy inference mechanism for conceptual resonance computing After describing the four fuzzy variables for calculating the conceptual resonance between an existing concept and a new instance, the parallel fuzzy inference architecture proposed by Lee et al. [25], Kuo et al. [7] and Lin and Lee [10] is used in this study. The structure comprises the premise layer, rule layer and conclusion layer. The model has two classes of node, fuzzy linguistic nodes and rule nodes. A fuzzy linguistic node denotes a fuzzy variable, and manipulates information related to a linguistic variable. A rule node denotes a rule, and determines the final firing strength of the rule during inference. The premise layer performs the first inference step to calculate the matching degrees. The conclusion layer is responsible for making conclusions and defuzzification. Each layer is described here in detail. A. Premise layer The first layer, called the premise layer, represents the premise part of the fuzzy inference system. Each fuzzy variable appearing in the premise part is represented by a condition node. Each output of the condition node is connected to some nodes in the second layer, forming a condition specified in some rules. The premise layer performs the first inference step to calculate degrees of matching. The input vector is given by x = (x1, x2, . . . , xn), where xi denotes the input value of linguistic node i. The output vector of the premise layer is thus l1 1⁄4 ððu1 ; u1 ; . . . ; u1 1 1 Þ; ðu1 ; u1 ; . . . ; u1 2 2 Þ; . . . ; ðu1 ; u1 ; . . . ; u1 n n ÞÞ 11 21 N 12 22 N 1n 2n N ð7Þ where u1 denotes the matching degree of linguistic term j in condition node i. The membership degree u1 of the ij ij four fuzzy variables can be calculated by their membership functions described in Section 3.1. B. Rule layer Each node in the second layer, called the rule layer, is a rule node denoting a fuzzy rule. The links in this layer perform precondition matching of fuzzy logic rules, and the output of a rule node in the rule layer is linked with associated linguistic nodes in the third layer. In the proposed model, the rules are defined by the domain expert and are listed in Table 3. In the rule node, function fr provides the net input for this node as in Eq. (8): fr 1⁄4 N X ð8Þ li i1⁄41 Hence, this value fr is not in a fixed range between 0 and 1. Therefore, a normalizing function S is adopted in this study. Function S is calculated by Eq. (9): 8 x<a > 0; > > xÀa 2 < 2ð Þ ; a 6 x 6 aþb bÀa 2 ð9Þ Sðx : a; bÞ 1⁄4 xÀb 2 > > 1 À 2ðbÀa Þ ; aþb 6 x < b 2 > : 1; xPb C. Conclusion layer The third layer is called the conclusion layer, and also is composed of a set of fuzzy linguistic nodes. The output fuzzy variable in the proposed system is the conceptual resonance strength yCRS, which denotes the conceptual resonance strength between an existing concept and a new instance. The four linguistic terms CRS_Low, CRS_Mediun_Low, CRS_Mediun_High, and CRS_High can be defined in [0, 0, 0, 0.3], [0, 0.3, 0.3, 0.6], [0.3, 0.6, 0.6, 0.9], and [0.6, 0.9, 1, 1], respectively. Defuzzification may be needed at the end of the inference process. The final output y of the proposed approach is the crisp value produced by combining all inference results with their firing strength. Eq. (10) presents the defuzzification formula: Pr Pc k k i1⁄41 j1⁄41 y ij wij V ij y 1⁄4 Pr Pc k k ð10Þ y w i1⁄41 Pn 1 j1⁄41 ij ij l where wk 1⁄4 i1⁄41 i ; Vij denotes the center of gravity; r denotes the numbers of corresponding rule nodes; c den notes the number of linguistic terms in the output node; n denotes the numbers of the fuzzy variable in the premise layer, and k denotes the current layer number. The values of r, c, n, and k adopted in this study are 36, 4, 4, and 2, respectively. Table 4 shows an example of the conceptual resonance between a new instance ‘‘ (Zimkulu)’’ and existing concepts ‘‘ (airstream)’’, ‘‘ (rain)’’, ‘‘ (typhoon)’’ and ‘‘ (calamity)’’. The value of conceptual resonance between ‘‘ (Zimkulu)’’ and ‘‘ (typhoon)’’ is the highest, so the inference mechanism infers that the new instance ‘‘ (Zimkulu)’’ belongs to the concept ‘‘ (typhoon)’’. Therefore, a new instance belongs to an existing concept with the highest yCRS. If all yCRS values are smaller than the threshold, then the new instance must either generate a new concept or be discarded by the domain experts. 4. Experimental result This study adopted the Chinese 2002 FIFA (Federation Internationale de Football Association) World Cup news and the typhoon news as the domain data to construct the 2002 FIFA World Cup ontology and the typhoon ontology. In the experiments, the input data were divided into two parts, namely training and testing data. The experimental results for these two domains are listed herein. The 2002 FIFA World Cup domain had 879 documents, of which 440 were placed in the training data, and 439 in the testing data. Table 5 lists the Concept Clustering results by the SOM. Table 6 lists the results of the Attributes_Operations_Associations Extraction. The Min and Win are the minimum occurrence and window size, respectively. Table 7 lists the precisions of the Attributes_Operations_Associations Extraction judged by domain experts. Fig. 6 shows a part of the constructed 2002 FIFA World Cup ontology. The typhoon domain had a total of 185 documents, of which 93 documents were placed in the training data, and 92 in the test data. Table 8 lists the results of the Attributes_Operations_Associations Extraction. Table 9 shows the precision of the Attributes_Operations_Associations Extraction judged by domain experts. Experimental results indicate that the larger the training data set, the better the precision results for attributes, operations and associations. Although the result is not perfect, some intermediate results of this approach can help domain experts validate the domain ontology and discover further domain knowledge. 5. Conclusions and future work This study proposes an episode-based fuzzy inference mechanism to extract domain ontology from unstructured Chinese text documents. After finishing the domain ontology construction, the domain expert is required to validate and correct the generated ontology. Ontology editors and other toolkits are also necessary for the whole process of automated ontological construction and maintenance. Additionally, the SOM algorithm was adopted for concept clustering and defining taxonomic relationships. The attributes and operations of concepts can be extracted based on ontology construction episodes. The non-taxonomic relationships are also generated from episodes. Moreover, the fuzzy inference mechanism is adopted to obtain new instances for the ontology learning. Experimental results indicate that the proposed approach can successfully construct the Chinese text domain ontology. However, for some special cases, such as a domain with rapid changing terms and concepts or with complex semantics, it is seen as an area of application for automated ontology construction – consider for example the potential of automated ontology construction for query expansion in information retrieval in the news domain. In the opposite, if a very carefully and accurate designed ontology is needed, an ontology engineer might be quicker to do that by hand – for example, an ontology describing the functions and behaviors of an airplane might possibly be constructed manually rather than generated from the airplanes documentation. Future work will include efforts to improve the precision of the proposed method, and studying the learning mechanism for fuzzy inference rules. The proposed approach will also be applied to other languages with semantic corpus or semantic dictionaries such as CKIP. Acknowledgements The authors would like to thank the anonymous referees for their constructive and useful comments. This study is partially sponsored by Department of Industrial Technology, Ministry of Economic Affairs, R.O.C. under the grant 95-EC-17-A-02-S1-029 and partially supported by the National Science Council of Taiwan under the Grant NSC94-2213-E-024-006. 
Automatic generation of probabilistic relationships for improving schema matching. Schema matching is the problem of finding relationships among concepts across data sources that are heterogeneous in format and in structure. Starting from the ‘‘hidden meaning’’ associated with schema labels (i.e. class/attribute names), it is possible to discover lexical relationships among the elements of different schemata. In this work, we propose an automatic method aimed at discovering probabilistic lexical relationships in the environment of data integration ‘‘on the fly’’. Our method is based on a probabilistic lexical annotation technique, which automatically associates one or more meanings with schema elements w.r.t. a thesaurus/lexical resource. However, the accuracy of automatic lexical annotation methods on real-world schemata suffers from the abundance of non-dictionary words such as compound nouns and abbreviations. We address this problem by including a method to perform schema label normalization which increases the number of comparable labels. From the annotated schemata, we derive the probabilistic lexical relationships to be collected in the Probabilistic Common Thesaurus. The method is applied within the MOMIS data integration system but can easily be generalized to other data integration systems. 1. Introduction Schema matching is the problem of finding relationships among concepts across data sources that are heterogeneous in format and in structure. Schema matching is a critical step in many applications, including data integration, data warehousing, E-business, semantic query processing, peer data management and semantic web applications [20]. Flexible systems capable of identifying mappings in an automatic and dynamic way are increasingly in demand: schemata exhibit a significant evolution over time, due to changing market conditions and evolving user sophistication and needs. The manual identification of such mappings is time-consuming and tedious, and clearly impossible in the context of dynamic schema matching. In this work, we focus on schema matching in the context of dynamic data integration [7]. Dynamic data integration systems are systems where semantic mappings among schemata of different sources have to be identified on the fly with minimal human intervention or with no intervention at all (i.e. in a semiautomatic or automatic way). However, in performing automatic schema matching, several problems arise: (1) there is no uniform conceptualization of the schemata and the semantics of the information in the various sources involved; (2) schema elements can be ambiguous in their semantics and recognizing the meaning of schema elements can be difficult for designers themselves; (3) schema labels can be non-standard words: this occurs in particular when the schema contains abbreviations and acronyms (e.g. ‘‘QTY’’) or complex compound nouns (e.g. ‘‘PurchaseDeliveryAddress’’); (4) automatic schema matching is intrinsically uncertain and the more the information to be matched, the more difficult it becomes to determine an exact match [16]. For example, applications such as Google Base, the large number of sources present in the ‘‘deep web’’ and the tools used for processing biological data all require flexibility and the handling of uncertainty [30]. Based on our previous works [6,8], we propose a method for the automatic discovery of probabilistic lexical relationships, which represents the first step for a fully dynamic data integration system. Our method focuses on the extraction of the underlying lexical knowledge from the schemata. Schema elements of a data source (referred to as ‘‘terms’’ or ‘‘words’’ from now on) are automatically annotated according to the lexical reference database WordNet (WN) [33] by using WSD techniques. The strength of a thesaurus like WN is the presence of a wide network of semantic relationships among meanings. Its main weakness is that it does not cover different domains of knowledge with the same level of detail and that many domain dependent terms (called non-dictionary words) may not be present in it. Non-dictionary words include Compound Nouns (CNs), abbreviations and acronyms (these last two will from now on be referred to simply as ‘‘abbreviations’’). The result of automatic annotation is strongly affected by the presence of these non-dictionary words in the schemata. For this reason, our method includes a ‘‘normalization’’ phase to expand abbreviations and to semantically ‘‘annotate’’ non-dictionary CNs. In the following, we will refer to this method as schema label normalization. In our method, lexical annotation is performed by Probabilistic Word Sense Disambiguation (PWSD), an automatic algorithm that combines several WSD algorithms by using the Dempster–Shafer’s rule of combination [44]. For each schema element, the PWSD combines the output of several WSD algorithms and produces a probabilistic distribution on meanings. The main advantage of PWSD is its flexibility: it is possible to add or remove algorithms very easily; what is required is just that the output of the new WSD algorithm is a probabilistic distribution on meanings. By using the Dempster–Shafer’s rule of combination, PWSD is able to model the uncertainty of the WSD algorithms (i.e their ignorance). In this paper we focus and evaluate PWSD on pre-published WSD algorithms [8,5]; we slightly modified the algorithms to change their output in a probabilistic distribution on meanings. The probabilistic annotations, generated through PWSD, are used to derive probabilistic lexical relationships among local sources that are subsequently collected in the PCT (Probabilistic Common Thesaurus). In contrast with other approaches, we do not choose the ‘‘best’’ discovered relationships only but we compute all possible relationships among schemata and assign a probability value to each of them. This paper constitutes a significant advancement in the direction of a fully automatic schema matching system: our method increases schema matching precision by incorporating schema label normalization and uncertainty into the lexical relationship discovery process, where uncertainties are quantified as probabilities. In short, our contribution can be summarized as follows: 1. We introduce the notion of probabilistic word sense disambiguation applied on structured and semi-structured data sources, and we present the automatic PWSD algorithm. 2. We modify the schema label normalization method proposed in [45] (in particular for the treatment of CNs) and integrate it with PWSD. 3. We add the treatment of uncertainty on the lexical relationships and we propose an automatic probabilistic relationship discovery method (this paper substantially extends the traditional MOMIS relationship discovery process described in [4]). 4. We demonstrate the effectiveness of our method in preliminary experiments within the MOMIS data integration system. The paper is organized as follows: Section 2 gives a definition of the problem we address in this paper. In Section 3, we describe the architecture of the probabilistic relationship discovery method. Subsequently, in Section 4, we summarize the normalization method and describe the new contributions w.r.t. [45]. Section 5 and Section 6 describe, respectively, the PWSD algorithm and the generation of probabilistic relationships. Section 7 sketches out the evaluation of PWSD and the relationship discovery process in a real scenario, comparing the results with other WSD approaches. In Section 8, we discuss related work. Finally, in Section 9, we make some concluding remarks. 2. Problem definition In this Section, we formally define the semantics of our method for automatic generation of probabilistic relationships. Moreover, we introduce a running example used here to informally explain our model. For the sake of simplicity, we limit our discussion to the context of schema matching between two data sources, but our treatment can be generalized to N schemata. The goal of a schema mapping system is to identify the semantic relationships between two schemata. A powerful means to discover semantic relationships starts from lexical annotation. Definition 1 (Lexical Annotation). Lexical annotation of a schema element is the explicit assignment of its meanings w.r.t. a thesaurus. We define lexical annotation as the connection of a schema element with its meanings defined in a lexical resource. However, from now on, the paper will also make reference to the annotation of the label of a schema element. WN, the lexical resource we employ in this work, uses the term ‘‘synset’’ to refer to a meaning: in WN all terms are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept [33]. The synset is further clarified with a short defining gloss (i.e. definition and/or example sentences). Fig. 1 shows an example of two schemata to be integrated. Let us consider, for instance, the element ‘‘address’’ contained in the schema (b). In WN the noun ‘‘address’’ has eight different meanings, including very similar ones such as ‘‘written directions for finding some location; written on letters or packages that are to be delivered to that location’’ or ‘‘a sign in front of a house or business carrying the conventional form by which its location is described’’. In the literature, we found many WSD approaches that generate a single (forced) annotation for each word. However, in such cases choosing a given individual annotation would be difficult even for a human annotator: generating probabilistic annotations may be more useful and possibly the only solution that avoids losing semantic information. We enrich the concept of lexical annotation by adding the notion of uncertainty. Uncertainty is an intrinsic feature of automatic and semi-automatic processes and provides a quantitative indication of the quality of the result. In our method, uncertainty is qualified as probabilities, where the probabilities are values in the interval [0–1]. Definition 2 (Probabilistic Annotation). Let T be a schema and t be an element (class or attribute) 2 T. We define St 1⁄4 fs#1 , . . . ,s#n g as the set of all possible meanings of t w.r.t. a lexical resource (such as WN). The probabilistic annotation of t is the triple /T,t,At S, where At = {a1,y, ak} is the set of annotations associated with t. In particular, ai is defined as the couple ðs#i ,Pðs#i ÞÞ, where s#i 2 St is a selected meaning for the element t and Pðs#i Þ is the probability value, in the interval [0–1], assigned to this annotation (this probability indicates how well the meaning s#i represents the element t). The schemata shown in Fig. 1 contain many elements labeled with non-dictionary CNs (e.g. ‘‘CustomerName’’) and abbreviations (e.g. ‘‘PO’’ and ‘‘QTY’’). Non-dictionary words cannot be directly annotated, because they do not have an entry in WN. In order to perform automatic lexical annotation, schema label normalization is needed. Schema label normalization (also called linguistic normalization [20]) is the reduction of the label of a schema element to some standardized form that can be easily recognized. In our case, by label normalization we mean the process of abbreviation expansion and CN annotation through the creation of new WN meanings. Definition 3. Let CN be a non-dictionary compound noun composed of more words called constituents. The annotation of a non-dictionary compound noun is the task of creating a new WN synset starting from the meanings associated to its constituents. Definition 4. Let AB be an abbreviation (or short form). Abbreviation expansion is the task of finding a relevant expansion (long form) for the given abbreviation AB.2 Starting from the lexical annotation of schema labels it is possible to derive lexical relationships among elements on the basis of the semantic relationships defined in WN among their meanings. Definition 5 (Lexical relationship). Let T1 and T2 be two heterogeneous schemata, and the elements ti 2 T1 , tj 2 T2 . A lexical relationship is defined as the triple /ti ,tj ,RS where R defines the type of the relationship between ti and tj. The types of lexical relationship are: SYN (synonym-of): defined between two elements whose meanings are synonymous (it corresponds to a WN synonymy relationship), formally ti SYN tj iff ( s#w synonym of s#u where s#w is an annotation assigned to ti and s#u is an annotation assign to tj; BT (Broader Term): defined between two elements where the meaning of the first is more general than the meaning of the second (the opposite of BT is NT, Narrower Term), (it corresponds to a WN hypernymy/ hyponymy relationship), formally ti BT tj iff ( s#w hypernym of s#u where s#w is an annotation assigned to ti and s#u is an annotation assign to tj; RT (Related Term): defined between two elements whose meanings are related in a meronymy hierarchy (it corresponds to a WN meronymy relationship, i.e. part-of relationship), formally ti RT tj iff ( s#w meronym of s#u where s#w is an annotation assigned to ti and s#u is an annotation assign to tj. In our method, the uncertainty introduced during lexical annotation is propagated to the lexical relationship discovery process: for each relationship a value representing the probability of that relationship is computed. Definition 6 (Probabilistic Lexical Relationship). A probabilistic lexical relationship is defined as the couple ð/ti ,tj ,RS,PÞ where /ti ,tj ,RS is a lexical relationship between ti and tj of the type R, and P is the probability value, in the interval [0–1], associated to this relationship. Probabilistic lexical relationships are inter-schema relationships collected in the PCT. Moreover, the PCT contains intra-schema relationships called structural relationships. Definition 7 (Structural Relationship). Let T be a schema. A structural relationship is an extensional relationship defined as the triple /ti ,tj ,RS where ti ,tj 2 T, and R specifies a type of structural relationship between ti and tj. The types of structural relationship are: SYNEXT: ti is equivalent to tj iff extension(ti)= extension(tj); BTEXT: ti BTEXT tj (or ti subsumes tj) iff extensionðti Þ + extensionðtj Þ (the opposite of BTEXT is NTEXT). Structural relationships derive directly from the structure of schemata, so they are not affected by uncertainty. In order to insert them in the PCT, we define each structural relationship as an ‘‘ordinary relationship’’ that is described by a probability value equal to 1. Probabilistic lexical annotation, associated with schema label normalization, improves the schema relationship discovery process in two main ways. First, it reduces the number of discovered false positive/false negative relationships. Second, it is able to model the implicit uncertainty of lexical relationships. Definition 8. Let /ti ,tj ,RS be a lexical relationship. The triple is a false positive relationship, if the element ti is not related by R to the element tj. Definition 9. Let /ti ,tj ,RS be a lexical relationship. The triple is a false negative relationship if the element ti is related by R to the element tj , and the schema matching process does not return this relationship. For example, let us consider the two schema labels ‘‘CustomerName’’ and ‘‘CLIENTIDENTIFIER’’ in the sources ‘‘PurchaseOrder’’ and ‘‘PO’’, respectively (Fig. 1). Without CN normalization, we are forced to annotate the terms ‘‘Customer’’ and ‘‘Name’’, and ‘‘CLIENT’’ and ‘‘IDENTIFIER’’ separately. Then we would discover a SYN relationship between them (see Definition 5) because the terms ‘‘Customer’’ and ‘‘CLIENT’’ share the same WN meaning. However, this relationship is wrong, as these two CNs represent ‘‘semantically distant’’ schema elements. In this way, a false positive relationship is discovered.3 Let us now consider two corresponding schema labels: ‘‘amount’’ in the ‘‘PurchaseOrder’’ source and ‘‘QTY’’ (abbreviation for ‘‘quantity’’) in the ‘‘PO’’ source (Fig. 1). Without abbreviation expansion, we cannot identify the SYN relationship between the two elements. Another advantage is that we handle and model the uncertainty during the lexical relationship discovery process. Let us reconsider the previous example about the annotation of the schema element ‘‘ADDRESS’’. If we annotate this element without performing probabilistic annotation, we can choose the WN meaning ‘‘the place where a person or organization can be found or communicated with’’, and we discover a BT lexical relationships with the term ‘‘MailingAddress’’ in the schema (a), or we can choose the meaning ‘‘written directions for finding some location; written on letters or packages that are to be delivered to that location’’ and we discover a SYN lexical relationship with the term ‘‘destination’’ in the schema (a). In both cases, by selecting one meaning only we may miss the right relationships between this element and the other schema elements, thus obtaining false negative relationships. By using our method, we do not exclude a priori any of these lexical relationships, but we handle their uncertainty by associating to each of them a probability value depending on the previous probabilistic annotations. 3. Architecture This Section describes the overall architecture of our probabilistic relationship discovery method. We introduce our specific contributions in the context of this architecture, and describe the individual phases of the probabilistic relationship discovery method. As shown in Fig. 2, the process can be seen to comprise three main phases: Source schema extraction (Wrapping) enables our method to manage structured and semi-structured data sources. We need specialized software (wrappers) for logically converting the format of the data source schema into an internal object language ODLI3 (Fig. 2-A). ODLI3 is an extension of the standard ODMG-ODL [14] aimed at dealing with semistructured data sources and its semantics is based on description logic. This phase is not a new contribution of this work, but is performed by using the MOMIS wrappers [4]. The output of source schema extraction is the set of schemata to be integrated into the ODLI3 format. Lexical knowledge extraction: represents the core of our method. During this phase the lexical annotation process is performed and the semantics of schemata is made explicit by using WSD techniques. This phase includes two steps (Fig. 2-B): 1. Schema label normalization: the input of the normalization process are the ODLI3 schemata; normalization is performed for all the schema elements that do not appear as entries in the lexical resource WN. The normalization process is responsible for tokenization (parsing labels into single tokens), abbreviation expansion and CN annotation. The schema normalizer component interacts with the PWSD algorithm (for the annotation of CNs) and with the lexical resources WN and WND (WordNet Domains). WND is an extended version of WN in which each synset has been associated with one or more domain labels. WND has been proven a useful resource for WSD [21,8]. The output of the schema normalizer are the normalized schemata (i.e. the schemata where the abbreviations have been expanded and the CNs have been annotated). This step is described in more detail in Section 4. 2. Probabilistic word sense disambiguation: during this phase the PWSD algorithm performs automatic probabilistic lexical annotation of schema elements, by combining a set of WSD algorithms. PWSD associates a probability value to each annotation; this value shows the uncertainty of the annotation process. PWSD receives the normalized schemata as input, and provides the annotated schemata as output. In the annotated schemata, each schema element is associated with one or more probabilistic annotations. PWSD performs lexical annotation w.r.t. WN. This step is described in more detail in Section 5. PCT generation extends the lexical relationship discovery process of the MOMIS system [4] by including the treatment of uncertainty. PCT collects a set of intra and inter-schema relationships (Fig. 2-C). Starting from the annotated schemata, probabilistic lexical relationships are derived among schema elements on the basis of WN semantic relationships. Lexical relationships are collected in the PCT, together with the ordinary structural relationships that are directly derived by the structure of schemata. Moreover, the PCT contains inferred probabilistic relationships detected by means of subsumption computation, performed by using the description logics techniques of ODBTools [10]. This paper will not go into detail as regards the discovery of structural relationships. These can either be supplied by ODBTools or be added by the designer with any other tool. This phase is described in more detail in Section 6. 4. Schema label normalization In this Section, we describe how the schema normalization approach, presented in [45], has been integrated into our method. The schema label normalization method consists of three steps (see Fig. 3): (1) classification for normalization, (2) abbreviation expansion and (3) CN annotation. In the following, we summarize the classification for normalization and the abbreviation expansion steps, as they were not modified w.r.t. the work in [45]. We describe the last step in more detail as it was substantially modified in order to be integrated with PWSD. 4.1. Classification for normalization Classification for normalization consists in selecting labels that need to be normalized, tokenizing selected labels into separate words and then identifying CNs and abbreviations among them. CNs (e.g. ‘‘company name’’) and abbreviations (e.g. ‘‘GDP’’ standing for ‘‘Gross Domestic Product’’) having an entry in WN need no normalization. Moreover, on the basis of our manual inspection, we have identified a set of exceptions (standard schema abbreviations) that, although they have an entry in WN, are mostly used as abbreviations in the context of schemata (e.g. ‘‘id’’, which in WN is a concept in psychology, while in schemata is often used as a short form of ‘‘identifier’’). All such abbreviations are put into a ‘‘Standard Abbreviation Dictionary’’ and automatically identified for normalization. To select the labels that need to be normalized, we propose the following classification heuristic: Definition 10. A label has to be normalized if it occurs in the list of standard schema abbreviations or if it does not have an entry in WN. 4.2. Automatic abbreviation expansion Our method provides some utilities allowing the designer to manually specify the right expansion for each abbreviation. However, when the designer does not know the right expansion or the process has to be performed ‘‘on the fly’’, an automatic abbreviation expansion algorithm is needed. The problem of abbreviation expansion cannot be reduced to a simple substitution, as many abbreviations are ambiguous, i.e. the same abbreviation may refer to different concepts (e.g. ‘‘CC’’ may mean ‘‘Credit Card’’, ‘‘Country Club’’ or ‘‘Carbon Copy’’). The automatic abbreviation expansion algorithm involves two steps: (1) search for candidate long forms for the given short form; and (2) selection of the most appropriate long form from the set of candidate long forms. For each abbreviation, the algorithm queries four resources (User Abbreviation Dictionary, Online Dictionary, Context and Complementary Schemata) for candidate expansions and creates a list of all possible candidate long forms. The next step concerns the selection of the form that best expands the considered short form from the list of candidate long forms. To this end, a score is computed for each candidate long form associated with the target abbreviation. The candidate with the maximum score is chosen.4 The abbreviation expansion algorithm does not produce probabilistic annotations but returns the best one expansion for a given short form. The selected expansion is subsequently annotated by using PWSD. For further details we refer to [45]. 4.3. CN annotation The CN annotation method proposed in [45], has been modified in order to be integrated with PWSD. As in [45], we make use of the classification introduced in [39], where CNs are divided in four distinct categories: endocentric, exocentric, copulative and appositional. In this work, we consider only endocentric CNs. Definition 11. An Endocentric CN consists of a head (i.e. the categorical part that contains the basic meaning of the whole CN) and modifiers, which restrict this meaning. A CN exhibits a modifier-head structure with a sequence of nouns composed of a head noun and one or more modifiers, where the head noun always occurs after the modifiers. The constituents of endocentric CNs are two nouns (noun-noun) or an adjective plus a noun (adjective-noun), where the adjective derives from a noun (e.g. ‘‘Asian food’’, where the adjective ‘‘Asian’’ derives from the noun ‘‘Asia’’). Our restricted focus on endocentric CNs is motivated by the following observations: (1) the vast majority of CNs in schemata fall in the endocentric category; (2) endocentric CNs are the most common type of CNs in English; (3) exocentric and copulative CNs, which are normally written as individual words, are often present in a dictionary (e.g. ‘‘loudmouth’’, ‘‘sleepwalk’’, etc.); (4) appositional CNs are not very common in English and are less likely to be used as elements of a schema (e.g. ‘‘actor director’’, ‘‘maid servant’’, etc.). Moreover, we performed a set of tests in order to verify that endocentric CNs are also the main category of CNs in the context of structured and semi-structured data sources. These tests have been performed on several real data sources in different domains and formats (relational and XML): Financial, DBLP, TPC-H, GeneX, Mondial5, PurchaseOrder6. Our tests show that, on average, endocentric CNs account for 85% of the total number of CNs appearing in a given source. In the following, we will refer to endocentric CNs simply as CNs. We consider CNs composed of only two constituents, because CNs consisting of more than two words need to be constructed recursively by bracketing them into pairs of words and then interpreting each pair. The CN annotation algorithm includes four main phases: (1) disambiguation of CN constituents, (2) identification of redundant constituents, (3) CN interpretation via semantic relationships and (4) creation of new WN meanings for a CN. The phase that has been modified w.r.t. the method described in [45] is the CN constituent disambiguation phase. In [45], the CN annotation algorithm does not produce probabilistic annotations as it uses a nonprobabilistic WSD algorithm. In our work, the algorithm produces a set of probabilistic annotations, as the CN constituents are annotated by using PWSD. During the disambiguation of CN constituents, a syntactic analysis is first performed to identify the part of speech of each constituent. If a CN is not endocentric (noun–noun or adjective–noun), it is ignored. The next step is the disambiguation of the head and the modifier. During this phase, PWSD returns a set of probabilistic annotations for each CN constituent. For example, for the term ‘‘Delivery Company’’, the annotations returned by PWSD are ‘‘(Company#1, 0.50)’’, ‘‘(Company#3, 0.12)’’ for ‘‘Company’’ and ‘‘(Delivery#1, 0.34)’’ for ‘‘Delivery’’. In this case, to obtain all possible meanings for ‘‘Delivery Company’’, we combine all the annotations for the term ‘‘Company’’ with all the annotations for the term ‘‘Delivery’’. We obtain the following combined annotations: ‘‘(Company#1Delivery#1, 0.17)’’, and ‘‘(Company#3Delivery#1, 0.04)’’ where the probability values of the combined annotation is the product of the probability value of the individual annotations. To compute the probability of new CN meanings, we assume that the single probabilities are independent. This assumption does not usually hold. However, factoring out dependencies in WSD context is extremely difficult, as they are usually hidden7 [40]. When there is a high number of returned annotations, the size of the possible combined annotation set may be quite large, and incorrect CNs can be generated. To ensure that clearly incorrect CN annotations are not generated, we apply a threshold. In this case, by excluding the combined annotations whose probability is below the threshold of 0.1, we obtain only the annotation ‘‘(Company#1Delivery#1, 0.17)’’. For the next steps, i.e. redundant constituent identification, CN interpretation via semantic relationships and the creation of a new WN meaning for a CN, we refer to [45]. 5. Probabilistic lexical annotation Lexical Annotation is an enabling technology that can give an important contribution to the identification of relationships among schema elements in data integration and ontology matching scenarios. In particular, it can be performed by using WSD algorithms. However, to prove effective in the context of dynamic data integration, lexical annotation has to be computed by automatic techniques. Ensemble methods are becoming increasingly popular as they overcome the weaknesses of individual approaches. Combining WSD algorithms of a different nature can improve results of disambiguation on text data, as shown in NLP [36], and even on structured data, as we have proved in [8]. Different combination strategies can be applied, such as majority voting, probability mixture, rank-based combination, maximum entropy combination or probabilistic combination. We implemented a combined WSD algorithm; PWSD consists of a number of self-contained modules (the WSD algorithms) each producing a probability distribution on meanings. Most of the traditional WSD approaches perform an exact annotation, assigning only one meaning to a term. The exact annotation is simple, but it suffers from some limitations: It may be very difficult when a fine-grained lexical knowledge source such as WN (i.e. one with many meaning distinctions) is employed instead of a coarsegrained one; in this case, even in manual annotation there are words for which designers cannot choose one meaning only. In a lexical knowledge source there are cases of homonymy (i.e. identical words having unrelated meanings) and cases of polysemy (i.e. the presence of several meanings for a single word) and it would be better not to penalize algorithms as much for selecting a related meaning as for selecting a meaning which is entirely wrong. On structured and semi-structured data sources in particular, there is not as wide a context as on a text source. In addition, there are less words that concur to the definition of a concept (i.e. only classes and attributes). It is therefore challenging to select only one meaning for each schema element (an element in a schema encloses a wider meaning than a term in a sentence). In [42], Resnik and Yarowsky suggest that a measure based on cross-entropy or perplexity would allow for the case where a number of very fine-grained meanings are essentially correct. These measures are based on a WSD algorithm producing a probability distribution on meanings. This is the main reason for our choice of a probabilistic WSD algorithm, as it can assign a high probability to all close meanings rather than choosing one through a forced-choice method. However, the output of the algorithm cannot only be used directly by applications that deal with probability, it can also be converted into an individual meaning assignment (choosing the best annotation, i.e. the annotation with the highest probability value computed) and used by applications that deal with individual annotations. PWSD needs to satisfy the following requirements: The algorithm needs to produce a probability distribution (so, for example, majority voting is not suitable). The algorithm needs to cope with probability distribu tions over meanings and distributions over subsets of meanings (such as those produced by a WSD algorithm which assigns probabilities to a set of meanings and not to individual meanings). The algorithm must be able to deal with ignorance, which means modeling not only what a WSD algorithm knows but also what it does not know (i.e its uncertainty or ignorance). These conditions are satisfied by both the Dempster– Shafer theory [44] and Bayes’ theorem. However, several works [15,23] have pointed out that while Bayes’ theorem is most suited to problems where there are probabilities for all events, it is least suited to problems where there is partial or complete ignorance, and limited or conflicting information. The Dempster–Shafer theory, on the other hand, can model various types of partial ignorance and limited or conflicting evidence: it is a more flexible model than Bayes’ theorem. This ability to explicitly model the degree of ignorance makes the theory very appealing. It has been applied in NLP [27] to combine several WSD algorithms. In [41] a comparison is made between performances of the WSD system which combine information sources by employing Dempster–Shafer theory and performances where sources are combined through Bayes’ theorem and weighted interpolation. On a text corpus (i.e. the Senseval-28) both the Dempster–Shafer and the Bayes’ theorem combination methods outperform the linear interpolation combination. However, even though the Bayes’ theorem combination method eliminates a bias towards the most frequent meaning, it does not lead to a great improvement over the Dempster–Shafer combination method. Based on these motivations, we have opted for the Dempster–Shafer combination method. However, we think that the focus of our future work should compare our algorithm with other combination methods. 5.1. WSD algorithms The main feature of PWSD is its flexibility, as it is composed of multiple probabilistic algorithms (Fig. 4): such modularity is made possible by the application of Dempster–Shafer theory. All the WSD algorithms, we combine in PWSD, have been implemented ad hoc for structured data sources or by adapting previous WSD algorithms developed for text data. As we require that all of our algorithms produce a probability distribution (in order to be combined by PWSD), we have slightly modified the five WSD algorithms developed and tested in [8,5]: The Structural disambiguation algorithm tries to disambiguate source terms by exploiting the structural relationships extracted from the sources. It examines concepts that are related by a structural relationship and searches for a semantic relationship between the meanings associated to these concepts in the WN hierarchy [9]. The WordNet domain disambiguation algorithm tries to disambiguate terms by exploiting domain information supplied by WND. The use of WND overcomes one of the main problems with WN: its excessive granularity in distinguishing the different synsets that makes the WSD process non-trivial for many real applications [9]. The WordNet first sense heuristic selects the first WN meaning (that is, the most frequent sense) for a given term. It has been observed that it is quite difficult for a WSD algorithm to beat the WN first sense heuristic. The WN first sense heuristic provides a good default value for words which do not obviously have another meaning (from another module), and thus WN first sense heuristic forms a part of our PWSD. The Gloss similarity algorithm is based on mining the glosses (i.e. textual definitions) given for terms in an online dictionary (WN in our case). The method is inspired by Lesk’s method [29] and is based on maximizing the similarity of the meanings assigned to schema elements. The rationale of this method is that the glosses of the possible meanings for the elements in the ‘‘vicinity’’ of a given element t should contain more words related to a particular gloss of a meaning of t. For example, when disambiguating the element ‘‘Bank’’ in ‘‘DB1.Account’’(‘‘Bank’’, ‘‘Branch’’, ‘‘Number’’, ‘‘IBANcode’’), we can expect the glosses of ‘‘Account’’, ‘‘Branch’’, ‘‘Number’’ and ‘‘IBANcode’’ to collectively contain more terms related to the meaning of Bank as a financial institution than to its meaning as a hydraulic engineering artifact. The ‘‘vicinity’’ of an element t has been detected to be the set of all the elements of the data source where t is contained (or a subset of it, if it contains more than 200 elements). The Iterative gloss similarity algorithm [5] is an iterative relaxation labeling technique [24] based on the Markov Random Field theory. It consists in an iterative algorithm: the synsets of the elements are initially attributed by means of the Gloss Similarity Algorithm, and they are then refined iteratively by assuming that the meanings attributed to the other elements at the previous iteration are correct. At present, each WSD algorithm produces a set of probabilistic annotations for the target element, which are computed according to its confidence (in our case, the confidence of each algorithm is selected as the precision of the algorithm evaluated on a benchmark). As a matter of fact, not every algorithm finds a meaning to assign to each term. In addition, each algorithm may be more appropriate to certain situations, so its behavior is not 100% trustworthy. To model the uncertainty of the WSD algorithms, we use the confidence of the algorithm and its ignorance (that is, the complementary value of the confidence). As a case in point, let us consider the term ‘‘name’’. In WN we find six different meanings for ‘‘name’’ (name#1 ,name#2 , . . . ,name#6 ). Suppose we have to combine three algorithms that give different outputs (as shown in Fig. 5): WSD1 chooses a set of meanings formed by name#1 , name#2 ; WSD2 provides name#1 as the correct meaning; and WSD3 does not give any result. Since WSD1 has a 70% of confidence this means that the algorithm has a 30% of ignorance. The probabilistic distribution on meanings for WSD1 is a 70% assignment to (name#1 ,name#2 ), while for WSD2 is a 50% assignment to (name#1 ). What we want to obtain is a rate of confidence to be assigned to each possible meaning of the term under consideration. 5.2. The Dempster–Shafer theory The Dempster–Shafer theory of evidence [44,38] provides a mechanism for modeling and reasoning on uncertain information in a numerical way. The theory tacitly assumes that the probabilities being combined are independent, an assumption which does not usually hold. However, factoring out dependencies in general is extremely hard, as they are usually hidden [41]. In [1], the role of independence for classifier combination in Dempster–Shafer evidence theory has been studied. The paper has demonstrated that the independence of the classifiers should not be considered necessary in multiple classifier combinations using probabilistic evidence representation and Dempster’s rule of combination. As WSD can be viewed as a classification task (where the meanings are the classes, and an automatic classification method is used to assign each occurrence of a term to one or more classes), we can safely adopt the independence assumption on the set of WSD algorithms. The theory deals with the so-called frame of discernment, the set of base elements y in which we are interested (in our case, the set of all possible meanings for the term under consideration), and its power set 2y , which is the set of all subsets of the interesting elements (all the possible subsets of the set of possible meanings). The foundation of the Dempster–Shafer theory is a probability mass function m(Á) that assigns zero mass to an empty set and a value [0,1] to each element of 2y , the total mass distributed being 1 so that: We can define our belief in a subset A of the set of all propositions as the sum of all the probability masses that support its constituents: X BelðAÞ 1⁄4 mðBÞ BDA Each of our WSD algorithms will assign a belief mass to a meaning or a set of meanings for every term. We derive the belief mass function from the output and the confidence of the WSD algorithms. Combining the outputs of several WSD algorithms means combining more probability assignments, then it is necessary to use the Dempster–Shafer’s rule of combination: where n is the number of the WSD algorithms that supplied a disambiguation output for the term under analysis. To obtain the probability assigned to independent meaning, we eventually need to smooth the belief mass function concerning a set of meanings. The following provides an example of how the PWSD algorithm is applied. As shown in Fig. 5, source elements are automatically annotated by the application of a set of WSD algorithms. PWSD combines the outputs without considering the algorithms that do not supply any annotation for the element. So, in this case, the evaluation will only be executed on the outputs of WSD1 and WSD2. If a WSD algorithm has a 70% confidence this means that the algorithm has a 30% ignorance. The application of the Dempster–Shafer’s rule of combination is shown in Fig. 6. As WSD1 supplies a set composed of two meanings, the probability will be assigned to this set. The results obtained after the application of the Dempster–Shafer’s rule of combination show the probabilities assigned to different sets of meanings. In order to use this result for computing lexical relationships, we have to go back to the case of probabilities assigned to individual meanings. As shown in Fig. 7, the probability assigned to the set of meanings fname#1 ,name#2 g will be split in the single probabilities assigned to name#1 and name#2 . 6. PCT Generation Once we have obtained annotations for schema elements, we use these probability distributions over the set of possible meanings to infer probability distributions for lexical relationships among elements. The annotation output of PWSD is used to compute the lexical relationships that will be included in the PCT. Our method derives lexical relationships among local source elements from the semantic relationships defined in WN between meanings. It generates lexical relationships by using the following WN constructors: synonymy (similar relation) corresponds to a SYN relationship; hyponymy (sub-name relation) corresponds to an NT relationship; hypernymy (super-name relation) corresponds to a BT relationship; holonymy (whole-name relation) corresponds to an RT relationship; meronymy (part-name relation) corresponds to an RT relationship; correlation (two terms share the same hypernym) corresponds to a RT relationship. The PWSD algorithm associates a set of probabilistic meanings to a term in a source. So, a term t is described by the meaning t#i with a certain probability. Since all the provided meanings are included in the lexical resource WN, each of them is located within a network of lexical relationships. When we assign the meaning t#i to the term t, t will inherit the same lexical relationships that occur for the synset t#i within the WN relationships network. In a data integration scenario, we restrict to the subnetwork of relationships that branch off from t#i , in the context of the analysis of the sources to be integrated. From this sub-net of lexical relationships among meanings, lexical relationships among elements are derived. A probabilistic lexical relationship exists between two elements, if there exists a lexical relationship between their meanings in WN lexical database. The probability assigned to lexical relationships depends on the probability value of the meaning under consideration for an element. Thanks to the formula of the joint probability, the probability value associated to an relationship holding among t#i and s#j can be defined as PðRelðti ,sj ÞÞ 1⁄4 Pðti Þ Ã Pðsj Þ Collecting these probabilistic lexical relationships, we populate the PCT that already contains the structural relationships (Fig. 8). Probabilistic lexical ODLI3 relationships are collected in the PCT, as well as the ordinary structural relationships, that we extract from schemata by ODBTools [10] (however, structural relationships can be supplied by other tools or can be added by the designer). PCT is structured very much like an Associative Network, where nodes (class or attribute names) are connected through bidirectional relationships. Eventually, to minimize the introduction of errors, probabilistic relationships with a probability value under a certain threshold can be filtered. 7. Experimental results The prototype that implements our method has been developed within the MOMIS system in order to test our approach on real-world sources. Our evaluation goals are: (1) to evaluate the effectiveness of automatic lexical annotation, (2) to verify whether, by handling uncertainty during the lexical annotation phase, our method improves the lexical relationship discovery process, (3) to evaluate the performance and the computational complexity of our method. The evaluations have been executed on two test cases: the first is a set of three ontologies from the benchmark OAEI 20089; the second is composed of two relational schemata of the well-known Amalgam integration benchmark for bibliographic data [34]. Details on the data sets are given in Table 1. To evaluate the effectiveness of the disambiguation results and the lexical relationship discovered, we use the metrics of precision, recall and F-measure. We compare the results obtained by our method with manually determined annotations/relationships. We determine the true positives, i.e. correctly identified annotations/relationships (B), as well as the false positives (C) and the false negatives (A) [18]. Based on the cardinalities of these sets, the following quality measure are computed: Precision 1⁄4 jBj=jBj þjCj reflects predictions on reliability of the annotations/ relationships; Recall 1⁄4 jBj=jAj þjBj specifies the share of real annotations/relationships that is found; F-Measure 1⁄4 2 Ã Precision Ã Recall=Precision þ Recall provides a combined measure of precision and recall. 7.1. Lexical annotation evaluation In this Section, we evaluate both the effectiveness of our normalization method, and the effectiveness of the automatic annotation performed by PWSD. We compare the results of our normalization method and PWSD with those obtained by an expert (i.e. a domain expert that knows the sources and their characteristics). The expert manually normalized each schema label and then associated one or more WN synsets to each element. The annotation results were evaluated in terms of the metrics described above. Let us suppose that for the element ‘‘name’’ the expert selected the synsets ‘‘name#1’’ and ‘‘name#2’’, while PWSD selected the synsets ‘‘name#1’’, ‘‘name#2’’ and ‘‘name#3’’. In this case, the precision of the automatic annotation is 0.66, the recall is 1 and the F-measure is 0.80. When the automatically normalized label does not correspond to the manually normalized one, the annotation is considered to be completely wrong (precision and recall equal 0). In order to discard incorrect annotations, we apply a threshold. We investigated the output of PWSD by varying the threshold between 0.05 and 0.4 in steps of 0.05. The optimal threshold value was found to be 0.2, which generated the results shown in Figs. 9 and 10. However, the results did not substantially deviate w.r.t those returned with other thresholds in the range. The importance of the normalization process is highlighted in Fig. 9, where we compare the results obtained by PWSD combined with the normalization process and PWSD without the normalization process. Without schema label normalization, the recall obtained by PWSD rapidly decreases, due to a high number of non-dictionary words contained in the schemata (as shown in Table 1). However, the recall value improvement for the OAEI ontologies is significantly smaller than the improvement for the Amalgam schemata. This result is due to the presence in these ontologies of several non-endocentric CNs such as ‘‘writtenBy’’, ‘‘publishedBy’’, ‘‘InProceeding’’ (also called ‘‘prepositional verbs’’ in the literature). In contrast with NLP and with our experience on relational and XML schemata, the endocentric CNs in these ontologies represent only 56% of all CNs present in the sources. This result does not exclude the possibility of using normalization for ontologies, since the recall value is improved anyway, but points to the necessity, especially for ontologies, to study a suitable method to annotate other kinds of multi-word labels.10 In Fig. 10, we compare our method w.r.t: (1) forced annotation, which associates only the best annotation to each term (i.e. the one with the highest probability value) and (2) WNFS (a traditional baseline to evaluate WSD algorithms [32]). The results show that PWSD outperforms both forced annotation and WNFS. By selecting more meanings for each term, the recall of PWSD outperforms the recall obtained by forced annotation and WNFS. Thus, even if the precision of PWSD decreases, the F-measure still dominates the one obtained by the other methods. 7.2. Lexical relationship discovery In this step of the evaluation, we only analyzed the relationships between two relational schemata of Amalgam and the relationships between two ontologies from OAEI (101, 209). We considered a limited set of schemata, as the process to manually determine lexical relationships is a very complex and time-consuming task, especially when more than two schemata are considered. The evaluation was focused on the performance of our method with and without schema label normalization and on the loss of information caused by a forced approach (i.e. an approach that only maintains the relationship between two schema elements that is characterized by the highest probability value). During the evaluation, a probabilistic lexical relationship was considered correct if it was present in the set of manually determined lexical relationships. For the evaluation of OAEI ontologies, we also considered the ontology alignments provided by OAEI, which we interpreted as SYN lexical relationships. We investigated the value of the best threshold by varying it between 0.05 and 0.3 in steps of 0.05. The optimal threshold was found to be 0.15 which generated the results in Figs. 11 and 12. However, as for the lexical annotation process, the results did not substantially deviate w.r.t those returned with other thresholds in the range. In Fig. 11, we compare the results of our own probabilistic lexical relationship discovery method with and without the normalization process on both the evaluation data sets (Amalgam and OAEI ontologies). Without schema label normalization, we discover few lexical relationships with low precision due to the presence of many false positive and false negative relationships. In particular, for the Amalgam schemata, where the majority of schema elements are labeled with CNs and abbreviations, we obtain very low values for both recall and precision. In Fig. 12, we compare the probabilistic lexical relationships discovered by our method with those discovered by using forced annotation and WNFS. With forced annotation and WNFS, the loss of information due to the annotation method is propagated to the lexical relationship discovery process. This caused a dramatic reduction in performance in terms of precision, recall and F-measure. On the contrary, by handling uncertainty during the lexical relationship discovery process, our method outperformed significantly both forced annotation and WNFS. We obtained very good results for the relationship discovery process on Amalgam schemata, while the recall on OAEI ontologies was quite low. In the 209 ontology of OAEI, we find several terms, such as ‘‘publishedBy’’ and ‘‘writtenBy’’, that should be associated by a SYN relationship with the terms of the 101 ontology ‘‘publisher’’ and ‘‘author’’. However, our method only found a RT relationship between these terms. On the Amalgam data set, we surprisingly obtained very good results for both precision and recall w.r.t lexical annotation. We discovered that the majority of terms wrongly annotated in the first Amalgam schema are not related to any term in the second Amalgam schema. As a consequence, the majority of wrong annotations do not affect the results of the lexical relationship discovery process. 7.3. Performance evaluation In order to evaluate the performance of our method, we have computed the average execution time of the whole probabilistic relationship discovery process and of each individual phase (schema label normalization, PWSD, lexical relationship discovery) for both the Amalgam and OAEI sources. The size of the data sets (146 elements for Amalgam, and 462 for OAEI ontologies) is sufficiently large to evaluate the running time of our method. It has been implemented using Java and the experiments have been carried on a PC compatible machine, with Intel Core Duo Processor (2.00 GHz), 2 GB RAM, running Windows Vista and JRE 1.6. Table 2 shows the average running times of our method and its phases. As it can be seen, the total average time to discover lexical relationships is reasonably low. Moreover, the results in Table 2 suggest that the execution time depends on the number of schema elements: the time needed to process OAEI schemata is higher than that for Amalgam as they contain more than three times the Amalgam schema elements. In general, the performance and execution time of our method may be affected by the following factors: Number of schema elements. The number of schema elements represents the number of labels that have to be processed by our method. As a consequence, the greater the number of elements, the higher the time needed to complete the lexical relationship discovery process. Number of non-dictionary words and their complexity. Our method is able to directly annotate all the labels that exist in WN. All the other labels need to be normalized. Thus, the greater the number of nondictionary words in the schema, the greater the time needed to process them. Moreover, the running time increases with the complexity of the labels (e.g. CNs composed by more than two words or complex abbreviations). Polysemy of the labels. The polysemy of a label indicates the number of possible meanings for the given label in WN. The more synsets exist for a label in WN, the longer PWSD will take during the WSD process for the identification of the probabilistic annotations. The computational complexity of our method can be expressed as OðNormalizationðnÞ þ PWSDðnÞ þ ProbabilisticRelationshipsðnÞÞ where n is the number of source schema elements, Normalization(n) is the cost of the normalization process, PWSD(n) is the cost of the WSD algorithms plus the cost of the Dempster–Shafer’s theory of combination and finally, ProbabilisticRelationships(n) is the cost of the lexical relationship discovery process. The asymptotic complexities of the normalization process (equal to O(n2)), of the WSD algorithms (equal to O(n4)) and of the lexical relationship discovery process (O(n2)) are polynomial. The asymptotic running time of the whole our method is determined by the implementation of the Dempster– Shafer’s theory of combination that, as sketched in [44], is (at worst) exponential (since the problem has been proved be #P-complete11 in [37]). However, the running times in Table 2 show that our method has performed well for the considered data sets. In [47] exact and approximate methods to reduce the Dempster–Shafer’s theory complexity have been proposed. The study and the implementation of these approximate methods represent an interesting future line of research to enable our method to deal with very large data sets. 8. Related work Works related to the issues discussed in this paper are in the area of schema normalization, disambiguation techniques and probabilistic systems. 8.1. Schema normalization The problem of linguistic normalization has received much attention in different areas such as machine translation, information extraction and information retrieval. As already observed, the presence of multi-word labels (including CNs) and abbreviations in schema elements may affect the quality of schema matching and requires additional techniques of disambiguation [17]. Surprisingly, current schema matching systems either do not consider the problem of abbreviation expansion at all or solve it in a non-scalable way by including a user-defined abbreviation dictionary or by using simple string comparison techniques only. Both the well-known CUPID [31] and COMA/COMA++ [2] schema matching systems rely on the availability of a complete user-dictionary or a tool for abbreviation expansion. Dealing with abbreviations using an abbreviation dictionary suffers from a lack of scalability: (a) the vocabulary of a domain evolves over time and it is necessary to update the table of abbreviations used in the domain, and (b) the same abbreviations can have different expansions depending on the domain. Moreover, the corpus-based normalization approach still requires the intervention of a schema/domain expert. Similarly, in the context of data integration and schema mapping only few papers address the problem of non-dictionary CN. In [46] a preliminary CN comparison for ontology mapping is proposed. This approach has two main limitations: first, it starts from the assumption that the ontology entities are accompanied with comments that contain words expressing the relationship between the constituents of a CN; second, it is based on a set of manually created rules. The well-known CUPID algorithm [31] considers elements such as abbreviations and punctuation but not CNs. Other schema and ontology matching tools employing syntactical matching techniques do not normalize CNs but they treat constituents separately [28]. This oversimplification leads to the discovery of false positive relationships, thus worsening the matching results. 8.2. Disambiguation techniques Several language-based methods have been experimented in the context of ontology matching and data integration. H-MATCH [13] makes use of linguistic and contextual features of OWL ontologies to calculate an affinity value between two concepts. CUPID [31] implements an algorithm comprising linguistic and structural schema matching techniques, and computing similarity coefficients with the assistance of domain specific thesauri. Falcon-AO [25], a system for matching OWL ontologies, is made of two components, one for performing linguistic matching and the other for performing structure matching. Some methods rely on algorithms only (intrinsic methods), while other methods make use of external resources such as dictionaries (extrinsic methods). Intrinsic methods produce a linguistic normalization of entities in order to represent ontology entities as set of words that can be compared by string-based techniques. Extrinsic methods exploit external resources to find similarities between terms. However, they open new possible matches between entities because they recognize that two terms can denote the same concept. Unfortunately, since they recognize that the same term may denote several concepts at once, these techniques provide many possible matches. Unlike these methods, our approach is based first of all on the lexical annotation of ontology/schema elements. It is only after this phase that the similarity between elements is computed, thus overcoming the limitation of extrinsic methods that cannot recognize the meaning of the elements. To the best of our knowledge, [3] is the first work that introduces WSD techniques in an integration process. In that paper, WSD is presented as the first step in an ontology integration process. The paper presents an approach to automatically disambiguate the meaning of OWL ontology classes by providing lexical annotation from WN. The approach associates a WN synset to an ontology class and defines an affinity coefficient. One of the main limitations of this approach is that it does not make use of normalization techniques to process CNs, and this is reflected in a low coverage of the method. Indeed, the disambiguation techniques are not able to annotate most of the ontology elements labeled with nondictionary words (as shown in the evaluation Section of the paper). Moreover, the method has been proposed in the context of ontology matching and the two disambiguation techniques developed are focused on ontology classes, which means that the method is not applicable to schemata. In the area of NLP, where WSD is a challenging topic, combination methods have been shown to be an effective way of improving WSD performance. In [11] an evaluation study on different combination of WSD algorithms is presented, and it is shown that combination systems outperform the behavior of the individual algorithms of which they are composed. Preiss [40] proposes a combination of probabilistic WSD algorithms based on Bayes’ theorem, demonstrating that it is a strong competitor to state-of-the art WSD algorithms. 8.3. Probabilistic systems Modeling uncertainty in probabilistic schema matching has been an active area of research for some years [35]. Our method takes inspiration from [43,19], where the concept of probabilistic schema mapping is introduced and an algorithm for uncertain query answering is presented. This paper starts from initial probabilistic schema mappings, and without dealing with the generation of probabilistic mappings, proposes a probabilistic query answering method. The paper describes the requirements of a data integration system to support uncertainty; the authors maintain that data integration systems need to handle uncertainty at three levels: uncertain schema mappings, uncertain data and uncertain queries. In the paper, probabilistic mediated schemata are formally defined and the problem of generating a probabilistic mapping between a source schema and a mediated schema is tackled. A mapping is constructed from a set of weighted attribute correspondences between a source schema and the mediated schema. The goal of our paper is the generation of a set of probabilistic (and ordinary) relationships that can be assimilated to weighted correspondences and represent the first step in calculating a set of probabilistic mappings aimed at generating a probabilistic mediated schema. In the literature many tools for automatic ontology mapping are offered, but only a few use a probabilistic approach. Some authors working on ontology matching have proposed a method to resolve semantic ambiguity in order to filter the appropriate mappings between different ontologies [22]. The limitation of this method is that it does not disambiguate the labels of the ontology classes, but only evaluates the possible meanings. In [12] a method for discovering schema mappings, based on the lexical relationships extracted from WN, is proposed. However, since it considers all the synsets associated to a term by WN, this approach does not realize any sort of disambiguation. The main disadvantage is the inclusion of wrong synsets and therefore the extraction of lexical relationships that can define erroneous mappings. For these reason, we propose a probabilistic WSD algorithm that ensures that more accurate relationships are identified. 9. Conclusion and future work We have presented a method which is able to generate probabilistic lexical relationships in the context of data integration. The approach is based on three steps: (1) the normalization of schema labels in order to expand abbreviations and to deal with non-dictionary CNs, (2) the lexical annotation of schema elements, performed by a probabilistic algorithm (PWSD), and (3) the discovery of probabilistic lexical relationships among elements. The approach has been evaluated on two different data sets: three ontologies from the OAEI ontology alignment benchmark and two relational schemata from the Amalgam data integration benchmark. The experimental results show the effectiveness of our method, which significantly improves the results of the automatic lexical annotation process and, as a consequence, also improves the quality of the discovered inter-schema lexical relationships. We demonstrated that, due to the frequency and productivity of nondictionary words, it is not possible for a data integration system to ignore CNs and abbreviations without compromising recall. Moreover, we verified that the handling of uncertainty during the lexical annotation processes is beneficial and that, in complex integration problems, the information loss caused by the removal of uncertainty leads to a worsening of the schema relationship discovery process. Our method can find application in the context of mapping discovery, ontology merging and data integration systems. Our method is currently integrated in the MOMIS system. Future work will be devoted to developing a stand-alone tool and improving the normalization method by studying: (1) other kinds of multi-word units (e.g. prepositional verbs such as ‘‘writtenBy’’); (2) the use of conjunctions (such as ‘‘and’’ or ‘‘or’’) in schema and ontologies labels; (3) a different kind of non-dictionary words, i.e. words which are not present in a lexical resource as they belong to a specific application domain (e.g. medicine, architecture or biology). We will also investigate an approach for clustering uncertain data by exploiting the probabilistic relationships discovered by the approach illustrated here. The problem of clustering is a well-known and important one in the data mining and management communities. The incorporation of uncertainty into the clustering techniques can significantly affect the quality of the underlying results as shown in [26]. Another future line of research regards the refinement of the relationships discovered. All the probabilistic relationships discovered by our method have a probability value that is in the range 0–1; within that range, we can distinguish between ‘‘strong’’ relationships and ‘‘uncertain’’ relationships (i.e. relationships with a low probability value). Uncertain relationships could be seen as candidate relationships that need confirmation. In this scenario, instance-based techniques could prove useful in approving or rejecting some of the previously determined candidate relationships, thus improving accuracy. Acknowledgements We thank S. Bergamaschi, A. Corni and M. Gawinecki for their help with the work reported on this paper. This work was funded by the ‘‘Searching for a needle in mountains of data!’’ project funded by the Fondazione Cassa di Risparmio di Modena within the Bando di Ricerca Internazionale 2008 (http://www.dbgroup.unimo.it/key mantic) and by the MIUR FIRB Network Peer for Business project (http://www.dbgroup.unimo.it/nep4b). 
Enabling the development of base domain ontology through extraction of knowledge from engineering domain handbooks. Domain ontology, encompassing both concepts and instances, along with their relations and properties, is a new medium for the storage and propagation of domain specific knowledge. A significant problem remains the effort which must be expended during ontology construction. This involves collecting the domain-related vocabularies, developing the domain concept hierarchy, and defining the properties of each concept and the relationships between concepts. Recently several engineering handbooks have described detailed domain knowledge by organizing the knowledge into categories, sections, and chapters with indices in the appendix. This paper proposes the extraction of concepts, instances, and relationships from a handbook of a specific domain to quickly construct base domain ontology as a good starting point for expediting the development process of more comprehensive domain ontology. The extracted information can also be reorganized and converted into web ontology language format to represent the base domain ontology. The generation of a base domain ontology from an Earthquake Engineering Handbook is used to illustrate the proposed approach. In addition, quality evaluation of the extracted base ontology is performed and discussed. 1. Introduction Gruber [1] defined ontology as, ‘‘a formal, explicit specification of conceptualization.” The ‘‘conceptualization” described in this definition refers, in simple and logical terms, to the overarching worldly domain. Issa and Mutis [2] explained, ‘‘Conceptualizations are described by a set of informal rules used to express the intended meaning through a set of domain relations.” Noy and McGuinness [3] indicated how the domain ontology could facilitate shared understanding of the structure of information among people or software agents, which in turn would stimulate the reuse of domain knowledge. From the perspective of knowledge representation, Issa and Arciszewski [4] defined ontology as ‘‘a knowledge representation in which the terminologies have been structured to capture the concepts being represented precisely enough to be processed and interpreted by people and machines without any ambiguity.” Therefore, computer programs could follow the specification of concepts so that responses could automatically take their proper course. Besides knowledge representation and sharing, ontology also plays an important role in knowledge engineering, Russell and Norvig [5] indicated that ‘‘ontological engineering”, translating important domain-level concepts into logic-level names, is an important stage in knowledge engineering. Several researchers have presented ontology applications in the field of intelligent agents or knowledge-based systems [6–13]. Research on the application of ontology to knowledge management ratifies it as a feasible and efficient solution [14,15]. Many domains have developed domain ontologies tailored to suit their specific requirements. For example, domains such as biology, medicine and pharmaceutics have each produced several comprehensive ontologies to present genetic knowledge [16], inclusive of research related to protein [17], biomedicine [18] and so on. SWEET is an earth sciences ontology developed by NASA, which is widely used in many projects [19]. The engineering design ontology also has been developed and utilized [20]. Within the civil engineering domain, the e-COGNOS project successfully presented the ontology application of knowledge management and information retrieval [21]. Several construction domain ontologies have also been developed for different applicatory purposes, such as knowledge management [22–24], interoperability among different software systems (e.g. CAD and GIS system) [25], and project collaboration [26]. Although few researches or projects focused on developing a civil engineering domain ontology, there are ongoing investigations of ontology applications [27–29]. Prior to investing in applied ontology research, an appropriate ontology for utilization must be developed first. Various methodologies for developing domain ontologies have been proposed. Rezgui [30] used information retrieval techniques to extend domain ontologies with semantic resources, and proposed an iterative approach to maintain an existing ontology. Chu et al. [31] illustrated how to build domain ontologies from taxonomies. However, many of these methodologies still have the room for refinement, or they may be complementary to one another. For example, the methodology proposed by Rezgui [30] needs an existing ontology to be the ‘‘Core Ontology” and the methodology proposed by Chu et al. [31] demands expert intervention for defining concept relations before normalization. Since the procedure of ontology building should be iterative, a base ontology, which is defined here as a good starting draft of ontology, will be quite useful to kick start the process. Although the base ontology might be incomplete in some aspects but could facilitate the ontology development process with incremental improvements. Therefore, the methodology presented in this paper will nicely complement the methodologies that are positioned to make a domain ontology more complete. Fig 1 shows a general procedure that the authors summarized from two well-known exemplars. The left area of Fig. 1 features a skeletal methodology proposed by Uschold and Gruninger [32] for building ontology in programming applications. Fig. 1 illustrates this methodology’s partitioning of the developmental procedure into three parts. Noy et al. [33] suggested an iterative procedure to design and implement a domain ontology using their ontology editor called Protégé. The authors reorganized procedure in [33] by merging comparable operational steps. The reorganized procedure is presented in the right hand area of Fig. 1. Both procedures imply that the domain scope should be defined first. Class functions and the coding format are integral features of the software programming aspect of ontology application. Hence it may be surmised that in previous attempts to build a domain ontology, the major objective was the definition of concepts/class, entities/ instances and relations/hierarchy. Another significant finding of these researchers was that ontology development should be an iterative process in order to keep up with a changing world. The participation of domain experts is also integral to the development of domain ontologies. The iterative procedure, from enumerating important terms and defining concepts, to identifying hierarchy and relations, requires an intensive effort on the part of participating experts with respect to discussion and subsequent formulation of revisions. Mindful of this fact, the authors were motivated to create an approach to ontology development that could reduce the burden on participating experts. While an increasing number of civil and infrastructure engineering specifications or information standards have been established to describe the structure components and construction contracts, they also serve as useful resources for ontology development by providing detailed domain knowledge. Recognizing the potential of these resources and the limited amount of research in the engineering domain (especially civil engineering) concerned with ontology creation, the authors propose an ontology development solution using domain handbooks. Published engineering handbooks are in abundance and searching the keyword ‘‘handbook” in Amazon returns more than 175,000 book titles in the general domain, while a search of the subcategory ‘‘Engineering - Civil” lists around 3000 books. These handbooks, replete with domain knowledge, were obviously edited by experienced domain experts. Each volume not only records the domain knowledge but also provides more professional content than web pages. This observation inspired the authors to come up with a rapid, straight forward procedure for constructing the base domain ontology, by reusing the domain handbooks as knowledge resources. An immediate challenge for the proposed procedure was how to competently handle the large number of text documents featured in a handbook. Digitizing the complete contents of domain handbooks is too laborious and not feasible if the digital version of the handbook cannot be acquired. It was decided that the process could be expedited if an ontology could be generated from only the table of contents, definitions and index of a domain handbook. The approach is innovative in that it not only takes advantage of existing resources, (i.e. extracting knowledge from domain handbooks), but also reduces the workload of participating experts, who are often consulted to define the complex relationships between concepts and instances during ontology development. This paper proposes a practical approach from engineers’ point of view to establish a general draft (or base) of a domain ontology from an engineering domain handbook which defines the scope and knowledge representation space of the targeted domain. The remainder of this paper is organized as follows. In the second section, the authors present the proposed ontology development procedure and elaborate on the details of each development step. To demonstrate how the proposed procedure could be applied to develop a base domain ontology, an example of developing the earthquake engineering base ontology using the ‘Earthquake Engineering Handbook’ [30] is also provided in the second section. The third section discusses the advantages and shortcomings of the proposed method by evaluating the consistency and completeness of the developed ontology. Implications for future research, along with overall conclusions regarding the problems at hand are discussed in the final section. 2. The approach The main objective of the present approach is to produce a good base domain ontology that, in practical use, can reduce the workload of ontology development for participating experts. Fig 2 illustrates the overall semi-automatic procedure for constructing a base domain ontology from domain handbooks, and Table 1 summarizes the main task of each stage. A domain handbook consisting of the ‘‘Table of Contents (TOC)”, ‘‘Definition” or description of domain-specific terms and the ‘‘Index” of these terms are digitized (if the digital version of the handbook is not available) to provide the initial input for the ontology development. The proposed procedure is intended to extract important terms or phrases and build the complex relationships between them before expert assessment and evaluation. In the first step, these terms and definitions are collected and re-arranged to generate the domain glossary. Next, the ‘‘TOC” and the ‘‘Index” carrying the term hierarchy-related information are used to extract the relationships between these domain-specific terms. Ideally, the domain glossary and these related terms also provide the needed suggestion for experts defining the domain upper-level concepts in the third step. Terms extracted from the handbooks could further be classified into concepts and instances in step 4. In this research, concepts refer to the top-level domain classes, whereas an instance is either a bottom-level domain class or a specific object that could be derived from one concept with various properties. For example, ‘‘Bridge” is a top-level concept, while ‘‘cable-stayed bridge” and ‘‘Brooklyn Bridge” are two different instances of a bridge (i.e. a bottom-level class derived from the concept of ‘‘Bridge” and a specific bridge object, respectively), entailing their own specific shapes, construction methods and materials. In order to separate concepts and their instances from the glossary and term relationships, three weighting rules are used to make an initial judgment on whether a term is more likely a concept, or an instance, in step 4. Domain experts only participate in steps 3 and 4 for definition of the upper-level concepts along with the review and evaluation of the concepts and instances generated by the proposed weighting rules. In the last step, the glossary, relations, hierarchy, concepts and instances are all edited into a phrase map. The authors stored the terms, relationships and hierarchy information in XML format. These relationships and hierarchies could thereby be represented as a tree or net graph with the XML document, which could then be easily transformed into the final base domain ontology in web ontology language (OWL) format. Evaluation of the phrase map through the participation of domain experts is also considered an effective mechanism to obtain expert feedback. Throughout the evaluation process, experts score each relationship according to the intensity of term (or phrase) connectivity. The proposed approach is also an iterative procedure because the evaluation results could potentially provide feedback to assist with the revision of the generated ontology relationships. In order to verify the feasibility of the above development procedure, the authors developed a base ontology for the ‘‘Earthquake Engineering” domain. Experts from Taiwan’s National Center of Research on Earthquake Engineering (NCREE) were invited to participate in the evaluation phase of the ontology development procedure. The corresponding handbook – ‘‘Earthquake Engineering Handbook – New Directions in Civil Engineering” [34] – was chosen as the knowledge input for this work. Details of the processing procedures will be described in what follows. 2.1. Glossary development To facilitate computational data processing, as indicated by Step 1 in Fig. 2, all the necessary information was gathered and converted into digital format. The handbook was comprised of five technical sections, 34 chapters and 1512 pages. Each chapter is consigned to one of the five sections, including ‘‘Fundamentals”, ‘‘GeoScience Aspects”, ‘‘Structural Aspects”, ‘‘Infrastructure Aspects” and ‘‘Special Topics”. For example, ‘‘Reinforced Concrete Structure”, ‘‘Bridges” and ‘‘Structural Control” were chapters falling under the ‘‘Structural Aspects” section. It was evident that the TOC was used by the handbook editors as a main channel to advocate how they perceived the domain and the organization of domain concepts. Fig. 3 illustrates the upper-level concepts extracted from the TOC. Furthermore, each chapter consisting of basic theoretical discussions, engineering practices, research and state of the art technologies, was written by an internationally known expert in a corresponding sub-domain. Each chapter was supplemented with a glossary of between 2 and 20 domain-specific term definitions (see Fig. 4a). Domain related phrases, such as people, places, events and technologies, could then be extracted from these term definitions. Last, but not least, the handbook’s Index section, (see Fig. 4b for a portion of index ‘A’), assisted with the extraction of domain related phrases. To facilitate further operation and application of the extracted terms and phrases, the authors designed a simple XML structure for their storage and processing (see Fig. 4c). After the ‘‘Earthquake Engineering Handbook” was completely processed, 2880 phrases were collected; 679 phrases were drawn from term definitions and 2201 phrases from the handbook index. 2.2. Generating hierarchy and relationships Establishment of the domain glossary is the first stage in ontology development. In the second step, relationships between phrases of the domain glossary furnish a domain vocabulary, while also acting as a form of knowledge representation, including, for example, taxonomy [35] and inference [36]. The glossary collected from the first step contained phrases (or terms) featured in either the chapters or the index. In the former case, the terms or phrases from the chapters’ glossaries were used to identify the connections between terms and chapters. In the latter case, information carried by phrases and terms from the index was utilized to reference their related sections. Any attempt to judge the exact relationship between two phrases (or terms) was an inherently difficult and complicated enterprise. In order to alleviate this problem, with respect to the glossary phases, the authors strategically generalize these relationships into two types: the ‘‘is-a” relation and the ‘‘has-a” relation. The benefit of such generalization was that term relationships could be picked up quickly from the handbook in order to highlight the hierarchical structure between terms. These relationships still needed to be revised by consulting experts at a later stage to make the produced ‘‘taxonomy-like” ontology more complete. In this step, the authors only focused on storing the complicated relationships for further analysis and operation. For instance, our generalization strategy dictates that if A is a bridge and B is a pier, the relationship B is a ‘‘part-of” A, (i.e. ‘a pier is one part of a bridge’), can be simplified into either ‘‘A ‘has-a’ B”. Leveraging the concept of relationship generalization, the authors were then able to explore relationships from the handbook’s TOC, term definitions and indices. After extracting and accumulating all three relationship components, the terms and relationships in combination could be presented as a complex hierarchical concept network. By the end of this step, 4508 relationships were generated from the handbook. To facilitate subsequent ontology editing and revision in a visual environment, the authors applied a graphical layout to present the complex hierarchical concept network derived from the consulted handbook. The GraphXML [37], which is a graph description language in the XML format, was thus selected to convert the terms-relationships into nodes-edges representation. The authors also observed that the total number of relationships generated after this step was 4508, making the task of expert revision problematic. To overcome this problem, the authors designed a set of weighting rules, which will be discussed later in Section 2.4, to facilitate reasonable revision and reorganization of the phrase network. 2.3. Revising the upper-level concepts Uschold and Gruninger [32] defined several approaches to develop the concept hierarchy of domain ontology: top-down, bottom-up and combination development processes. In this research, the authors combined the top-down and bottom-up approaches. A bottom-up development process starts with the definition of the most specific classes, with subsequent grouping of these classes into more general concepts. Given how the processes of developing the glossary and generating relationships could enumerate most domain concepts and instances, they were considered ‘‘bottom-up”. A top-down process starts by defining the most general concepts in the domain, and then compiling the lower-level sub-nodes for each general concept. Hence the proposed extraction of terms from the TOC was treated as a top-down process. In order to make the developed earthquake engineering ontology more complete and accurate, 17 experts from National Taiwan University and NCREE were invited to revise the upper-level concept map based on the extracted TOC of the earthquake engineering handbook. During the review and revision process, there were three recommended principles: (a) each term or phrase shown in the upper-level concepts should be an independent concept; (b) the chosen terms or phrases should be frequently used by the developed glossary and; (c) the relationships between concepts should not be limited to one-to-one. For example, one-to-many or many-to-many relationships were allowed. The experts reorganized the fist-level nodes as ‘Geoscience’, ‘Geotechnical Earthquake Engineering’, ‘Seismic Technologies’, ‘Structure’ and ‘Earthquake Risk Management’. In comparison with the TOC hierarchy, the ‘Fundamentals’ was renamed as ‘Geoscience’; ‘Infrastructure Aspects’ was integrated within ‘Structure’; and some technical issues were combined as ‘Seismic Technologies’. Regarding hazards, emergency planning and loss assessment issues that appeared in ‘Special Topics’, the experts arranged these topics as ‘Earthquake Risk Management’. Each descendant node of the first-level nodes was then further reviewed by two or three experts in accordance with their professional specialties. Fig. 5 shows the upper-level concepts for earthquake engineering after the experts had completed their revisions. The authors then combined the top-down and bottom-up processes by merging the upper-level concepts with term relationships. This effectively lent greater clarity and consistency to the earthquake engineering ontology that was produced. 2.4. Applying weighting rules for instance and term filtering Prior to exhaustive revision by our experts of the terms and their relationships, a filtering process was applied to reduce the number of term duplicates. These duplicates were the results of synonyms, (i.e. terms appearing in both singular and plural forms), and different verb tenses. While developing the glossary in the first step, the authors had preliminarily eliminated the duplicated terms in the collected data. The authors then identified and clustered the terms with similar concepts to help examine and eliminate the redundant nodes in the developed phrase map. Determining if a term or phrase was a concept or instance posed further difficulties. Because almost all terms had direct relationships with all 34 chapter titles, there was a risk of unmanageability if each title became a huge cluster with too many child nodes. These huge clusters should therefore be partitioned into more hierarchical levels to present a more logical distribution of concepts. This research designed an evaluation and refinement procedure to: (a) filter out those extracted terms that might be produced by typos, (b) integrate synonymous terms and phrases into a single concept, (c) differentiate concepts and instances, and (d) facilitate the experts’ task of defining classes, properties, instances and their exact relationships, as subject to appropriate revision. The phrase map, consisting of 2547 terms and 4508 relationships, served as input for the procedure. The authors implemented three different weighting rules to score each term or phrase for the purposes of term filtering, generation of concepts and instances filtering. In order to illustrate the weighting rules in this paper, the phrase network is simplified as a tree model, as per Fig. 6. In the first weighting rule, the number of relationships a term had was assigned to the term as its base score. In that case, a term’s score was solely dependent on the number of relationships, irrespective of the direction of the relationship (i.e. from upper or lower levels). With this simple rule, the erroneous operation (e.g. typos of terms or phrases, and missing connections of relationships) could be found by examining nodes that scored 0 points. If a node scored only 1 point (e.g. nodes I, H, J, F), it had to be at the end of the tree, meaning that the term or phrase that the node represented was probably an instance. By way of contrast, if a node got a higher score (e.g. nodes C and D), it implied that this term or phrase could be more general, with potential to be a concept or class. Although most term duplicates had been removed in the glossary development step, 24 terms were still found to score 0 point after applying the first weighting rule. Considering that the direction of a relationship could also influence the hierarchical level of the connected nodes, the authors applied the second weighting rule to assign positive or negative scores to a node, depending on the type of relationships the node was attached to. As shown in Fig. 6b, a node received a positive score if it was attached with a ‘‘has-a” relationship; if an ‘‘is-a” relationship connected to the node, it received a negative score. In the example shown in Fig. 6b, for illustrative purposes, the authors arbitrarily selected 1 and À0.01 as the positive and negative scores. In keeping with the second weighting rule, we could easily know how many ‘‘has-a” and ‘‘is-a” relationships are connected to a term. The third weighting rule enacts a propagation of the positive/negative scores assigned by the second rule (see Fig. 6c). As stipulated by the third rule, the score of an upper-level node was the sum of its own score and all scores of its lower-level child nodes. Evaluating the entire phrase map using the three rules helps clarify if a term or phrase was essentially a concept or an instance. For example, in Fig. 6c, even though nodes D, E and F were all at the thirdtier of the tree, node F obviously could have a much higher chance to be an instance. Meanwhile, although nodes B and C were both at the second-tier of the tree, the fact that node C received a higher score because the child nodes presented a deeper (or larger) subtree, implied that either: (a) it was more adequate to select node C as a concept, or (b) node C probably had an improper hierarchy that needed to be revised. After re-calculating the scores of all nodes by the third rule, 2025 terms were recognized as probable instances and 498 terms were suggested as concepts. 2.5. Revising term relationship by experts In the expert assessment step, 17 experts from National Taiwan University and NCREE were invited to a workshop to revise the extracted terms and relationships. The authors selected the nodes that received a top-20 score as assessment categories. Each category containing 83 terms on average (see Table 2) was assessed by one or two experts according to their research interests and background. Experts were asked to score each relationship (i.e. the relationship between each term and category) from 0 to 3 according to the intensity of the terms’ correlation. For each term and phrase, experts also provided suggestions about the wording, synonyms and recommended classification. After the assessment workshop, the authors gathered the scoring data and suggestions, and calculated the scores of categories that were assessed by the experts. Relationships receiving a 0 score were removed from the generated hierarchy. The assessment results were then used to refine the weighting of the third rule for evaluating the relationships. The reduction factors 0.33, 0.66 and 1 were multiplied to the score of the original relationships when the relationships were scored 1, 2 and 3 point, respectively, by assessment. With the refinement, the hierarchy of the developed phrase map was expected to be more reliable since the suggested concepts and instances were more consistent with the experts’ understanding of the discipline. After re-evaluating the term weighting in the entire phrase map by applying the scoring data assessed by the experts, the number of terms reduced from 2547 to 2406, while the number of relationships decreased from 4508 to 3790. Furthermore, 407 terms were suggested as concepts and 2146 as instances by the third weighting rule. The final step in developing the ontology is to convert the concept-instance hierarchy into OWL format. The OWL specification is the most popular ontology representation format endorsed by the World Wide Web Consortium (W3C) [38]. The primary sublanguage of OWL, OWL Lite, is appropriately used to support a classification hierarchy and simple constraints and was thus selected by this research. A parser was developed in this research to automate the conversion of the developed phrase map into the OWL Lite format. 3. Discussion After the first iteration of the earthquake engineering ontology development procedure had been completed, the authors further evaluated the produced base ontology. Fig 7 shows the term distribution among the five major categories (or top-level concepts) of the phrase map obtained. This result implied that the structure domain played a major role within the earthquake engineering handbook. In order to find the successes or mistakes of the developed ontology, the authors followed Gomez-Perez’s evaluation criteria [39] to evaluate the quality of the base earthquake engineering ontology. These criteria are: consistency, completeness, conciseness, expandability and sensitiveness. Consistency refers to whether it is possible to obtain contradictory conclusions from valid input definitions. Although GomezPerez provided several conditions to examine the consistency of an ontology, the conditions were not directly applicable when the huge number of relationships needs to be checked in the developed ontology. The authors, instead, evaluated the consistency of an ontology by checking three possible errors that can be made when building an ontology: a. Circularity errors – circularity errors occur when a class is defined as a specialization or generalization. While developing the earthquake engineering ontology, the authors adopted two steps to prevent circularity. By applying the first weighting rule discussed in Section 2.4, a node scoring 0 was eliminated since it had no parent or child nodes (i.e. it was a class unto itself). The third weighting rule with the reduction factor assessed by experts could also suggest if a relationship was strong or weak enough to be kept or removed when a circularity error occurred within a set of nodes. b. Partition errors – these often occur when a subclass or instance belongs to more than one upper-level class. For example, categorization of the class ‘‘Dams” as a subclass of both ‘‘Structure: Infrastructure” and ‘‘Geotechnical Earthquake Engineering: Soil Structure Interaction”, could result in an error of this type. This is because when we define ‘‘Hoover Dam” as an instance of ‘‘Dam”, people would be unable to predict whether we are talking about ‘‘Hoover Dam as an instance of infrastructure” or ‘‘the soil-structure issues or characteristics of Hoover Dam”. Although experts had endeavored to prevent this type of error when defining upper-level concept maps, partition errors still exist in ontologies that are generated automatically. c. Semantic inconsistency errors – semantic inconsistency errors occur whenever classes or instances are incorrectly classified. These errors were avoided in this research by assigning score 0 to the improper subclasses or instances, which were then removed at the expert assessment step. Completeness of a domain ontology is very difficult to evaluate because the knowledge boundaries of a topic cannot be drawn with any reliable degree of exactitude in the real world. In order to provide a mechanism to evaluate completeness, Gomez-Perez proposed checking the domain or scope of the functions, class hierarchy and property definitions to find the incomplete portion of a developed ontology. The authors recognized this type of error and tried to avoid them by consulting domain experts when developing the upper-level concepts. For example, in the ontology assessment workshop, experts from the geotechnical earthquake engineering domain indicated that the extracted terms and relationships were not abundant and detailed enough for this sub-domain. The authors also observed that the extracted terms and relationships fitted better in the sub-domains of Structure and Seismic Technologies than in ‘‘Geotechnical Earthquake Engineering”, ‘‘Geoscience” or ‘‘Earthquake Risk Management”. To compensate for the shortcomings of these sub-domains in the future, an extension of the glossary and phrase map will be necessary. This will involve the extraction of knowledge from another handbook (e.g. Geotechnical Earthquake Engineering Handbook [40] and Geoscience Handbook [41]). Conciseness relates to whether the ontology stores any unnecessary definitions and redundant relationships. Redundancy errors were the most obvious problems in developing the earthquake engineering ontology. As referred in Section 3.4, almost all of the terms had direct relationships to each chapter title. These kinds of relationships might contribute to the redundancy errors if one class or instance belonged to a chapter title and another superclass simultaneously. For example, if we define ‘‘Passive Control” as a subclass of ‘‘Structural Control” and ‘‘Structural Control” as a subclass of ‘‘Seismic Technology,” the direct relationship that defines ‘‘Passive Control” as a subclass of ‘‘Seismic Technology” would lead to a redundancy in the class hierarchy. The authors have proposed the three weighting rules to assist in the elimination of redundant relationships. The remaining criteria for evaluating an ontology are expandability and sensitiveness. These two features are highly dependent on the ontology storage language and format. Throughout the proposed ontology development procedure, XML documents which could provide extensible and transformable utilities were selected to present the glossary and relationships, as well as the final OWL ontology format. However, ontology development should ideally be an iterative procedure to ensure enrichment of the glossary and specification of the concepts, instances and their properties in as progressive and precise a manner as possible. In the first iteration, the developed base earthquake engineering ontology performed the class hierarchy by using only the hClassi and hsubClassOfi definitions from the OWL Lite specification. The results of the analysis that processed the phrase map with the proposed weighting rules indicated whether a term was a concept or an instance. These results could assist the definition of properties and individual enumeration for the next iteration of ontology development. Therefore, future work must verify each relationship in the base ontology by consulting domain experts. The base earthquake engineering ontology obtained in this research will be published at the IR4AEC project website (<http://ir4aec.caece.net>). Ontology updates and supplementary applications will also be available at the project website in the near future. 4. Conclusions This paper presents a procedure for developing base domain ontologies by extracting knowledge from domain handbooks, whilst reducing the intensity of expert participation. In the procedure, terms and relationships are rapidly collected from handbook TOC, definitions and index. A set of weighting rules are used to evaluate and revise the extracted phrase map. Finally, the developed base ontology is represented in OWL Lite format to store the domain glossary and concept hierarchy information. A base domain ontology was developed as an example for the earthquake engineering domain using the proposed procedure to help evaluate the method’s feasibility. The evaluation results indicated that the developed ontology was not definitive (since the produced ontology was essentially a base ontology) but did address the premises of consistency and conciseness. Although the base domain ontology developed could still be incomplete, the procedure provides a practical approach to quickly prepare a base domain ontology which can be later enriched incrementally. With the help of computer algorithms developed in this research to automate the process of glossary development, generation of hierarchy and relationships of terms, and differentiation between concepts and instances, the participation of domain experts is minimized but still serves as an essential part to help assure the quality of the ontology developed. In addition, the presented approach is generally applicable to generate base domain ontology for all engineering domains whenever similar handbooks are available. One of the lessons learned from the implementation and assessment of the earthquake engineering ontology was that the coordination between domain experts and ontology engineers is of crucial importance. It was difficult for ontology engineers to develop an ontology without the assistance of domain experts. By the same token though, the experts required the fundamental materials provided by ontology engineers to help them improve the accuracy and completeness of the developed ontology. It was demonstrated that extracting knowledge recorded in domain handbooks could substantially facilitate the task of collecting the domain glossary and building relationships between concepts, as opposed to a group of experts delivering the task by brainstorming. Overall, the preliminary results are indicative of a promising future for the proposed approach and its potential to provide a simple, rapid and feasible way of facilitating the development of a base domain ontology. Throughout the proposed process, base domain knowledge can be quickly extracted to assist knowledge management (e.g. [42]) and information exchange for the purposes of education and professional practices. Acknowledgement The authors would like to thank the experts of the National Center of Research on Earthquake Engineering for their participation and help in the presented work. 
GRAONTO: A graph-based approach for automatic construction of domain ontology. Extracting domain knowledge and taking its full advantage has been an important way to reducing costs and accelerating processes in domain-related applications. Domain ontology, providing a common and unambiguous understanding of a domain for both the users and the system to communicate with each other via a set of representational primitives, has been proposed as an important and natural approach to represent domain knowledge. Most domain knowledge about domain entities with their properties and relationships is embodied in document collections. Thus, extracting ontologies from these documents is an important means of ontology construction. In this paper, a graph-based approach for automatic construction of domain ontology from domain corpus, named GRAONTO, has been proposed. First, each document in the collection is represented by a graph. After the generation of document graphs, random walk term weighting is employed to estimate the relevance of the information of a term to the corpus from both local and global perspectives. Next, the MCL (Markov Clustering) algorithm is used to disambiguate terms with different meanings and group similar terms to produce concepts. Next, an improved gSpan algorithm constrained by both vertices and informativeness is exploited to find arbitrary latent relations among these concepts. Finally, the domain ontology is output in the OWL format. For ontology evaluation purposes, a method for adaptive adjustment of concepts and relations with respect to its practical effectiveness is conceived. Evaluation experiments show that GRAONTO is a promising approach for domain ontology construction. 1. Introduction In this knowledge era, creating values via product knowledge has been an efficient approach for enterprises to reduce costs and improve product quality. Collecting useful information from engineering documents has been an important way to accumulate knowledge in today’s enterprises. However, information contained in engineering documents was originally designed for human understanding. Additionally, the number of documents increases and the content evolves rapidly during daily design and manufacturing activities. Therefore, extracting knowledge manually from enormous and cluttered documents would be tedious, difficult and time-consuming (Navigli, Velardi, & Gangemi, 2003). Ontology, defined as the formal, explicit specification of a shared conceptualization, has been proposed as an important and natural approach to representing real-world knowledge (Studer, Benjamins, & Fensel, 1998). Domain ontology, describing a set of representational primitives with which to model a domain of knowledge or discourse, provides a common and unambiguous understanding of a domain for both the users and the system to communicate with each other. In practice, the languages describing ontologies are closer in expressive power to first-order logic than languages used to model databases. Therefore, ontologies are said to be at the ‘‘semantic’’ level, whereas database schemas are models of data at the ‘‘logical’’ or ‘‘physical’’ level (Horrocks, 2008). Since ontologies are able to provide extensible vocabularies of terms, each with a well-defined meaning and relationships with other terms, they are essential components in many knowledgeintensive applications, such as knowledge management (Han & Park, 2009), integrating heterogeneous databases (McClean, Scotney, Morrow, & Greer, 2008), enabling interoperability among disparate systems (Paul & Ghosh, 2008), and knowledge-based services (Niaraki & Kim, 2009). Despite the fact that there exist several standardized available ontologies for use, e.g., WordNet (Miller, 1995), Dublin Core (Weibel, 1997), and General Formal Ontology (GFO) (Herre, 2006), they are not so suitable for specific domains applications. The reasons lie in the fact that they are combinations of general taxonomy and a controlled vocabulary, and are thus not able to represent typical concepts and relationships of such domains. Although ontology has made great impact on several fields, e.g., biology and medicine, most domain ontology constructions are not performed either systematically or automatically (Gacitua, Sawyer, & Rayson, 2008). Building ontology manually is still the dominant means to acquire knowledge of a domain. However, this is a difficult and timeconsuming task that involves domain modellers and knowledge engineers, which are typically overwhelmed by the potential size, complexity and dynamicity of a specific domain. Domain-related documents, recording almost every aspect of a particular domain, are generated, maintained and updated when the corresponding work in such fields has been carried out. Most domain knowledge concerning domain entities with their properties and relationships is embodied in documents and collections. Thus extracting ontology from these documents is a natural and important means of ontology construction. However, present techniques for learning domain ontology from structured or unstructured natural language documents have a few drawbacks (Weng, Tsai, Liu, & Hsu, 2006). Therefore, extracting candidate concepts still depends mainly on heuristic rules provided by the corpus and dictionary about a domain, even if such information is not explicitly available for some domains. Hierarchical and taxonomic relationships are still the main objects to learn among concepts, even if other kinds of relationships, such as chronological and topological, are more essential for modeling a domain. Moreover, if the coverage of documents used for constructing the ontology cannot encompass a domain, it is hard to guarantee that ontology obtained is correct, complete and meets the needs of the applications. To overcome these shortcomings, GRAONTO, a graph-based approach for automatic construction of domain ontology from document corpus is proposed in this paper. To the best of the authors’ knowledge, it is the first time an ontology construction problem is addressed using graph-based methods. The main contributions of the work are: Using graph-based representation of a document, where an ordered and structural description of the document that can preserve the inherent structure of the original document can be obtained to conduct concept extraction and relation extraction. Together with node weighting (term weighting) and edge weighting (connections between terms), a weighted and undirected graph reflects not only the frequency information of terms and term adjacencies, but also the structural information of the documents. Moreover, graph-based document representation addresses concept extraction and relation extraction as a whole since it is a synthesized descriptor and considers both the terms and the relations between the terms simultaneously. Thus it provides a unified framework to integrate concept extraction and relation extraction into a united data structure, such that they are not separated phases; instead, both phases are interrelated and have impacts on each other. Employing the random walk term weighting scheme to determine how informative a term is to a document corpus. Through its iterative mechanism, the random walk term weighting goes beyond simple graph connectivity: it can weigh the term/term unit itself and the weights of the other term units it links to. Therefore, not only the local context of a term or term unit (vertex) is taken into account, the global information recursively drawn from the entire document (graph) is also considered, which makes assessing term/term unit importance complete. Proposing an arbitrary relation extraction method based on frequent informative subgraph mining, rather than only extracting predefined or hierarchical relations. The relation extraction problem is examined as a restrained informative frequent subgraph mining problem. Informativeness of subgraphs is considered to complement traditional subgraph discovering algorithms, so that only meaningful relations in the domain are extracted. The mining procedure is restricted by vertices as well, i.e., only those subgraphs (relations) with at least one of the vertices representing a concept are discovered. This proposed approach is totally unsupervised and highly portable to different domains, genres, or languages. Although the graph-based document representation embodies semantics of the document context, neither deep linguistic knowledge nor domain or language specific annotated corpora is required for document description. Random walk term weighting and subgraph mining are completely unsupervised, and they rely exclusively on the information drawn from the document itself. Hence, GRAONTO, the proposed graph-based approach for ontology construction, is flexible and can be expanded easily to other domains. The remainder of this paper is organized as follows. Section 2 introduces the related work and a few unaddressed issues on domain ontology construction. Section 3 describes the conceptual basis and the technologies used in this work. The details of the proposed ontology construction method are explained in Section 4. The experimental results and comparisons with other algorithms are shown in Section 5. Finally, this paper concludes in Section 6. 2. Related work and unaddressed issues This section presents a brief survey of representative domain ontology construction methodologies related to the proposed work, and discusses several challenging issues in this research field. 2.1. Literature review Due to the critical importance of ontology in the various fields and applications, constructing domain ontology efficiently and effectively has been an important research area recently. Thus, different mechanisms and methodologies for designing and building ontology have been proposed. At present, most of the research on ontology construction depends on data classification determined by the domain experts. However, it is difficult to update the concepts and relationships as information increases and changes rapidly. Hence, using manual methods to build and modify ontology lacks flexibility, and is costly and time-consuming (Weng et al., 2006). The need to overcome the bottlenecks present in the manual construction of ontology has generated several studies and research in semi-automatic (He & Hou, 2008; Makni, Khelif, Dieng-Kuntz, & Cherfi, 2008) or automatic methods (Chen & Chuang, 2008; Chen, Liang, & Pan, 2008; Dahab, Hassan, & Rafea, 2008; Lee, Kao, Kuo, & Wang, 2007; Wang, Tao, & Zhu, 2007; Weng et al., 2006) to build ontology, which commonly involve machine learning techniques to discover ontological knowledge. Several researchers have presented integrated algorithms to construct ontology from unstructured documents. Lee et al. (2007) used Chinese documents as an example and their approach is a synthesis of previously reported approaches, including the episode concept, the Chinese text documents clustering concept, and the fuzzy inference mechanism (Lee et al., 2007). Their approach has been validated using both Chinese and English. However, their algorithm is complicated and has low accuracy. In addition, the learning mechanism for the fuzzy inference rules requires further studies. Chen et al. (2008) used the recursive adaptive resonance training (ART) network to construct domain ontology based on TF-IDF (Chen et al., 2008). Their system selects the related web-pages from a domain and removes the stop words in these pages. Next, a TF-IDF is calculated to find the major keywords in this domain. These keywords are transferred into duality data using the singular vector decomposition (SVD) operation, and the data is clustered using an ART network. Finally, a Boolean operation is used to find the hierarchy between the keywords. This algorithm is efficient but can only construct the ‘‘Is_A’’ relations between terms. The discovery of non-taxonomic relationships has been neglected. In addition, it is difficult to determine whether the candidate keywords are representative of the groups of terms for ontology construction. The output of the ontology is in the RDF format, which is not suitable for semantic inference. Sanchez and Moreno (2008) presented an approach for discovering non-taxonomic relationships from scratch (Sanchez & Moreno, 2008). This method was able to discover relevant verbs for a domain as the knowledge base to learn and label nontaxonomic relationships automatically without supervision. It is a domain independent solution because no domain assumptions are formulated and no predefined knowledge is needed. It is a general purpose approach covering the full process of learning ontology relationships. However, when dealing with natural language resources such as web-pages, problems related to semantic ambiguity may arise (mainly, polysemy and synonymy). Lastly, this algorithm cannot deal with similar relationships between concepts. Chen and Chuang (2008) addressed the automatic construction of a domain ontology using a projective adaptive resonance theory neural network and Bayesian network (Chen & Chuang, 2008). They first used search engines to collect web-pages related to the problem domain, labeled the HTML tags, and employed WordNet to identify and select the meaningful keywords as terms. After these steps, a projective adaptive resonance theory neural network is utilized to cluster web-pages to find the representative term of each cluster using the entropy value. Bayesian network is used to complete the hierarchy of the ontology. This method has low computational costs and can disambiguate possible meanings for the knowledge in the ontology. However, it cannot be expanded to unstructured text documents that have no structural tags. It can only extract hierarchical relations. This method will not produce satisfactory results for non-hierarchical and domain specific relations. In addition to these typical ontology construction methods, a number of systems have been proposed for ontology extraction from text. A few examples are as follows. OntoLearn (Navigli & Velardi, 2004) extracts a domain terminology from available documents. The complex domain terms are semantically interpreted and arranged in a hierarchical fashion. A general-purpose ontology, namely, WordNet, is trimmed and enriched with the detected domain concepts. Text2Onto (Cimiano & Volker, 2005) is a complete re-design and re-engineering of KAON-TextToOnto. It combines machine learning approaches with basic linguistic processing, such as tokenization or lemmatizing and shallow parsing. It is based on the GATE framework. TextOntoEx (Dahab et al., 2008) analyses natural domain text using a semantic pattern-based approach to extract candidate relations and maps them into meaning representations to facilitate constructing ontology. OntoLancs (Gacitua et al., 2008) provides a framework for ontology learning from text to support and evaluate the usefulness and accuracy of different techniques as well as the possible combinations of techniques into specific processes. 2.2. Challenging issues Although much research has been conducted for ontology construction, several challenging issues are still not well-addressed. These issues are explained as follows: (1) Traditional approaches for ontology construction regard term extraction and relation extraction as two separated sequential phases, which duplicates the computational loads when the overlapping parts in both phases are not dealt with at the same time. These two steps are interrelated and can affect each other. Therefore, a unified framework is required to consider both steps synthetically. (2) The TF-IDF term weighting scheme is widely used for concept extraction. However, a lengthy vector is required for describing the TF information of the terms, which increases the computational costs and makes it unsuitable for large corpus. Moreover, the TF-IDF term weighting scheme implies that features in the vector space are independent. It neglects any dependencies that may exist between the terms in the documents and the position information of these terms, such as the order in which the terms appear, where in the document terms appear, how close the terms are to each other, etc. This results in the missing of structural information which could probably be useful when building concepts and relations of the domain ontology. (3) Most research studies concentrate exclusively on the detection of predefined relations, which means only relations in a predefined library can be extracted. However, there are various kinds of relations in a domain, which are difficult to be enumerated and defined. Hence, important relations that are not in the library would not be detected. Moreover, specific knowledge about a domain would have to be defined as a relation library, which is domain-specific and lacks flexibility. (4) There are a few effective methods and tools that can discover patterns based on a wide range of measures, through which almost all the frequent patterns can be mined. However, the informativeness of the frequent patterns, which is an important aspect, is ignored in the mining procedure. As a result, some meaningless frequent patterns are discovered as well, which is time-consuming and increases the computational cost. (5) In concept extraction and relation extraction, a domain-relevant corpus and dictionary that can provide adequate knowledge are needed to analyze the features of the terms for semantic concept clustering. However, not all domains have such explicit knowledge accumulation. For those domains that already have domain-related corpus, it is not guaranteed that the knowledge is unprejudiced, effective and complete for concept clustering since many human factors are involved and the domain knowledge is always up to date. (6) One essential character of ontology lies in its ability to infer implicit concepts and relationships that are covered in the enormous data. This would be very useful when the explicit coverage of source documents is not sufficient for building the entire ontology. However, existing construction approaches have not addressed this aspect. As a result, ontology constructed without performing inference could be incomplete, and may lose some important elements of this domain ontology, leading to inevitable mistakes when using the ontology in some applications. 3. Conceptual basis and technologies This section presents the conceptual basis and technologies that form the foundations of the research presented in this paper. These include formal notation of graphs, the random walk term weighting scheme, graph clustering and frequent subgraph mining. 3.1. Formal notation of graph In mathematics and computer science, a graph is a mathematical structure used to model pairwise relations between objects from a certain collection. A graph in this paper refers to a collection of vertices (or nodes), which correspond to some objects or components, and a collection of edges that connect pairs of vertices, which represent the relations between the vertices. It can be defined as follows: Definition 1 (Graph). A graph G is defined by a quadruple: G = (V, E, a, b), where V is a set of vertices (or nodes), E # V Â V is a set of edges connecting the vertices, a: V ? RV is a function labeling the vertices, and b: E ? RE is a function labeling the edges (RV and RE being the sets of labels that can appear on the vertices and edges, respectively). There are a few important graph matrices. The most natural matrix to associate with G is its adjacency matrix AG (Diestel, 2005). 3.2. Random walk term weighting A random-walk is a mathematical formalization of a trajectory that consists of taking successive steps, where the direction of each step is random and does not depend on the previous steps. In recent years, much research has gone into the random-walk on a graph and its application in many fields. Given a graph and a starting vertex, the random-walk method selects a neighbour of this vertex at random and moves to this neighbour, and the process is repeated. The random sequence of the vertices selected is a random-walk on the graph (Lovasz, 1993). It is guaranteed that under certain conditions, such as if the graph is aperiodic and irreducible, the random walk would inevitably converge to a stationary distribution of probabilities associated with the vertices in the graph, which intuitively represents the probability of finding the walker at that vertex during the random-walk. Thus, it represents the importance of the vertex within the graph. It can be seen that the random-walk on a graph is a way of determining the importance of a vertex within a graph, taking into account the global information recursively drawn from the entire graph, instead of relying only on local vertex-specific information. The number of times a vertex is being chosen in random walks is proportional to the number of links it has with other vertices, i.e., the degree of this vertex in an undirected graph and the out-degree in a directed graph. The higher the number of times a vertex is selected, the higher would be the importance of this vertex (Mihalcea & Tarau, 2006). Therefore, this model can be used to determine whether a term is informative and important in the document corpus. Two schemes for weighing a vertex in a graph have been proposed by Brin and Page (1998) and Mihalcea and Tarau (2006). Hence, a graph-based representation of document, where the vertices and edges denote the terms and connections between the terms, respectively, is a prerequisite for weighting terms based on the random walk model. Random walk term weighting, along with a graph-based representation of document, can benefit text processing-related studies and applications. 3.3. Graph clustering Generally, clustering is to discover natural groups of similar elements in data sets, achieving a trade-off between intra-cluster density and inter-cluster sparsity. Similarly, graph clustering is the task of grouping the vertices of a graph into clusters with respect to the edge structure of the graph, such that there would exist many edges within each cluster (intra-cluster density) and relatively few between the clusters (inter-cluster sparsity). Given that it is possible to represent terms of a domain corpus with their structural information as graphs, a graph clustering algorithm can be employed for term clustering, such that those concepts in a domain can be extracted. The Markov Cluster (MCL) algorithm has been developed for weighted graph clustering using flow simulation (Dongen, 2000). MCL is simple, scalable and has a number of attractive properties, enabling it to deliver high-quality clustering. MCL has been widely used in many fields, such as biology and bioinformatics. The key concept in the MCL algorithm is that for a graph possessing a cluster structure, random walks of small length k will tend to stay in the same cluster relatively long before moving to another cluster, i.e., dense regions in graphs correspond to regions where the number of k-length paths is relatively large. The Markov matrix describes the transition probability between all pairs of vertices. Definition 2 (Markov matrix). Let G be a graph with n vertices, the Markov matrix MG(mij)n Â n associated with the graph G is defined as MG 1⁄4 AG Â DÀ1 ð1Þ where MA is the adjacency matrix of graph G; D denotes the diagonal matrix dij 1⁄4 8 <d kk : 1⁄4 n P i1⁄41 dij 1⁄4 0 AGik if i 1⁄4 j ð2Þ The MCL algorithm discovers the cluster structure using a mathematical bootstrapping procedure, where this ‘long stay in the same cluster’ effect is deliberately boosted by alternating two operations, namely, expansion and inflation. The expansion step calculates the steps in a random walk using normal matrix multiplication. Thus, new probabilities are assigned for all pairs of vertices (departure node and destination node). Since the number of paths with longer length for pairs of vertices lying in the same dense cluster is larger than that belonging to different clusters, the probabilities for the vertex pairs within a cluster are larger compared to the probabilities of vertex pairs in different clusters. Therefore, expansion enables more vertices to be seen in their neighbourhoods. Inflation step promotes favoured neighbours and demotes less favoured neighbours via a parameterized operator Cr, called the inflation operator (Dongen, 2000). From the definition, it can be noted that the inflation operator does not need any a priori knowledge of the domain in question and cluster structure, instead, the effect of inflation is only the result of cluster structure being present. Therefore, the inflation step is domain irrelevant, which makes it more suitable for ontology construction. After a few iterations of expansion and inflation, the Markov matrix is eventually translated into a doubly idempotent matrix, i.e. the matrix remains invariant even with further operations of the expansion and inflation. The graph is finally segmented into different groups. There are few paths between clusters, and the collection of resulting groups is interpreted as a clustering according to the rule below: Definition 3. Let M be a non-negative column allowable idempotent matrix of dimension n, let G be its associated graph on the node set V = {1, ... , n}. Let Ei, i = 1, ... , k be different attractor systems of G. For v e V, v ? Ei donates there exists e e Ei with v ? e. Then possible clustering C = {C1, ... , Ck} associated with M is defined by C i 1⁄4 Ei [ fv 2 Vjv ! Ei g ð3Þ where column allowable means that there is no zero column in the matrix. 3.4. Frequent subgraph mining Given a graph database, the frequency of a graph pattern is the number of graphs in the database containing at least one subgraph isomorphic to the pattern. The frequent subgraph mining problem is to determine the set of frequent patterns which frequency is higher than a minimum threshold (minsupp) (Kuramochi & Karypis, 2001). Formally, it can be defined as follows (Berendt, 2006): Definition 4 (subgraph and frequent subgraph). Given two graphs G1 = (V1, E1, ‘1) and G2 = (V2, E2, ‘2), an embedding of G1 in G2 is an injective function f: V1  ́ V2 such that (1) "v e V1: ‘1(v) = ‘2(f(v)) and (2) "(v1, v2) e E1:(f(v1), f(v2)) e E2 and ‘1(v1, v2) = ‘2(f(v1), f(v2)). The graph G1 is a subgraph of G2, denoted by G1 # G2, if there is an embedding of G1 in G2. For a dataset D = {Gd|Gd = (Vd, Ed, Ld)} of graph transactions Gd, a frequent subgraph is one that can be embedded in at least minsupp Â |D| transactions. Much attention has been paid to mining frequent subgraphs in a graph database in the last few years. Many algorithms have been proposed to solve this problem. However, the search process in an arbitrary graph structure involves costly subgraph growth as well as graph and subgraph isomorphism tests. According to the results of quantitative comparison of several commonly used subgraph miners (Worlein, Meinl, Fischer, & Philippsen, 2005), the gSpan algorithm (Yan & Han, 2002) achieves a good balance between run-time and memory consumption, which is the basis of authors’ work. DFS lexicographic order and minimum DFS code are two core concepts in gSpan (Yan & Han, 2002). gSpan adopts a depth-first search to define an order in which the vertices and edges are visited. A DFS lexicographic order based canonical labeling system is designed to support DFS. Each graph is assigned a unique minimum DFS code, based on which a hierarchical search tree is constructed. It then discovers all the frequent subgraphs with the required support by traversing the search tree in a predefined order. The most attractive features of the gSpan algorithm are: (1) since two graphs are isomorphic to each other if and only if the minimum DFS codes of these graphs are equal, gSpan converts the problem of mining frequent subgraphs to the mining of their corresponding minimum DFS code. The latter problem is a sequential pattern mining problem, which is easier to solve. (2) gSpan integrates the frequent subgraph growth and subgraph isomorphism into one procedure, such that it can avoid significant computational costs and accelerates the mining process effectively. The proposed method for extracting arbitrary relations via frequent subgraph mining is based on the gSpan algorithm. It is adapted to suit the requirements of this research through adding restraints and taking the practical meanings of subgraphs into account. In a nutshell, a graph-based representation is used to describe a document to take into account the term and term unit frequency information, as well as the structural information of text. The random walk term weighting scheme is employed to determine the importance of a term in a document corpus from both the local and global perspectives, which is important in concept extraction and relation extraction. Graph nodes clustering is exploited to group nodes of a graph that have similar connection attributes, in order to classify similar terms into concepts and disambiguate terms with different meanings. Frequent subgraph mining is utilized to discover latent arbitrary relations implied in the text corpus, and it is constrained by the vertices (concepts obtained from concept extraction) and the informativeness (whether the subgraph discovered is meaningful). 4. Graph-based ontology construction algorithm This section describes in details the construction of a domain ontology from scratch. The framework consists of five parts, i.e., documents pre-processing, document graph generation, concept extraction, relation extraction, and ontology evaluation. The construction and the data and control flows among different steps are illustrated in Fig. 1. The proposed algorithm is based on a graph representation of documents, which is used as a bridge to connect different steps in domain ontology construction. The core steps of this approach interact and share data information with each other such that concept extraction and relation extraction are considered under a unified framework. The random walk term weighting is employed to measure the importance and informativeness of the terms. Graph node clustering is used to group the terms to produce concepts and frequent subgraph mining with informativeness and constraints is utilized to find the relations among these concepts. In addition, concepts and relations are estimated using ontology evaluation such that those that cannot meet the application requirements are fed back to the original steps for adjustment. Therefore, a closed loop for ontology construction is formed in the proposed approach, facilitating communication among concept extraction, relation extraction and ontology evaluation. 4.1. Document pre-processing Document pre-processing converts domain documents into the correct format that can be used by the subsequent steps in ontology construction. It removes stop words that are irrelevant to the text meanings, reduces inflected words to their stems, tags part of a speech for each remaining term, and finally counts the frequency and adjacency information of the terms and the term units that are the basis for further operation. Stop words removal: Elimination of stop words with the objective of filtering out words with very low discrimination values for retrieval purposes. A stop word list1 is employed in this research to filter out those common words that carry less important meaning, e.g., common words such as a and the. Text stemming: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form. For example, measure is the stem for {measuring, measured, measurement}. Stemming of the remaining words is to remove affixes (i.e., prefixes and suffixes), such that the extraction of concepts and relations containing syntactic variations of the terms is allowed. The Porter Stemming Algorithm2 (Porter, 1980) is used in this research to stem text. Part-of-speech tagging: Part-of-speech (POS) tagging is the process of marking up the words in a text/corpus as corresponding to a particular part of a speech, based on both its definition and relationship with adjacent and related words in a context. The Stanford Log-linear Part-Of-Speech Tagger3 (Toutanova & Manning, 2000) is employed in this approach to identify the noun, verb, adjective and adverb terms, of which the noun terms and verb terms can be regarded as the initial candidates of the concepts and relations, respectively. Statistics of term and term unit: For each document of a corpus, the term frequency information (used as the label of the node) and the term unit frequency information (used as the label of the edge) are collected. Term frequency counts how many times a term (after stemming) appears in one document, while term unit frequency counts how many times two terms appear adjacent in one document. As the outcome of document pre-processing, terms with their frequency information and adjacency information are extracted. Both types of information are important in the subsequent steps since they reflect how important a term or term unit is to the documents to some degree, but frequency information alone is not enough to capture all the information of a text. Structural information is also essential, which can be captured by the structure of the document graphs created based on adjacency information. 4.2. Generation of document graph In this phase, each document in the corpus is represented as a graph. According to Definition 1 (Section 3.1), the document graph can be interpreted as follows: V represents a collection of meaningful terms that appear in a document, E represents a collection of edges indicating the relations between the terms. a and b denote the labels of the nodes and edges, respectively; they are flexible and adjustable, and they can have different function combinations. As both parameters are dominant parameters in graph representation and different function combinations can reflect different information of the documents in different levels and aspects, these combinations would be tested to select a most suitable one. An adapted version of the normalized frequency representation (Chow, Zhang, & Rahman, 2009) is employed in this research. Since a directed graph does not have any significant benefits on document representation (Mihalcea & Tarau, 2006), an undirected weighted graph with the vertices and edges describing meaningful terms and connections between the terms respectively is used in this research. Each vertex is labeled with the term it represents, and only one single vertex is created even if a term appears more than once in a document. The edge connecting two terms indicates that both terms are adjacent in a document. Each vertex and each edge is labeled with a frequency measure, which represents the total number of occurrences (on the vertices) and cooccurrences (on the edges). For vertex, the frequency measure signifies how many times the associated term appears in the document; for edges, this signifies how many times the two connected terms appear adjacent to each other in the specified order. However, this representation scheme may be biased due to large differences in document sizes. The frequency of a vertex or edge in a large-size document is probably larger than the same one which appears in a small size document, even if the importance of this term or relation is almost identical for both documents. Therefore, the frequency should be normalized to reflect how important a term or a relation actually is for the document. A normalized value in [0, 1] is assigned to a vertex by dividing each vertex frequency value either by the maximum vertex frequency value that occurs in the graph or by the total frequencies of all the vertices in the graph; a similar procedure is performed for the edges. In addition, every vertex is labeled with the term it represents, while every edge is labeled with the labels of both nodes connected by this edge. The pseudo-code for generating a document graph, together with the document pre-processing is shown in Fig. 2. Example 1. Given the following text in a document related to fixture design: Fixture configuration design is a subtask of fixture design. The task of fixture configuration design is to select fixture elements and place them into position to locate and hold the workpiece. This can be viewed as the structural design of the fixture. After the removal of stop words, such as ‘in’, ’a’, ’of’, etc., a graph can be created to represent above text as in Fig. 3. Fig. 3(a) is a graph labeled with the original frequency information representing the number of times the terms or term units appear in the above text, while Fig. 3(b) describes the document graph with labeled edges after normalization. 4.3. Concept extraction Concept extraction consists of two core steps, namely, weighting terms based on random walks to measure how informative a term is to the domain corpus and using the MCL algorithm to cluster the weighted terms for extracting the candidate concepts. In the first step, each node of a graph is assigned a weight to assess the importance of the term represented by this node with regards to its local and global contexts. After that, the MCL algorithm is employed to group these nodes into clusters in terms of the graph structure and term weights, so that terms with similar semantics can be grouped into one group. Next, each cluster is interpreted as one concept according to the weights of the terms in this cluster. The random walk term weighting is generally an iterative procedure, during which the score of each vertex is being updated in each iteration with regards to the new weights that its adjacent vertices have accumulated, until all the vertices converge at a pre-defined threshold. Since an undirected graph is used in this research, the score WS(Vi) of each node Vi is re-calculated using Eq. (4) to estimate the importance of a term to the domain corpus. where wij denoting the weight of the edge connecting from vertex Vi to vertex Vj. After the random walk term weighting, the node weights of the graph change from measuring how many times a term appears in a text to an estimate of how informative a term is to the text. From Eq. (4), it should be noted that the weights of edges adjacent to the vertices are used to derive the similarity measures for the vertices in this work. The vertex similarity is estimated based on the structural properties of the graph instead of some application specific properties imposed on the vertices. It is domain independent and no further knowledge is needed during this term weighting step, making this approach to ontology construction suitable for various domains. In order to cluster nodes, an edge should be weighted with regards to the vertices connected by this edge since the MCL algorithm clusters nodes in terms of the edge information of a graph. Taking the normalization of the edge weight into account, Eq. (5) is used to assess an edge according to the nodes that it is connected to. WEðeij Þ 1⁄4 qffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi WSðV i Þ2 þ WSðV j Þ2 jWSðV i Þj þ jWSðV j Þj ð5Þ where Vi and Vj are two nodes connected by eij. From Eq. (5), the edge weight in the proposed algorithm is within the interval [0, 1]. Next, the MCL algorithm is used to cluster the nodes of the graph with the new node and edge weights. The flow of MCL is illustrated in Fig. 4. First, a loop is added for each node of the input graph, which weight is assigned as the maximum weight of all the edges connected to the node by default. Second, this new graph with the loops is translated into a stochastic Markov matrix, which describes the transition probability between all pairs of the nodes. Thus, the probability of a random walk of length n between any two nodes can be calculated by a process referred to as expansion, i.e., raising this matrix to the exponent n. Since the higher length paths are more common between nodes within the same cluster than nodes between different clusters, the probabilities between nodes in the same cluster will typically be higher in the expanded matrices. The MCL further exaggerates this effect by an inflation step, i.e., taking entry wise exponents of the expanded matrix, and rescaling each column so that it remains stochastic. This process iterates until the Markov matrix converges to a doubly idempotent matrix. Finally, a mapping from the non-negative column allowable idempotent matrix to the set of clusters is established to determine clusters according to Eq. (3). It should be noted that only a single-word term is considered as a candidate concept during the above processes. However, there exist many multi-word concepts in reality. Therefore, after candidate concepts are extracted using MCL, a post-processing step is employed to reconstruct adjacent terms into multi-word concepts. During post-processing, all candidate concepts are marked on the nodes of the graph. If there exists a path that connects a sequence of adjacent nodes, the candidate concepts represented by such nodes are collapsed into a multi-word concept. The pseudo-code for concept extraction, including the random walk term weighting and the MCL for clustering terms is shown in Fig. 5. Considering Example 1, the transition from the original graph to the term weighted graph, the weighted transition matrix and the associated column stochastic Markov matrix for the latter graph are illustrated in Fig. 6. 4.4. Relation extraction Intuitively, a general and important relation in a domain would appear frequently in the documents about this domain, and this corresponds to a frequent sub-structure of the graph representing the documents. Therefore, relation extraction for domain ontology construction can be converted into discovering frequent subgraphs within graphs describing the domain-related corpus. Moreover, informativeness should be considered during the mining procedure since relations discovered should have practical meanings, rather than just combinations of terms. Therefore, the relation extraction problem is considered as a constrained informative frequent subgraph mining problem, i.e., mining subgraphs which frequency exceeds the required threshold both in a single graph and a set of graphs. Moreover, the mining procedure is also constrained by the vertices, i.e., only discovering those subgraphs (relations) where at least one of which vertices represents a concept. To meet these requirements, the threshold can be increased to discover only a few frequent subgraphs. However, the most frequent graphs are generally the least interesting and the relations extracted from these subgraphs are not domain-specific. A better solution is to extract a restricted yet informative subset of frequent subgraphs. In information theory, informative patterns are simultaneously discriminating (i.e., subsuming a small subset of all the possible object descriptions) and representative (i.e., having a high frequency) (Pennerath & Napoli, 2007). Thus, the extraction of the most informative subgraphs is to find a subset of the frequent subgraphs that provides an optimized trade-off between their frequency and the information contained in their descriptions, which can be assessed using an arbitrary scoring function. Therefore, the relation extraction can be formulated as: Given a dataset of labeled-graph transactions (each graph transaction corresponding to a document), a taxonomy of concepts (concepts are the outcome of the concept extraction phase), a mapping from the labels to the concepts, and a minimum support threshold, find all the frequent informative subgraphs and interpret them as relations. In the proposed approach, the gSpan algorithm is used to discover the frequent subgraphs. The traditional procedure for gSpan is as follows: First, it builds a DFS code for different DFS trees of each graph, and maps each graph to a unique minimum DFS code as its canonical label according to the DFS lexicographic order. After that, based on this order, it adopts the depth first search scheme to mine the frequent connected subgraphs efficiently. However, mining the frequent subgraph alone is not sufficient; the informative subgraphs should be discovered as well. Therefore, in order to determine the importance of the subgraphs, an information function to estimate the informativeness contained in the subgraphs is integrated into the traditional gSpan algorithm. The information function introduced by Pennerath and Napoli (2009) is adapted to determine the informativeness of a subgraph and it is described as follows: Definition 5 (Information function). the information function si is defined as: Si : ðg; rr Þ # IðgÞ Á rr where the factor I(g) of information related to subgraph g is the sum of the information carried by every vertex v e V(g) of weight wv(v) and every edge e e E(g) of weight we(e): IðgÞ 1⁄4 X i v ðv Þ þ v 2VðgÞ X ie ðeÞ ð7Þ e2EðgÞ Quantity of information associated with a vertex or edge weight is in turn: iv ðv Þ 1⁄4 Àlog2 ie ðeÞ 1⁄4 Àlog2 ! wv ðv Þ P ; 0 v 0 2d0 wv ðv Þ d0 2D0 ! X w ðeÞ P e 0 00 00 e0 2d00 we ðe Þ X ð8Þ where D0 , D00 are the subset of graph database D, respectively, D0 = {d D|$v0 e d, lv (v0 ) = lv (v)} and D00 = {d e D|$e0 d, le(e) = le (e0 )}. Rr is the frequency that the subgraph g appears in the graph database D. This new information function measures how a subgraph is informative with respect to both the entire graph database and the neighbourhood that the vertex or edge lies in. Besides this information function, three other constraints are added to make the algorithm more suitable for relation extraction: (1) The concepts obtained from the prior steps are used to determine whether the current vertex should be discovered as an element of one relation when mining frequent subgraphs. (2) For relation extraction, the POS attribute of a term is taken into account, i.e., each subgraph discovered must contain at least one noun and one verb or adjective. (3) If the subgraphs mined contain vertices that are not in the list of concepts, these vertices can be added as concepts with respect to their term weights, which can be regarded as the feedback from the relations to the concepts. After the frequent subgraph mining, the subgraphs obtained will be interpreted as relations between the concepts. A relation is described as a triple {concept1, relation, concept2}. A general way is to first locate the node labeled with a verb, and find the adjacent nodes. If both nodes are labeled with a noun, a relation between these three terms is established. If one or both nodes are labeled with a verb, these nodes are connected to form a new node labeled with the union of the verb labels. The new node will be used as the central word for the next iteration. This process is iterated until two nodes labeled with a noun are connected by a node labeled with a verb. Next, the two nouns and the verb are interpreted as concept1 and concept2 and the relation, respectively. This process is applicable to a few nouns that are adjacent. A few typical situations encountered in subgraph interpretation are illustrated in Fig. 7. It should be noted that more than one relation may be extracted from one subgraph, and the number of relations extracted from one subgraph depends on the number of verb concepts present in this subgraph. The pseudo-code for gSpanbased relation extraction is shown in Fig. 8. 4.5. Ontology evaluation Ontology evaluation is used to determine whether the constructed ontology is valid and suitable for practical applications. Most algorithms either omit this phase, or use manual methods to solve this problem, which is cost and time-consuming. In the proposed framework, domain ontology is examined for its suitability for practical applications. Domain testing documents, which are documents related to the domain but have not been used to construct the ontology, can be used to test whether the constructed ontology is valid and accommodates new documents. If contradictions occur, i.e., there are errors or conflicts in the constructed ontology, it needs to be improved. Thus, the concepts or relations that contribute to the contradictions are fed back to the system, and the original documents and the domain testing documents are used together to refine the ontology. These processes are iterated until no error or conflict occurs. As a result, a closed feedback loop is established between ontology construction and its application, making it possible to amend the domain ontology with respect to its practical effectiveness. 5. Experimental results This section presents the experimental results which have been carried out using the proposed system. Technical reports on the ‘‘fixture design’’ domain forming the fixture document corpus are collected to construct a fixture design ontology. Two experiments are designed to evaluate the effectiveness and efficiency of the proposed approach. The first experiment examines the efficiency of concept extraction and relation extraction phases as compared with traditional methods. In the second experiment, the ontology of the fixture design domain is produced from the corpus using the proposed approach. After the new ontology has been created, a comparison between the new ontology and an ontology on fixture design that has been built manually is conducted to demonstrate the validity of the proposed method. 5.1. Data set and runtime environment Two data sets are used in the first and second experiments to estimate the efficiency and effectiveness of the proposed approach, respectively. For the first experiment, a commonly used document collection in text retrieval, i.e., TREC-9 corpus (Robertson & Soboroff, 2000), is employed as a benchmark to measure the efficiency of the proposed method based on comparisons between different approaches. This corpus is a test collection consisting of titles and abstracts of publications from the medical domain (marked with ‘‘.T’’, ‘‘.W’’ respectively), Human-assigned MeSH terms (marked with ‘‘.M’’) and further meta-data, such as authors, subject, type of publication, etc. In the experiment, the abstracts are used as the sources from which concepts are extracted, while the Humanassigned MeSH terms are utilized as concepts benchmark to measure the approach. For the second experiment, the approach is evaluated to determine the effectiveness of the approach in achieving a satisfactory ontology of a domain. The data set used in this experiment is a corpus of 670 articles in text format on fixture design with abstracts and keywords from the Engineering Village database. Engineering Village offers a comprehensive, precise and intelligent engineering information search. In order to ensure that this corpus can cover most aspects of fixture design, different keywords are entered into the search engine to search for articles which titles contained the keywords, and these articles retrieved are compiled as articles for the corpus. The keywords and the corresponding articles retrieved to form the corpus are given in Table 1. All the simulations have been performed on a Windows XP PC with a 2.10 GHz Intel Core 2 Duo processor and 2G of memory. The extraction programs were written in the Java programming language, and all the programs for statistical analysis of the experimental outcomes were implemented in OriginPro 8. 5.2. Performance evaluation measures In order to evaluate the performance of the construction approach, an assessment scheme is defined to estimate the effectiveness of concept extraction and relation extraction. The Precision–Recall measure commonly used in data mining is used in this research to assess the performance of concept extraction. Precision is a measure of the exactness or fidelity, whereas Recall is a measure of completeness. For concept extraction, concept precision (C–P) reflects the ability of the proposed approach to differentiate the representative terms of a domain from normal ones. The concept–location–precision (C–L–P) demonstrates the concept precision and the ability to classify a concept into a correct relation. The concept recall (C–R) measures the ability of the approach to extract concepts completely. Likewise, similar measurements can be used to evaluate relation extraction. The Precision (R–P) and Recall (R–R) for relation extraction corresponding to Precision (C–P) and Recall (C–P) for concept extraction, respectively. These measures have also been adopted by Chen et al. (2008). In addition, the F-measure (F-M), a weighted harmonic mean of precision and recall, is also employed in the proposed approach to assess the precision and recall integratedly. The definitions of the measures can be referred to in Table 2 and the formulas as follows: PrecisionðC—PÞ 1⁄4 PrecisionðR—PÞ 1⁄4 PrecisionðC—L—PÞ 1⁄4 A AþB C CþD A AþE Precision Â Recall ð10Þ ð11Þ RecallðC—RÞ 1⁄4 RecallðR—RÞ 1⁄4 ð12Þ Fb ðF—MÞ 1⁄4 ð1 þ b2 Þ Â ð13Þ b2 Â Precision þ Recall where Fb measures the effectiveness of extraction with respect to a user who attaches b times as much importance to recall as precision. For ontology construction, precision is more important than recall as the concepts and relations of the domain ontology are those that can represent the typical elements in a domain. Therefore, the value of b is set as 1 (meaning precision is as important as recall) and 0.5 (meaning precision is twice as important as recall) in the experiments. 5.3. Experimental designs 5.3.1. Experiment 1 The performance of the proposed approach is objectively estimated using this experiment. This experiment evaluated the effectiveness of concept extraction and relation extraction, i.e., whether the proposed approach can discover the correct concepts and relations from the document corpus, and the precision and recall of the approach. The TREC-9 corpus is used for this evaluation, and based on this dataset, two combinations of evaluation indicators for concept and relation extraction are assessed: (1) Precision (C–P) versus Recall (C–R): Both are widely used statistical classifications for measuring the corresponding relationship between exactness and completeness, which are calculated using Eqs. (10) and (12), respectively. They are used to estimate the precision of the different methods with regards to various recalls, illustrating the relationship between precision and recall. (2) Fb(F–M) versus the size of the graph: The graph representing the domain corpus is generally large as many documents are involved and each document contains many terms. Since the size of the graph is an important factor that impacts the performance of graph-based algorithms, it is necessary to know the effects of the size of the graph on the proposed method (calculated using Eq. (13)) and determine whether the performance of the approach will decrease with a large graph. For comparison purposes, three other typical methods on termination extraction, together with the proposed algorithm are used in the experiments, namely, (1) TextRank (Mihalcea & Tarau, 2006): The undirected graph is used and the co-occurrence window is set as 2 as this set of parameters outperforms others. (2) SIGNUM (Ngomo, 2008): Weighted and bi-grams graphs are used in the experiment, and the edge weight between two nodes is set the same as the original scheme. (3) TF-IDF-ART keyword extraction (Chen et al., 2008): Single value decomposition and adaptive resonance theory network (ART) are employed. This method assigns different weights to words according to where they appear in the text, 1.0 is assigned for title and 0.6 is assigned for abstract for the dataset in this experiment. The concept clustering termination threshold for ART is set as 5. In addition, since this method is supervised, one third of the texts in TREC-9 are used for training. It should be noted that the first two approaches are based on graphs while the last one is based on traditional TF-IDF. The proposed approach GRAONTO, TextRank and SIGNUM are completely unsupervised and no training/development data is required. Hence, all the documents in TREC-9 are used for estimation purposes. Likewise, two other approaches of relation extraction are used as references to illustrate the performance of the proposed method, namely, (1) RelExt (Schutz & Buitelaar, 2005): RelExt is capable of automatically identifying highly relevant triples. It works by extracting relevant verbs and their terms from a domainspecific text collection and computing the corresponding relations through a combination of linguistic and statistical processing. (2) Non-taxonomic relationships learning (Sanchez & Moreno, 2008): It is an automatic and unsupervised methodology that addresses the non-taxonomic learning process for constructing domain ontology. It is able to discover domainrelated verbs, extract non-taxonomically related concepts and label relationships. In addition, a pure gSpan algorithm without constraints (labeled with GRAONTO-WC) is used to test the performance of the proposed approach with regards to the information function and three other constraints (Section 4.4). 5.3.2. Experiment 2 In this experiment, the fixture design domain is used as a case to demonstrate the effectiveness of the proposed approach. As mentioned in Section 5.1, the domain corpus on fixture design, consisting of 670 articles in text format with abstracts and keywords from the Engineering Village database, is used to produce the domain ontology on fixture design using the proposed approach. After creating the ontology using the OWL format, this new ontology is compared with the expert-defined FIXON ontology (Ameri & Summers, 2008). The Precision (C–L–P) (Eq. (11)) and Recall (C–R) are used to evaluate the method. 5.4. Evaluation and discussion An entire graph representation describing a document from the corpus is shown in Fig. 9. It can be seen that all the nodes form a net, which means the relationships among the terms are comprehensive and complicated. The nodes in the dense parts represent the core content of the document, which includes the important terms and informative relations among these terms, while the nodes in the sparse parts represent the terms that have comparatively fewer connections with others. However, the importance of these parts should be studied from the global perspective. Some natural clusters can be found in Fig. 9, which illustrates the application of the graph cluster algorithm in this proposed approach is reasonable. Indeed, there exist such clusters describing the similar terms in a graph. The experimental results of experiment 1 (Section 5.3.1) are shown in Figs. 10 and 11, which show the performance of concept extraction and relation extraction, respectively. Since the size of the graph has an impact on the performance of graph-based algorithms, all the experiments are carried out with a fixed graph size to eliminate this effect. In this work, the graph size is around 5000. For concept extraction (Fig. 10), the graph-based methods outperform the TF-IDF based method. This is mainly because the graph can capture both the structural information and the frequency information of the corpus while the TF-IDF mainly focuses on the frequency information. From the comparison of the three graph-based methods, the performance of the proposed approach is the best. This is because GRAONTO can weigh terms from both local and global perspectives and a graph cluster algorithm is used to group and disambiguate similar terms, which contribute to improvement in the precision. In addition, since there is no clustering step in TextRank and SIGNUM, the precision of both methods degenerates more than TF-IDF-ART and GRAONTO. This illustrates that the clustering step can improve the performance of the methods for concept extraction. For the relationship between the performance and the graph size (right-bottom sub-chart in Fig. 10), it should be noted that the performance of all the graph-based approaches deteriorates as the graph size increases. This might be an inherent shortcoming of the graph-based methods. For relation extraction (Fig. 11), similar results can be found. Graph-based methods outperform other methods. The performance of GRAONTO is the best compared to the other three methods. This is because GRAONTO considers the relation extraction problem as a frequent informative subgraph mining problem. An information function is introduced to evaluate how informative a subgraph is. It is important to domain ontology construction that only meaningful relations should be discovered. Moreover, a few constraints that are helpful to refine the results are considered. All these contribute to the performance of GRAONTO. The curves labeled with ‘GRAONTO-WC’ in Fig. 11 shows the impacts of the constraints on the performance. The performance of the graphbased algorithms for relation extraction degenerates with respect to the size of the graph, similar to concept extraction. In experiment 2, a fixture design domain ontology is constructed via the proposed approach. The resulting ontology is imported into Protégé and visualized using the Jambalaya plug-in. Two snippets of the ontology focusing on ‘fixture’ and ‘fixture design’ are illustrated in Figs. 12 and 13, respectively. Frames denote the concepts of the domain while the line connecting two frames indicates that there is a relation between these two concepts. The shadow texts in both figures denote the relations between the two concepts where the highlighted edge connects. The ontology obtained using GRAONTO is compared with the FIXON, which is manually built by experts. Since the sources from which the two modes of ontology are constructed are different, only the common parts of the two ontologies are compared. Fig. 14 shows the snippet of FIXON focusing on the same concept ‘fixture’ as Fig. 12. The parameter values corresponding to Table 2 are: A = 63, B = 26, C = 47, D = 16, E = 19. Based on these, the performance of GRAONTO as compared with FIXON is as follows: the Precision (C_P) is 70.8%, the Precision (C_L_P) is almost 71.4%, and the recall is just under 76.8%. Automatic domain ontology construction is a difficult problem in ontology research, especially for approaches that are independent of the domain. Since there is no domain knowledge involved in the construction process, it is difficult to recognize whether a relation is important and informative to the domain in question. Most methods for automating ontology construction rely on natural language processing (NLP). Thus, their performance depends largely on the performance of the NLP involved in the ontology construction. However, the approach proposed in this paper does not depend on NLP techniques for concept and relation extraction. It is based on graph and has been proved to be efficient and effective. From both experiments, it can be seen that the GRAONTO achieves better results as compared to traditional methods. Therefore, this research has shown that NLP is not the only way to automatic ontology construction. Graph-based approaches can achieve the same or even better results. In addition, they are widely used in many fields and applications. This paper shows that graph-based techniques can also be applied to ontology construction. The research reported in this paper provides a novel and promising way for automatic domain ontology construction from a new graph-based perspective. 6. Conclusions and future work In this paper, a graph-based approach is proposed for automating the construction of domain ontology. In order to take into account both the frequency and structural information of the domain corpus, the proposed algorithm is based on a graph representation of the documents. After the generation of the document graphs, a few graph-based techniques are involved: (1) the random walk term weighting is employed to measure the importance and informativeness of terms from both the local and global perspectives, which is the basis for extracting concepts and relations. (2) The MCL algorithm is used to disambiguate terms with different meanings and group similar terms to produce concepts for concept extraction, and (3) an improved gSpan algorithm constrained by both vertices and informativeness is used to find the arbitrary latent relations among the identified concepts. The produced domain ontology is finally output in the OWL format. In addition, a method is developed for the adaptive adjustment of concepts and relations with respect to its practical effectiveness for ontology evaluation. The proposed ontology construction approach is unsupervised, such that costly and time-consuming human efforts are avoided. In addition, neither deep linguistic knowledge nor domain or language specific annotated corpora is required in the proposed method. Hence, it can be used in different domains. Two experiments were carried out to evaluate the proposed method. The first examines the efficiency of the concept extraction and relation extraction phases of the proposed approach as compared with traditional methods, while the second experiment used fixture design as the problem domain and compared the ontology produced using the proposed method with an ontology built manually. The experimental results indicated that owing to the improvements made in proposed algorithm, this algorithm outperforms other similar methods. However, from the experiments, it can be seen that the performance of the proposed method degenerates with an increase of the graph nodes because a few complicated graph-relevant calculations, such as matrix multiplications for MCL and subgraph isomorphism tests for gSpan, are involved. In future, other promising techniques will be looked into to address this problem and improve the performance of the proposed method. A more flexible subgraph interpretation scheme will be studied to take full advantages of the subgraphs obtained. Furthermore, abstract relations and individual relations should be categorized to build up a more complete relation hierarchy. 
Information integration in chemical process engineering based on semantic technologies. During the design phase of a chemical plant, information is created by various software tools and stored in heterogeneous formats, such as technical documents, CAE databases, or simulation files. Eventually, these scattered information items need to be merged and consolidated. However, there is no efficient computer support for this task available today. While existing technologies like XML are capable of handling the structural and syntactic differences between the heterogeneous formats, these technologies cannot resolve any semantic incompatibilities. For this reason, information integration is still largely performed manually – a task which is both tedious and error-prone. Semantic technologies based on ontologies have been identified as an appropriate means to establish semantic interoperability. This contribution presents an ontology-based approach for information integration in chemical process engineering. The underlying knowledge base, which is based on the formal ontology OntoCAPE, is presented, and the design and implementation of a prototypical integration software are described. Further, the application of the software prototype in a large industrial use case is reported. 1. Introduction The design process for a chemical plant is characterized by a sequence of design phases and involves different departments, disciplines, and contractors. In the different design phases, various software tools are used by the project team to generate typical engineering documents, like equipment lists and piping and instrumentation diagrams (P&ID), which capture the major engineering design information (cf. Fig. 1). Some of these software tools are of a domain-specific nature (e.g., process simulators, CAD or CAE systems, etc.); others are of generic type and independent of the requirements of engineering design processes (e.g., word processors, project management systems, etc.). Typically, the contents of the documents and data files resulting from the activities in the various design phases are heavily intertwined and overlapping, due to the strong dependencies between the information generated. Generally, the information models of domain-specific software tools suffer from a lack of a well-structured, standardized information representation (Bayer, 2003; Bayer & Marquardt, 2004). Thus, the exchange of data between the different tools is often hindered by the inherent heterogeneities of the underlying data sources (Embury et al., 2001). Regarding the engineering design phase, the heterogeneity of the data models of the applied CAE tools (cf. Fig. 1) is reflected in the diversity of the corresponding native export schemas of the respective tools. Typically, each tool has its proprietary data export schema, which is hardly related or linked to the export schema of any other tool. Also, a data export schema can produce different data files, which do not necessarily have identical structures in terms of scope or content: For example, a document may contain different engineering objects – e.g., apparatuses and machines – the characteristics of which are described by different attributes, basically representing the actual properties of the objects. Finally, one and the same CAE tool can be used for different tasks during the design phase and thus may export data to create different engineering documents in different versions – for example, a tool can be used to create P&IDs as well as technical specific sheets for apparatuses and machines. To date, no adequate solution is available in industrial practice to cope with all of these heterogeneities. Past approaches, mainly driven by the software vendors, addressed different slices of this overall problem: Data and information management issues were addressed separately, leading to stand-alone systems with limited capabilities and significant integration barriers. Accordingly, most of these tools are mainly applied in a stand-alone manner since a considerable implementation effort would be required to achieve interoperability with other dedicated tools. Consequently, the relations and dependencies existing between the different documents and data stores are often not captured explicitly and hence cannot be checked and managed easily. Thus, integration and management of this scattered data provides a real, yet unsolved challenge for the users and vendors (Marquardt & Nagl, 2004). As a result, the lack of appropriate tool interoperability and data integration are major cost drivers in the design phase. In this respect, the National Institute of Standards and Technology (NIST) in the U.S. recently reported that the lack of interoperability costs the U.S. capital facilities industry 15.8 billion dollars per year, compared to a hypothetical scenario where the exchange of data and the access to information are not restricted by technical or organizational boundaries (Gallaher, O’Connor, Dettbarn, & Gilday, 2004). Hence, there is an enormous potential for productivity gains, and consequently for cost reduction and quality improvement. Besides the general interoperability problem, engineering departments are also challenged by a more specific, self-imposed problem: The necessity to reduce time to market, forced by increasing global competition, calls for complex work processes like concurrent and distributed engineering. Accordingly, the different parties involved in a design project work in parallel and possibly even operate at geographically distributed sites. In particular, concurrent engineering (Kusiak, 1993) is a frequently applied concept to reduce project duration. However, the advantage of all these procedures, i.e., simultaneous work on related engineering issues, is at the same time its biggest disadvantage: In a fully sequential design process, team members in the downstream phases (i.e., the later phases of the design process such as detail engineering) work on finalized data sets that were created upstream (i.e., in the earlier design phases such as basic engineering). By contrast, in an overlapping, concurrent work process, the team members working downstream have to rely on preliminary information. Since the design processes in chemical engineering typically are of a creative and evolutionary nature, this preliminary information may frequently change substantially. Consequently, team members working downstream must readjust their work if colleagues from upstream change the design requirements or specifications in an unexpected way. Under these conditions, upstream engineering changes may cause significant downstream rework, potentially delaying the whole project (Eastman, 1980). For the efficient handling of such complex design processes, effective computer support is highly desirable to enable the integration and consolidation of the distributed design information. Such computer support has to fulfill two major requirements: (i) it has to provide a single point of access to the miscellaneous, heterogeneous data sources such that they appear to the user as a single, homogeneous data set, and (ii) it must determine the relations between the contents of these data sources in order to detect and reconcile possible inconsistencies in the distributed information. These relations can be both temporal and logical. Temporal relations result from versioning, where information item X is replaced by Y. Logical relations exist, if there is any conditional dependency between information items A and B. To our understanding, there is a clear mismatch between the urgency and significance of the concrete problem in industrial practice and the published solutions actually available and applicable in the chemical engineering domain. Due to the complex nature of the problem, it requires not only a thorough understanding of state-of-the-art concepts for information integration is required, but also a fundamental background on process engineering in order to grasp the problem in sufficient scope and detail. Solution concepts published by the computer science community tend to be highly generic and context free, and thus in fact non-applicable to a concrete chemical engineering problem. In the process engineering community, on the other hand, those few groups and organizations dedicated to this issue have published relatively little of their work due to the secrecy under which such (commercially valuable) systems tend to be developed. In order to close this gap, a research project was initiated as part of the collaborative research center CRC 476 (Nagl & Marquardt, 2008) at RWTH Aachen University, with the goal of tackling the aforementioned problem in close cooperation with partners from the chemical as well as the software industry (Evonik Degussa and Ontoprise). This paper summarizes the results of the aforementioned project. We present a prototypical ontology-based software tool for the integration and consolidation of distributed design data in chemical process engineering. Particular emphasis is put on the conceptual design and the implementation of the semantic information model of the tool. Furthermore, its applicability in an industrial use case is illustrated. Due to the close collaboration with our partners, all the developed concepts could be tested immediately in an industrially relevant setting to allow for a reliable proof of concept. The remainder of this paper is structured as follows: Section 2 introduces the necessary technical background and discusses the deficiencies of existing solutions. Section 3 summarizes the conceptual design of our approach and points to the important aspects. In Section 4, the implementation strategy for the integration prototype is introduced. In Section 5, the application of the integration prototype in an industrial use case is shown, including a discussion of its particular requirements, implementation details, and results. Section 6 gives an overview on related work available in literature. Finally, Section 7 summarizes the achievements and gives an outlook on future work. 2. Existing solutions and their deficiencies In order to enable efficient computer support of engineering design processes, essentially two aspects have to be taken care of: firstly the structural heterogeneity of data formats and stores, and secondly information integration.1 In the distributed database systems community, the problems arising from the heterogeneity of the data are classified as (i) structural heterogeneity (a.k.a. syntactic or schematic heterogeneity; e.g., Kashyap & Sheth, 1996; Kim & Seo, 1991) and (ii) semantic heterogeneity (Kim & Seo, 1991). While structural heterogeneity essentially means that different information systems store their data in different structures, semantic heterogeneity relates to the contents of information items and its intended meaning (Wache et al., 2001): A good example of semantic heterogeneity is the use of synonyms, where different terms are used to refer to the same concept. A comprehensive overview on the diverse types of semantic heterogeneities including their classification is given by Visser, Jones, Bench-Capon, and Shave (1998). Most of the approaches that have been published to date only address the problem of structural heterogeneity. Within recent years, several new technologies have become available for handling syntactic and structural heterogeneities. Particularly XML has gained wide acceptance as a way of providing a common syntax for exchanging heterogeneous data. However, the reconciliation of the semantic heterogeneities of the data models – i.e., the different meanings of the data within the context of the particular software data model – is still an unsolved problem (Cui, Jones, & O’Brien, 2001; Schneider & Marquardt, 2002; Tolk & Diallo, 2005). In particular, the identification of semantically identical data – i.e., data items with the same meaning – that are stored in different files or software tools is not possible so far. The ability of computer systems to communicate information and have that information properly interpreted by the receiving system in the same sense as intended by the transmitting system is also referred to as semantic interoperability. Particularly, the lack of semantic interoperability causes a significant overhead for the designers as they have to spend considerable time on the manual consolidation and integration of design data. 2.1. Traditional approaches to semantic interoperability There are three traditional approaches to achieve semantic interoperability: (i) brute-force, (ii) a global data standard, and (iii) interchange standardization (Gannon et al., 2005). While the bruteforce approach requires an enormous coding effort (for each pair of systems, an individual translation software must be created), the other two call for a common standard, with the effect that all the semantic differences disappear (at least from a data-exchange perspective) and there remains no need for translation. The global standard approach requires that all stakeholders (i.e., all application users and software vendors in a certain application domain) agree on a common format for the representation, processing, and storage of their data. The interchange standardization approach is less ambitious since the stakeholders only need to agree on a standard format for data exchange – the systems to be integrated may use their internal formats for data storage as long as each data sender generates the data according to the standard. Most of the industrially applied integration solutions are either using the brute-force or the interchange standardization approach. Commercial software systems for the integration and management of process design data typically employ a combination of both. Most of these systems are based on a central data warehouse (Jarke, Lenzerini, Vassiliou, & Vassiliadis, 2003), which can import and redistribute data created by application tools. These systems support the navigation and retrieval of data, and they provide some basic data management functions such as version and change management. A typical example is SmartPlant® Foundation (Intergraph, 2009). However, such off-the-shelf solutions can only process the proprietary data formats propagated by the respective vendors (nowadays often XML). Their applicability is thus limited to the data created by the vendors’ application tools, which normally represent only a minor subset of all data created in the course of a design project. Examples of such proprietary XML dialects are XMpLant (Noumonon, 2006) or cfiXML (EPlant, 2004). Extending these tools towards the handling of non-proprietary formats means that appropriate parsers and converters have to be implemented. More specifically, the engineering design data has to be mapped onto the internal data model of the integration tool. Since, in most cases, both the XML exchange schemas and the internal data models are poorly documented, this is not an easy task to accomplish. Numerous attempts towards an interchange standard have been made over the past years, not only in chemical engineering, but also in other engineering domains including STEP (Fowler, 1995), ISO 15926 (ISO 15926, 1999) or more recently CAEX (IEC 62424, 2006): STEP, also known as ISO 10303, covers information for the computer-interpretable representation and exchange of product manufacturing information. It consists of various parts, which can be used by different disciplines in many industries, such as the automotive, aerospace, and chemical industries. In spite of its detailed elaboration, it has never been fully accepted in the field of chemical engineering. Note that parts of STEP were originally implemented in the EXPRESS language, but have recently been translated into XML to meet the state-of-the-art implementation requirements. In the field of chemical engineering, one particular part of STEP has been further elaborated towards the ISO 15926, which has recently gained certain popularity. It is intended to incorporate all technical data in the life cycle of a chemical plant. Thus, it is supposed to facilitate the information exchange between all stakeholders starting from design to construction to operation. However, except in the oil and gas industry, the ISO 15926 has not found broad acceptance, due to the extreme complexity of the data model on the one hand (Bayer, 2003; Löffelmann, Zgorzelski, & Ahrens, 2005) and its rather narrow scope on the other hand (Bayer & Marquardt, 2003; Schneider & Marquardt, 2002). CAEX is a generic metamodel for the structuring of chemical engineering design data, which has been implemented in XML. It allows for the definition of specific modules, such as one for the description of a level control structure for a vessel, which can be reused and parameterized to a given task. An entire plant can then be composed of linked and parameterized modules. CAEX, however, does not force the modeler to use consistent elements, which may result in inconsistent representations of equivalent objects. The implementation of a global data standard is infeasible in engineering practice due to various reasons. The most important reason is the fact that in an interdisciplinary and international field, such as chemical process engineering, different standards are coexisting for the various disciplines, which makes it virtually impossible to agree on a single one which is obligatory. Principally, the idea of standardization has helped a lot to improve communication and syntax standards in various domains. Such standards incorporate the choice of language syntax, such as XML, or the choice of application layer protocols, such as http. However, standards regarding the information representation are much more difficult to agree on. Various ideas and approaches regarding information modeling have been proposed by academia within the last decades, including the work of Angus and Winter (1985),  ̃ Motard, Blaha, Book, and Fielding (1995), Banares-Alcántara (1995), Westerberg, Subrahmanian, Reich, and Konda (1997), Batres, Naka, and Lu (1999), Bayer and Marquardt (2003), and many more. A comprehensive overview has been provided by Bayer (2003). However, none of these ideas have found their way to industrial practice. Also, despite the tremendous efforts spent by the various initiatives (in particular STEP), a universal standard data models have not yet gained wide acceptance, neither in the chemical industry nor by the software vendors. Obvious reasons could be the incomplete coverage, which is always limited to parts of the life cycle, as well as the lack of coherence and consistency between the suggested standards stemming from (at least initially) independent initiatives. Experience has shown that it is hardly possible to agree on a life-cycle data model – not for technical, but rather for organizational reasons. Even agreement on the skeleton for a life-cycle data model on a coarse-grained scale can be difficult to achieve due to the divergent objectives of the various stakeholders (Schneider & Marquardt, 2002). Considering the diversity of the existing standards relevant to the different facets of process engineering, there is definitely a need for a framework into which they all could fit. However, since many of these standards have been developed from scratch, are based on different paradigms, and incorporate different legal (national) constraints, it seems unrealistic to find a framework that applies to all of them. But even if such a hypothetical framework had been developed, we would still face the problem that comprehensive standards, such as those briefly introduced previously, are not understood and used in a uniform manner. In fact, one major problem of existing data standards is the existence of different flavors: A flavor of a standard is evidenced when two different stakeholders interpret the same standard in two different ways and consequently find two ways for encoding the same piece of information. In other words: While an interchange standard can define an agreed set of labels for the exchange of design data, it does not solve the problem of semantic heterogeneity due to two reasons (Cui et al., 2001): First of all, it is very likely that more than one standard is applied in the future, and it cannot be assumed that these standards and their flavors will all be based on a consistent semantics. Secondly, it does not ensure a consistent use of a single terminology for the labeling the data contained in different files. Consequently, the problem of semantic heterogeneity would still exist if all data were exchanged using XML structures that are in accordance with standard schema-level specifications. Summarizing this section, we come to the conclusion that each of the three traditional approaches towards information integration has certain drawbacks that make them inappropriate, particularly if a large number of data sources are involved. In short, brute-force requires a large number of hand-coded converters that are costly to implement and difficult to maintain; a global standard is at least costly and may even turn out to be impossible to develop and to maintain. Also there are technological difficulties, and there is organizational resistance against a single uniform global data standard. Therefore, there will always be several competing and coexisting standards. Although interchange standardization certainly has advantages, such as local autonomy and the smaller number of converters required, it also has serious limitations: Comparable to a global standard, all stakeholders need to have a clear, common understanding about the domain in order to reach an agreement on the content. Furthermore, the exchange standard’s requirements will change over time. Continuous adjustments will obviously be necessary, which will affect and need to be harmonized between all systems. Finally, it still requires a large number of hand-written converters. In addition, all three approaches lack flexibility to adapt to changes because the data semantics are either hard-coded in the converters (i, iii) or in the standard (ii, iii). A suitable concept needs to overcome these shortcomings. As a consequence, the question whether or not the usage of an interchange standard is more efficient than working with the proprietary files is not relevant. There is no alternative to the usage of proprietary files or some in-house interchange standards, as the comprehensive application of universally applicable standards is simply not possible due to their absence. Accordingly, we are convinced that proprietary standards, partly accepted interchange standards, and in-house standards are the only available options in the near future. Compared to a ‘universal’ format, they have the advantage that they are de facto fast to agree on (although they often leave out debatable elements). However, they are flexible in terms of expendability and adaptability, and they are to some extent reusable. Thus, any solution to the problem of interoperability has to emanate from this situation. Finally, we can say that no accepted (global data or interchange) standard has been established to date, such that the companies are forced to work with various proprietary formats. As a consequence, many chemical and engineering companies are utilizing in-house solutions for data exchange, often based on XML schema specifications, such as PlantXML (Anhäuser, Richert, & Temmen, 2004). Such approaches can be seen as locally valid interchange standards. While these approaches are capable of solving the more basic problems of data exchange, they are less suitable for handling complex integration issues. 2.2. Ontology-based approaches to semantic interoperability Any appropriate solution of the problem of semantic heterogeneity has to formally specify the meaning of the terminology used by each system. Such a formal specification is a prerequisite for the computer to autonomously infer a translation between the different system terminologies. Unfortunately, the semantics assumed by a particular system are rarely documented – that is, there is no explicit representation of the semantics used by a particular data source (in the sense in that a data schema provides an explicit representation of the underlying data structure). Hence, to overcome this problem, the structured information stored in the XML document has to be connected to expressive domain knowledge, which relates meaning to the stored information within the context. To that end, ontologies have gained popularity as a convenient means for the representation of domain knowledge. Ontologies essentially constitute a structured framework for the storage of information and knowledge. More formally, the definition of Uschold and Gruninger (1996) states that “an ontology is an explicit specification of a conceptualization, typically involving classes, their relations and axioms for clarifying the intended semantics”. A well-structured ontology consists of reusable parts for different applications and captures consensual knowledge of a particular application domain. Accordingly, it can be used as the generic core of a knowledge base (Uschold & Gruninger, 1996). For an ontology-based solution to semantic interoperability, as it is proposed in this contribution, essentially two basic software components are required (Neches et al., 1991): a knowledge base, which contains generic domain knowledge (i.e., the ontology) as well as concrete facts about the case under consideration, and an inference engine (also known as reasoner), which processes the knowledge and facts stored in the knowledge base and autonomously inferences a solution for the case at hand. The term ‘semantic technologies’, as it is used in this contribution, refers to software systems that use ontologies as internal data models. A brief overview on related work and some exemplary ontology-based applications is provided in Section 6. 3. Information integration based on semantic technologies Our objectives have been (a) to develop a sound concept for integrating and consolidating semantically heterogeneous data across the different phases of a design process, and (b) to implement this concept in a software prototype. The software prototype should provide a convenient way of resolving semantic heterogeneities (requirement #1); moreover, it should be easily adaptable to different proprietary data formats (requirement #2). The prototype has been named Comprehensive Information Base, and we will refer to it as CIB in the following. The overall architecture of the CIB (conceptually as well as in the concrete implementation) consists of three basic modules (cf. Fig. 2): (i) a knowledge base, which holds the domain knowledge required for the integration and consolidation task; (ii) a peripheral software environment (PSE), which administrates and visualizes the integrated and consolidated data and provides functionality such as the import and export of XML data; and (iii) a powerful inference engine, which evaluates the knowledge modeled in the respective ontologies in order to reason about the facts. Regarding our requirements, the reconciliation of semantic heterogeneities (requirement #1) is performed by the semantic technologies (knowledge base, inference engine). As for the easy adaptability (requirement #2), the concept assumes that all data to be integrated is provided in the XML format – as long as this assumption is true, the CIB is very flexible in so far as it allows for the import of any XML dialect. In the following, we will primarily focus on the conceptual design of the knowledge base, putting special emphasis on the design of the underlying ontology; the characteristics of the PSE will only be sketched briefly. Overall, the development of the knowledge base was guided by two crucial aspects, which will be discussed in the following: the design of a flexible and intelligible structure, and the formulation of a concise but expressive knowledge base. 3.1. Design of the CIB as a mediation layer The goal of the design is to provide a flexible, open, and integrative framework for information integration, which meets the requirements of industrial practice. Therefore, it has to be able to process standard formats and it has to provide means to be easily adopted by engineering departments of chemical companies and EPC-contractors.2 Accordingly, the CIB basically represents a mediation layer, which is placed between the user and the data sources (Wiesner, Morbach, & Marquardt, 2008; cf. Fig. 2). It is organized by means of the so-called hybrid ontology approach (e.g., Lenzerini, 2002; Paton, Goble, & Bechhofer, 2000; Wache et al., 2001), which enables flexible manageability and easy expandability. The main idea is to use a shared vocabulary, which contains the basic terms of a domain and can be applied to all data sources in order to describe their respective semantics. That way, the concept is flexible in the sense that the vocabulary is generally applicable – i.e., independent of the syntax and structure of the data sources – and it does not need to be modified when a new data source is to be added to the CIB. In terms of easy expandability, any new data source can conveniently reuse the shared vocabulary. To that end, all mappings between the shared vocabulary and the previously integrated data sources have been conserved and thus can be reused for the alignment of the new data source. The hybrid ontology architecture consists of a so-called global ontology – i.e., the shared vocabulary – and several source ontologies, which are linked by means of mappings (Lenzerini, 2002) as shown in Fig. 3. Each data source (i.e., each XML document generated by some application tool) is described by a local source ontology, which basically replicates the document (its original structure as well as its data) in the ontology language of the CIB. In this context, the term ‘XML document’ denotes a set of XML files that (i) originate from the same source and that (ii) conform to the same XML schema. Such XML files are considered as different versions of the same XML document. The global ontology functions as a predefined vocabulary, by means of which a representation of the contents of the XML documents can be given. This neutral representation is independent of any native XML schema specific application tool. Thus, the global ontology provides the necessary vocabulary for a unified description of the data in the source ontologies. The entities of the source ontologies are linked to the entities of the global ontology via mappings, thus making the semantics of the source documents explicit. Thus, meaning is assigned to the data stemming from the different tools. The interactions between a source ontology and its corresponding XML document are handled by a bidirectional converter, which mediates between the XML representation and the ontology language of the CIB (Wiesner, Morbach, et al., 2008; cf. Fig. 3). There are different options for the realization of such a converter: The most common method directly translates the XML data into the representation of a “knowledge-carrying” ontology (which would correspond to our global ontology). However, our experiences so far show that the specification of generally applicable conversion rules for the realization of suchlike converters is a fairly complex task; moreover, such conversion rules are difficult to maintain due to the lack of transparency. Hence, we decided against such a type of converter. Instead, we built a converter that performs a pure syntax conversion: The data of the respective XML documents are represented as instances of intermediate schemas inside the knowledge base: The intermediate schemas comprise identical data structures and labels as the schemas of the XML document, but they are formalized in the respective formal ontology language. In the following, we will call these intermediate schemas ‘import ontologies’ (cf. Fig. 4). The extraction of the relevant data is conducted by mappings in the knowledge base. The user may interact with the CIB via the global ontology, assisted by a suitable user interface hiding all the complexity of the ontology (cf. Fig. 2). The global ontology serves as a global query schema3 : First, the user formulates a query in the terminology of the global ontology.4 Next, the inference engine transfers the query to the source ontologies via the existing mappings. That way, the inference engine retrieves and combines matching information from the different source ontologies. The results are then retranslated into the vocabulary of the global ontology and finally presented to the user via the user interface. The main advantage of this architecture is its ability to distribute the complexity of the integration and consolidation tasks to different steps, each of which are represented by distinct, generally applicable, and reusable modules within the global ontology. For the problem at hand, the global ontology is decomposed into different modules, which fulfill either integration or reconciliation tasks (cf. Section 3.3). For instance, data integration can be performed independently of data reconciliation if required. The use of these different modules for the diverse requirements of the CIB renders the approach highly flexible. 3.2. Global ontology requirements The above-described approach requires an appropriate (global) ontology that can (i) relate meaning to design data within the domain of chemical process engineering, and that can (ii) fulfill the task of information integration. Accordingly, a domain-task ontology had to be developed. Generally, a domain-task ontology5 is defined as an application-independent task ontology that is reusable in a given domain, but not across domains (Gomez-Perez, FernandezLopez, & Corcho, 2004). Hence, our global ontology corresponds to a domain-task ontology that has been created by extending an existing domain ontology and combining it with a given task ontology. The domain ontology OntoCAPE (Marquardt, Morbach, Wiesner, & Yang, 2010; Morbach et al., 2007) has been developed for applications in the domain of Computer-Aided Process Engineering and is thus particularly applicable to the integration of design data in chemical engineering. It is designed to be usable in different application contexts, and it is independent of a specific implementation (Morbach, Wiesner, & Marquardt, 2009). Thus, it could be easily reused for the purpose of our global ontology and adapted to the task at hand. To enable the task of information integration, specific OntoCAPE modules have been extended and customized in terms of adding relations, attributes, and axioms to enable, for example, the identification of design inconsistencies. Besides, a specific task-ontology, which is based on the ideas of the document model (Morbach, Hai, Bayer, & Marquardt, 2008), was added to the domain knowledge. Essentially, it is used to describe the formal structure and version history of the XML documents. By combining the document model and OntoCAPE, the contents of the engineering documents and their mutual dependencies can be modeled. Eventually, a global ontology has been derived from the domain ontology OntoCAPE, which incorporates also parts of the task ontology ‘document model’. Overall, the global ontology in its entirety is capable of fulfilling the following tasks. Firstly, redundant information has to be identified and rejected. In the first place, this concerns information objects that occur in multiple XML documents. These objects must be detected, and their attributes have to be reconciled within the consolidated data set. If the attributes of such an object are distributed across different XML documents, they have to be assembled, and outdated attributes have to be replaced by the current ones. Secondly, implicit dependencies existing between the different information objects have to be represented as explicit ones: Examples are topological relations between different plant items or the temporal and logical interrelations between diverse versions of these objects. 3.3. A knowledge base for information integration A concise and expressive knowledge base for information integration has to contain a global ontology as well as source ontologies as stated in Section 3.1. Fig. 4 represents all the sub-ontologies that constitute the knowledge base; these sub-ontologies belong either to one of the source ontologies or to the global ontology. Each source ontology essentially comprises two sub-ontologies – called document ontology and import ontology – which are specific to the XML schema of an application tool. As for the generic global ontology, three distinct parts can be distinguished, which were partly derived and customized from suitable OntoCAPE parts (namely the integration ontology and the consolidation ontology) and partly derived from the document model (namely the versioning ontology). Although all three parts of the global ontology are modeled as discrete modules, they all contribute to the overall shared vocabulary used for the task of information integration. In the following, the different ontologies will be described in detail, placing special emphasis on their contents and their specific roles within the knowledge base. 3.3.1. Import ontology As explained in Section 3.1, the source ontologies describe the contents of the XML documents in the ontology language of the CIB. The import ontologies simply replicate the data structures and labels of the XML documents in the ontology language (note that the information contained in the XML documents is only hierarchically structured, but does not capture any other dependencies). Every XML document belonging to a certain native XML schema is represented in a self-contained import ontology. Thus, for each XML schema, there is a single import ontology, which collects all the data that is formatted according to that particular XML schema. For example, all data stemming from the particular XML schema used by the CAE tool Comos® Feed (Comos, 2009) is gathered in an import ontology called ‘Comos Feed Schema’. 3.3.2. Document ontology In most cases, the data represented in the import ontologies cannot immediately be used by the global ontology. Typically, there are significant differences between the structures and contents of the diverse data sources (and thus of the import ontologies), which makes a direct mapping to the global ontology difficult. Therefore, an intermediate step is introduced, in which the so-called ‘document ontology’ is generated. This step basically comprises the following tasks: Not all data gathered in the import ontology are relevant to the task of information integration. The document ontology retains only those data which have been identified to be of special importance, in the sense of “technical master data”. Thus, by means of the document ontology, the complexity and quantity of the import data is significantly reduced. Further relations and attributes are added to account for import context information, which goes beyond the purely structural information. Typically, such information comprises relations crosslinking between information objects. A typical example would be a measurement device that belongs to a specific control loop: In XML, these links are typically realized only indirectly by means of internal identifiers. In the ontology, by contrast, a ‘measurement device’ and the ‘control loop’ would be represented as classes which are linked by the relation ‘has measuring device’. Clearly, the generation of such a document ontology requires expertise about the structures and contents of the respective engineering documents at hand. Thus, the required knowledge, in terms of classes, relations and axioms, can only be created by an expert. Since XML documents are typically bound to specific CAE tools, which in turn obey proprietary XML export schemas, such document ontologies and their linkage to the import ontology have to be built (only once) for each native XML export schema. Once the knowledge has been modeled, it applies to all successive instances stemming from XML documents obeying the same schema. The data contained in the document ontology constitute the basis for executing the semantic integration and consolidation. 3.3.3. Versioning ontology The versioning ontology is part of the global ontology and holds the basic concepts necessary for the versioning of engineering objects. Therefore, a uniform description of the structural composition of documents is required, such as the one proposed by Morbach, Hai, et al. (2008). The versioning, as it is applied here, assumes engineering documents, the contents of which can be represented by delimitable entities with definable properties and dependencies. For example, an engineering document may be composed of different (engineering) objects, which represent uniquely identifiable processing units or ‘plant items’. The characteristics of these objects may evolve within the course of a design project. Accordingly, different versions of each object may exist in different documents. Consider the example of a distillation column, which is given the unique identifier K-4001: In the course of the design project, this column is specified in various documents, e.g., in a process data sheet or a technical specification document. In order to describe the history of these objects their different versions and configurations have to be captured to follow their progress over time. To this end, specific attributes have to be assigned to all engineering objects of a document version, such as the creation time, the originating software tool, and the associated project. On the basis of this information, versioning can be performed. Versioning aims to satisfy two needs: Firstly, the most current entities captured by the respective document versions can be identified and possibly assembled in a single document for further processing. Secondly, the tracking of changes between different versions of a particular engineering object becomes possible. In particular, intentionally discarded or accidentally forgotten plant items can be determined. 3.3.4. Integration ontology The integration ontology holds knowledge about the constitution and description of complex technical systems. Such knowledge is captured by the OntoCAPE module ‘technical system’ (Morbach, Bayer, Wiesner, Yang, & Marquardt, 2008), which is applied and extended for the integration task. In doing so, it is not relevant what kind of function such a technical system fulfills – e.g., if it represents a reactor or an instrument device. For the integration, it is only important that each plant item can be clearly identified as an instance of a particular technical system (e.g., according to a unique identifier) and that all its subsystems (e.g., fixtures or nozzles) can be derived unambiguously from the application context. For example, the distillation column K-4001, its column internals KE-4001, and the diverse nozzles N1, . . . N20 can be identified from the respective document and related to each other. Note that, even though integration by means of unique identifiers may not seem to be a universally applicable option, it is in fact a far-reaching solution since such identifiers are widely applied in chemical and engineering companies for an unambiguous identification of plant items and their characteristics. The goal of the integration task is to merge the information items from disparate sources. By merging, we mean to gather and assemble all available information about a single engineering object (e.g., an apparatus or a pipeline), which may be scattered across different source ontologies. Different files of the respective CAE tools may represent different characteristics of identical real world objects. As a result of the integration task, one document version for each engineering discipline is produced, which includes all the relevant technical systems, subsystems (e.g., fixtures and nozzles), and properties (e.g., technically relevant attributes). In doing so, the individual information items from different documents and thus different CAE tools may be merged and summarized. Furthermore, an inventory of the plant items already specified in the CAE tools is obtained as a side effect. 3.3.5. Consolidation ontology The consolidation ontology eventually holds process engineering domain knowledge, which is provided by OntoCAPE and which is customized according to the prevailing requirements. Since the data captured by the engineering documents correspond either to a functional or a constitutional description of a chemical plant, this part of the global ontology holds knowledge about the function of the processing steps and their realization by means of plant items and their connectivity. Particularly, the mereological and topological concepts for the description of plant items, provided by the OntoCAPE modules ‘process’ and ‘plant’ (Wiesner, Morbach, Bayer, Yang, & Marquardt, 2008), have been reused. The concepts applied in the consolidation ontology basically represent a specialization of the concepts represented in the integration ontology, since specific features regarding the composition of plant items and their interconnections have to be considered. For example, equipments and pipes follow different rules with regard to connectivity: a piece of equipment may have connections to various pipes and instruments, whereas as a pipe typically connects exactly two pieces of equipment. An important principle, which is for instance applied for checking the consistency of interconnections, is the transitivity of properties. That way, e.g., the pressure levels within an integrated processing system or the kind of insulation of neighboring systems can be checked. As an example, a vessel has several nozzles by which it can be connected to other plant items (e.g., to instrument devices or pipes). In order to check if such a connection has been properly specified, the properties of the interfaces need to be assessed: e.g., if nozzle N1 has a nominal diameter of 50 mm and if N1 is connected to pipe R10, which also has a nominal diameter of 50 mm, the diameters of nozzle and pipe match. 3.4. Advantages of the proposed approach The proposed approach compensates for the most severe drawbacks of all of the established approaches presented in Section 2.1. In contrast to the brute-force approach, we only need to translate between the data sources and the global ontology: Thus, we need to create only one data converter for each data source (instead of one converter for each pair of data sources). Also, the semantic heterogeneities do not have to be resolved individually by the converter, but by a knowledge-carrying component of the CIB. Compared to the global data standard, the proposed hybrid method has the ability to lower or even eliminate the necessity of reaching a global agreement on the entire domain terminology. The global ontology is different from a data standard in that it contains rather universal, generally accepted concepts (e.g., technical system, instrument, and pipe). These general concepts show up in very similar form in any of the proprietary data models and typically do not need special consideration. According to experiences, differences in the conceptualization and representation of the domain knowledge only appear when it comes to details. The specific differences in the representation of the domain knowledge in the various tools are typically not only represented by special attributes in the data sources, but also by further refinement of the classes. This results in a large number of alternatives typically based on different classification principles in the various CAE tools. For instance, often special classes for pieces of equipment are introduced, such as a classification of pumps according to their function (e.g., centrifugal pump or hose pump). Even though the data models of such different pump classes only differ in few details, they are not entirely consistent. Applying only general concepts, as it is proposed here, constitutes the context for a more specific characterization of concepts without the need for a very fine-granular classification. For instance, a pump X in CAE tool A may correspond to a pump Y in CAE tool B, but it may not be classified identically and may not hold the identical set of attributes. However, since the most important attributes are captured by both representations, although in a slightly different manner, a matching of these crucial attributes and subsequent data integration is possible. In other words, focusing on these crucial attributes only facilitates understanding and mediating the specific meanings captured by the document ontologies: They represent the actual mediators in the system. They reduce the data to information by applying knowledge about the sources and user requirements (Wiederhold, 1992). Contrary to the approach of interchange standardization, the use of source ontologies and the specification of document ontologies allow for the autonomy and diversity of the data sources. In consequence, the data can be identified according to their context and can be classified according to the generally agreed basic domain knowledge in the global ontology, i.e. the problems imposed by flavors can be easily overcome. It is furthermore noteworthy that the knowledge captured in the global ontology is selectively reusable, as it is independent of the syntax and structure of the heterogeneous sources (Visser, Stuckenschmidt, Wache, & Vögele, 2000; Wache et al., 2001). If the structure of the existing sources is changed, or if other data sources are added from hitherto unconsidered CAE tools, only the source ontologies and the respective mappings between the source ontologies and the global ontology must be adjusted; all other semantic coherences are conserved. If additional aspects of the domain have to be modeled, the global ontology can be modified according to the domain ontology it is based on. Afterwards, it has to be made sure that the document ontologies capture and translate the required information, and the mappings to the source ontologies have to be extended and possibly adjusted if necessary. Finally, it should be mentioned that the global ontology provides a comprehensive view on the scattered data sets. It thus qualifies as a neutral point of access for the search and navigation within the data (Arens, Knoblock, & Hsu, 1996). 4. CIB implementation Ontologies can be modeled with different modeling techniques. Also they can be implemented in various kinds of languages (Uschold & Gruninger, 1996). For an overview on the various languages, including the corresponding underlying mechanism, the reader is referred to Neches et al. (1991) or Gomez-Perez et al. (2004). The tasks of merging and consolidating data require an ontology language and an associated inference mechanism which enables the reasoning of data given in rather complex contexts with many relations between the data objects. Also, a powerful inference engine is required to support the chosen mechanism to perform information integration even on an industrial scale. Two languages are basically qualified for such a task: the W3C standard Ontology Web Language (OWL) (Bechhofer et al., 2004) based on description logics, in combination with the Semantic Web Rule Language (SWRL) (Horrocks et al., 2004), which combines the rule language RuleML (Rule, 2009) and OWL, and the deductive ontology language F-Logic (Frame-Logic) (Kifer, Lausen, & Wu, 1996), which is an object-oriented and logic-based language originating from the deductive database community. Both languages, OWL in combination with SWRL on the one hand (Golbreich & Imai, 2004) and F-logic on the other hand (Maier, Aguado, et al., 2003), provide the expressiveness required to perform the integration tasks. However, according to Matheus, Baclawski, Kokar, and Letkowski (2005), particularly SWRL suffers from a number of drawbacks which renders it unsuitable for our application. The biggest disadvantage of SWRL is the need for checking for the consistency (i) within, (ii) across and (iii) between SWRL rules and OWL ontologies upon which they are built. Moreover, regarding practical implementation, further critical issues are reported including (i) the lack of negations, (ii) the need for procedural attachments, and (iii) the need for implementation of SWRL built-ins. By contrast, the deductive ontology language FLogic and the related inference engine OntoBroker (Angele, Kifer, & Lausen, 2009) have already been successfully tested in industrial applications (Maier, Schnurr, & Sure, 2003). According to Maier, Aguado, et al. (2003), the inference mechanism of deduction (i.e., the execution of production rules) is particularly useful for merging and consolidating distributed information. Also, more recently, Rector and Stevens (2008) reported on severe drawbacks of knowledge-driven applications based on OWL regarding usability and reliability. The deductive ontology language F-Logic and the inference engine OntoBroker have been chosen for implementation for the following reasons: Unlike most other ontology-based systems available today, OntoBroker is scalable and thus suitable for the processing of large data sets, as successfully demonstrated in industrial applications (Maier, Schnurr, et al., 2003). F-Logic (Kifer et al., 1996) allows the definition of rules for integration and mapping purposes as well as the formulation of expressive queries. It is a deductive, object-oriented database language, which combines the declarative semantics and expressiveness of a deductive database language with the rich data-modeling capabilities supported by the object-oriented data model. 4.1. Modeling in F-Logic For the modeling of the document ontologies and the global ontologies in F-Logic, the design environment OntoStudio (Weiten, 2009) has been chosen as an implementation basis. OntoStudio is a design environment for ontologies and semantic applications, which enables the representation of ontologies, the definition of mappings, and the formulation of queries. It further incorporates a graphical modeling interface for the creation of ontologies. Subsequently, these ontologies can be directly used as implementation models for the information integration tasks. Ontology engineering is furthermore supported by the inference engine, which checks the ontology for consistency, thus revealing logical errors and ensuring a reliable, validated data model. F-Logic not only supports the specification of mappings between individual classes and/or instances but also the definition of general integration rules. Rules encode generic information of the form: Whenever the precondition is satisfied, the conclusion also is. The precondition is also called the rule body and is formed by an arbitrary logical formula (e.g., using OR, NOT, AND, etc.). The conclusion, called the rule head, is derived from the facts stated in the rule body. Thus, rules may be used to represent declarative knowledge in the form “if A then B”, where A and B are statements about the extracted information expressed by means of ontological terms. The major advantage of using rules to formalize knowledge about concept relationships is that the knowledge can be expressed at the highest level of abstraction at which it is applicable. Thus, instead of establishing mappings between individual data items, the integration policy can be specified in more general terms. For instance, the assignment of all relevant properties (which are represented by attributes in the engineering documents) to plant items can be performed by a single rule statement within the CIB. Note that this rule applies to any part of any plant. Thus, the rule needs to be stated only once and can then be used for different tasks in the global ontology. This capability of advanced reasoning allows partial automation of the information integration process. Also, emanating from a given knowledge base, rules offer the possibility to derive new information – that is, to make implicit knowledge explicit and thus to extend the knowledge base incrementally. The newly derived information becomes part of the knowledge base and is thus available for the evaluation of subsequent queries. Rules have further advantages compared to conventional technologies regarding querying. The applicability of rules to the given data is automatically checked by the inference engine. Accordingly, the practicability of all queries using the particular rules is guaranteed. Also, rules are more intuitive and less error-prone with regard to querying than conventional database integration techniques (e.g., integration via SQL queries), especially in complex contexts with many relations between the data objects (Maier, Aguado, et al., 2003). Deductive logic enables to reduce the complexity of a query, since complex correlations between several classes may be expressed by a single relation. By contrast, the effort to realize SQL queries significantly increases with complexity. In the following, two examples of specific rules are given. The first example deals with the determination of a valid connection between a piece of equipment and the attached pipe. Therefore a couple of rules have to be considered for consistency checking. Typically, the pipe object specification holds the connection information for such an interconnection. It usually contains information about (i) the piece of equipment the pipe is connected to and (ii) to which particular part of the piece of equipment (e.g. nozzle) a connection exists; furthermore, (iii) a number of connection properties are listed. In order to check whether there is a connection between the pipe and a piece of equipment at all, only aspect (i) has to be considered. Yet for determining the validity of the connection, further checks have to be performed. These checks assess whether the right nozzle is used and whether the properties match, but also whether, by mistake, more than one pipe or instrument has been connected to the particular nozzle. When aspect (i) holds true, a relation of type ‘is connected to’ can be established between the instances representing the respective pipe and the piece of equipment.6 Then, after all further aspects have been verified, the connection between the pipe and the piece of equipment is finally regarded as valid. Exemplarily, the rule stating the validity of a nozzle-pipe connection (ii) is briefly explained. Informally, it reads as follows: IF THEN a piece of equipment AND a pipe belong to the same unit7 AND the piece of equipment possesses a nozzle AND the pipe has some connection information AND the identifiers of pipe and piece of equipment match the connection to the nozzle is classified as not inconsistent. A graphical representation of this rule generated by OntoStudio is given in Fig. 5. Essentially, it shows four different types of symbols: Ovals represent classes, rectangles denote predefined class-specific attributes, arrows indicate a relation interconnecting classes, and lines show which specific attribute (possibility including its value) are assigned to classes and may be further compared to other attributes. As a second example, the reconciliation of values belonging to major engineering attributes, is explained in the following. Any reconciliation is based on comparatively simple rules, which are formulated within the consolidation ontology as part of the global ontology. Such rules check, for instance, the relations between operating values, design values, and test values of an attribute – i.e., the operating value of a quantity must be less than or equal to the design value of that quantity, which in turn must be less than or equal to the test value. Another type of rules check if the value of a quantity is within a specified range, i.e., the min value must be less than or equal to the nominal value, which in turn must be less than or equal to the max value. The inference engine then determines all violations of these rules for all instances belonging to the respective attributes. A simplified representation of a rule comparing the minimal and the nominal value of an arbitrary physical quantity is as follows: the unique identifier of two plant items X and Y match AND X is assigned a min. design quantity AND Y is assigned a nominal design quantity AND the min. design quantity is greater than the nominal design quantity the min. design quantity of the plant item X is inconsistent. X and Y are variables which can represent all instances of the respective plant items. Since the F-logic rules obey first-order logic, the comparison includes the case that X and Y represent one and the same instance. Analogously, more specialized rules can also be stated for a combination of the previously mentioned value ranges and condition ranges, such as for a pressure checking as stated below: the unique identifier of two plant items X and Y match AND X is assigned a max. design pressure AND Y is assigned a max. operating pressure AND the max. operating pressure is greater than the max. design pressure the attribute max. operating pressure of the plant item X is inconsistent. Again, it should be stressed that the novelty of this approach does not consist in the formulation of these rather obvious rules; rather, it consists in the reduction of the required implementation effort: The designer/user only needs to specify a small number of rules at a high level of abstraction. The time-consuming work of applying the rules to the numerous application cases on the data level is then automatically performed by the inference engine. 4.2. Realization of a peripheral software environment The realization of the proposed CIB calls for an additional software environment to be of use in industry. Therefore, a peripheral software environment (PSE) has been developed, which enables the user-friendly application of the CIB by means of different query and management interfaces. The main purpose of the PSE is to hide the complexity of the ontology by mediating between the user and the underlying system. Ultimately, the project designer has to be able to operate the CIB without knowledge about the structure of the knowledge base and its underlying ontologies. However, if required, the knowledge base can be viewed and adjusted by means of the ontology editor OntoStudio at any time by an expert user. In order to gain acceptance in industrial practice, a graphical user interface (GUI, cf. Figs. 6 and 7) is mandatory. The GUI visualizes the results on demand in an intelligible manner and provides some additional data management functionality. This allows, on the one hand, for the visualization of the integrated and consolidated data inferred by the inference engine. On the other hand, it facilitates tasks like the import and export of the respective XML documents or the adjustment of the scope of data. Besides, the software system has to support collaborative work on different computers. To that end, data redundancy has to be avoided to the extent possible, which can be realized by concepts enabling shared access to centrally stored consistent data sets for every user. The demand for collaborative and distributed work without data redundancy requires a client–server system where the data is stored on a central server. For our purposes, OntoBroker has been chosen as the server software. As an additional benefit, this architecture allows to run the inference engine on a server machine with high computing power. Accordingly, the user interface and some other parts required for client–server communication constitute the client. All parts of the ontology – i.e., rules, queries and facts – are stored in F-Logic within the OntoBroker system. OntoBroker can be configured to store all data either in CPU-memory or persistent, i.e., in a relational database (Oracle, H2DB, etc.). The development of the GUI has been guided and tested by expert project engineers from Evonik Degussa. In order to achieve a platform-independent realization of the GUI, the Standard Widget Toolkit (SWT) has been used in combination with the development environment Eclipse (Budinsky, Brodsky, & Merks, 2003). Eclipse facilitates the realization of different ‘perspectives’. These perspectives can be customized by arranging window areas called ‘views’. These views can be filled with different SWT elements like buttons, drop-down boxes, tables, and trees (cf. Fig. 6). 5. Industrial application of the CIB Fig. 7. Screenshot of the GUI. Semantic inconsistencies are marked with red color. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.) The integration prototype has been applied for solving a specific information integration problem at Evonik Degussa. In this particular integration problem, the engineering data is represented in a specific in-house XML schema called PlantXML (Anhäuser et al., 2004). It should be noted that, although PlantXML has been chosen as the first application case, the CIB is essentially designed such that it can handle arbitrary XML formats. Evonik Degussa Engineering is the in-house engineering department of Evonik Degussa, the world’s biggest producer of specialty chemistry. The engineering department is responsible for such different tasks as feasibility studies, conceptual front-end engineering, basic and detail engineering, and construction of complete chemical plants. However, the engineering department mainly focuses on basic engineering and extended basic engineering, respectively. For detail engineering, external contractors are typically commissioned. PlantXML is an existing in-house solution for the information exchange between application tools used by the engineering department of Evonik Degussa. It defines specific XML schemas for the different main engineering objects: (i) XML-EQP for the design of machines and apparatuses, (ii) XML-EMR for the design of instruments and control systems, (iii) XML-RLT for piping engineering, and (iv) XML-SiAr for the design of safety-related fittings and safety valves. Custom-made converters handle the import and export of the XML files to and from CAE tools by mediating between the in-house standard and the internal data models of the application tools. To allow for an evolutionary and non-deterministic workflow, partial exports of documents are supported by PlantXML. PlantXML does not claim to be a completely defined, generally applicable data model. Instead, it represents a pragmatic solution tailored to the specific needs of Evonik Degussa Engineering. Thus, only task-specific engineering data are modeled and transferred; specific data of the software tools are not considered. Also, interrelations and dependencies between the data stemming from different schema are not explicitly modeled. Hence, although the respective schemas are structurally similar, they are not harmonized with respect to content. Hence, the contents of the four XML schemas are partially overlapping. The structure and nomenclature of specific plant item attributes are organized according to the datasheets, which were traditionally used for manual data exchange between the respective disciplines. Though the main purpose of PlantXML is to establish an overall exchange layer for engineering design data, it also provides a good starting point for a thorough data integration as it supports the access to structurally homogenous data sets. In the course of a typical project, different data sets of real world objects are generated. These versions arise for two different reasons: either they describe different functionality, or they are frequently modified over time. To shorten processing times, often partial exports of PlantXML files are transferred, which do not contain a full specification of a particular plant item. Also, PlantXML supports the exchange of subsets of data – that is, the contained objects may be fully specified, but represent only a part of the entire data stock of a design project, e.g., the part of the project which is assigned to a particular sub-contractor. Due to this kind of data exchange, no stringent versioning of the documents is possible. Accordingly, the project engineer is not able to determine the current status of the overall data stock on the basis of a simple version number of a document. The use case consisted of different scenarios with varying scope and complexity. It included different sets of PlantXML files containing data of different complexity, reaching from simple test data to real project data related to a design of a mid-size chemical plant, which comprised up to several hundred electrical and instrumentation devices and pipes, and up to 30 pieces of equipment. 5.2. Implementation As a first step of each information integration scenario, the engineering data distributed across the various PlantXML documents have to be assembled in the CIB. Subsequently, they can be merged and consolidated. Within the CIB, the data sets stemming from different XML documents are kept separately and are individually prepared for integration. A distinct import ontology is created for each PlantXML schema. The elaboration of the required document ontologies has been carried out in close cooperation with the respective experts from Evonik Degussa. 5.2.1. Versioning The main focus of the case study is the retrieval of the overall current status of the design data. In this context, the term ‘XML document’ denotes a set of XML files that (i) originate from the same source and (ii) conform to the same XML schema (e.g., XMLRLT). Such XML files are considered as different versions of the same XML document. As an example, all data stemming from the CAE tool Comos® Feed, which can be either converted to the EQP schema or the EMR schema, are separately prepared. Preparation includes the detection and elimination of outdated and redundant information as well as the resolution of versioning conflicts. Such preprocessing is necessary since there may be several versions of an XML document, as mentioned previously. In other words, each version may include both current information as well as outdated and/or redundant information. The resulting versioning conflicts need to be resolved before integration. The current status of the XML documents obeying one PlantXML schema and stemming from one source (CAE tool) is referred to as sum-file in the following. After the generation of a sum-file, it can be exported into the repository it came from, and it can be integrated in the design process if required. 5.2.2. Integration On the basis of the sum-files, the semantic integration of the design data can be performed. It essentially aims at two goals. On the one hand, it determines which plant items are specified in the different CAE tools. This is required since, for instance, data belonging to the EMR schema are processed by different CAE tools such as Comos® Feed, SmartPlant® P&ID (Intergraph, 2009) and others. As a result of this determination, conclusions can be drawn regarding the processing status within the respective CAE tool. Thus, it is identifiable, which plant items are not yet specified in a particular CAE tool and hence whether the project’s schedule could be delayed. On the other hand, a classical integration of the data sets can be accomplished, such that plant items, which are present in different files of the respective CAE tools, can be merged into a single file. Consequently, the semantic integration generates one version of each PlantXML schema, which contains all relevant plant items, its fixtures and nozzles as well as all relevant technical properties. In doing so, the plant items stemming from different CAE tools can be combined in any order, as long as they obey the same schema. The set of all integrated plant items is denoted as master-file, which can be exported to the respective CAE tools like the sum-file. 5.2.3. Consolidation Eventually, based on the current and integrated data (masterfile), the semantic consolidation can be performed. In the case study considered, only the realization perspective of plant items has been of interest. The focus has been on the following two aspects: (i) consistency checking of topological and mereological information, such as the matching of the inner diameter of a nozzle and the connecting pipe, or the existence of similar fixtures and nozzles in different specification of identical plant items; (ii) reconciliation of “crucial” design properties such as temperature, pressure, or construction material. The consistency check is performed in two steps: First of all, semantically equivalent properties and plant items are consolidated within each PlantXML schema, by checking for equality. For example, the minimal design temperatures of a particular apparatus should be identical, regardless of the CAE tool the data originates from. With respect to mereological consistency, a temperature loop should comprise the same sensors and actors, independent of the CAE tool it has been specified in. In addition, semantic interdependencies between plant items on the one hand and properties on the other hand can be reconciled in a second step. For instance, a rule is formulated for checking the design properties, stating that the minimal design pressure of an apparatus always has to be less than its maximum design pressure. Regarding the consistency check of the connectivity between plant items, we refer to the previously mentioned example of a connection between a nozzle and a pipe. The prototype would detect that the pipe is connected to a nozzle of a vessel, and thus establish a link between the pipe and the vessel. In summary, the following functions are realized within the CIB: Redundant information can be detected and removed by means of the integration. This applies particularly to data entities which occur in different versions describing an identical real world object. Those are identified and are represented by the most current entity within the merged data stock. In case the attributes of such an object are distributed over different PlantXML files, only the most current attribute values are considered within the CIB. Implicit dependencies between the entities can be identified and presented in an explicit manner by means of the integration prototype. This applies to, e.g., topological relations between different plant items of the chemical plant as well as to the linking of logically correlated information items and its different specifications. 5.3. Visualization of integration and consolidation results In order to demonstrate how the results of the integration and consolidation tasks are visualized, we have chosen the example of a data set describing electrical and instrumentation (E&I) devices. The results have been taken from the data of a midsize project, which comprises 521 devices. Before the results of the specific case are discussed, a general overview on the structure of the GUI is given. The implemented perspectives are divided into two main views: On the left, the selector-view allows for navigating the entire data imported in the prototype, sorted by the different types of engineering objects (e.g., equipment, E&I, piping). On the right, the main tables holding the engineering information are shown, separated by columns and distinguished by their respective data sources. In Fig. 6, a screenshot of the integration perspective of the CIB is given. The CIB provides several means to specify the project scope such that the user may only view and work on the relevant data. For instance, the project data displayed belong to project P471104712-00 (cf. input mask (1) as well as the message at the bottom right of screenshot). The selector-view comprises all E&I data which are contained in the most current sum-file (2). As an example, the selected project data comprises 468 instrument devices, a member of which is the control loop 8110.06 which in turn holds the mass flow sensor FT1 8110.06. The table view then directly states from which CAE tool (data source) the actual data were imported. The exemplary E&I data (3) were originally generated in two CAE tools, partly in Comos® Feed and partly in SmartPlant® P&ID (SPPID). The master-file, which contains in fact a combination of E&I items from each source, is shown in the ‘Master’ column (4). The collection of the items constituting the master-file, can simply be assembled by using the buttons ‘master column manipulation’ (5). The ‘Master’ column (4) holds all master data together with reference to the CAE tool the data was originally generated from. Furthermore, the GUI indicates consistencies and inconsistencies between the engineering data of the respective data sources by means of color and symbols to allow for a rapid screening and general support for the user. Whereas the color in the table view (6) refers to the consistency of the processing state (integration), the symbols used in the selector-view (2) refer to the consistency with respect to the consolidation. For instance, data highlighted in green are consistent in the sense that they have been specified in both sources as required (e.g., item 8110.01). Item 8110.12, on the contrary, is not yet consistent in terms of processing state – data from SPPID is missing – and hence it is highlighted in red. Regarding the symbols, the tick (2) in front of item 8110.03 means that there are no conflicting attributes and no conflicting mereological relations attached to this particular instance data item. By contrast, an X and the respective key (7) (e.g., FT1 8110.06) give some indication of the type of inconsistency. For the ease of inconsistency recognition – e.g., which kind of inconsistency problem occurs for the E&I-item – a simple “traffic-light” system has been introduced. Each icon in front of an object or its attributes has three “lights”. These can be either green (consistent) or red (inconsistent). These three lights are used to indicate (i) value inconsistency, (ii) unit of measure inconsistency or (iii) semantic inconsistency. Rudimentary information about the processed plant items within a project is visualized for the convenience of the project manager (8). Principally, it is indicated how many of the currently processed plant items are not yet fully specified or may contain attribute and mereological inconsistencies respectively. Eventually, the consolidated, possibly integrated data listed in the ‘Master’ column can be transferred back to the corresponding software tools (message “Create master-file” (9)), and thus the consolidated data can be directly reintegrated in the standard workflow. As an example for an intelligible visualization, we consider a scenario the results of which are shown in Fig. 7: In the course of the specification of a temperature sensor for a specific piece of equipment, the designer has made a mistake and has set the average temperature (tempUsual) to be higher than the minimum temperature (tempMin) of the temperature control sensor. Due to the vast amount of sensors specified in a typically project, this error was not detected directly. The undetected error would eventually lead to incorrect measurements in the worst case and hence would affect process control later on. However, for the final review of the design data, the designer uses the CIB, and so an inconsistency is displayed in the GUI as shown in Fig. 7. In the selector-view (left hand side of Fig. 7), the X marks control loops, (e.g., loop 4500.01), which has an inconsistent instrument (e.g., the temperature sensor TT4500.01). Fig. 7 also shows on the right how a semantic inconsistency is highlighted on the table view. By means of the CIB, the time necessary for carrying out the reconciliation task can be shortened significantly. Since the potential inconsistencies are presented concisely, the designer can quickly pick the marked items, interpret the inconsistencies reported, and eventually perform the occurring correction. In the particular case, a predefined rule has detected the inconsistencies, the data set has been labeled to be inconsistent, the properties tempUsual and tempMin are highlighted in red and the icon indicates a semantic inconsistency. 5.4. Computational effort Our experience with semantic technologies clearly reveals very high computational load when it comes to reasoning, particularly for large instance data sets. For the computation of the mid-size test data stemming from the use case provided by Evonik Degussa, a fairly powerful computer was used (Intel XEON 1.6 GHz 8 CPUs, 12 GB RAM). Unfortunately, it still turned out that CPU-memory has not been sufficient for a rapid reasoning (the data of which are shown in Fig. 6). For the reasoning of the data of the mid-size project, the installed OntoBroker uses a permanent storage implemented by means of a H2 database (H2DB, 2009), such that OntoBroker can store its data in database files inside the file system. For the mid-size project, about 22 GB of CPU memory are used at run time and about 50 GB of hard disk space of temporary inferencing data have to be saved locally in the database during the inferencing process. Overall, the run of 4 queries required for integration took about 8 h. However, applying a RAM-based storage configuration instead enables access to the temporary inference data faster and thus increases the system’s performance significantly. Hence, a more powerful computer (SGI Altix 4700 64 CPUs, 128 GB RAM) with capabilities to store the temporary inference data in the RAM has been used in a second test. The second machine obtained the results of the same 4 queries on the identical data set after about 2 min of computation time. 5.5. Lessons learned from the application example The applicability of semantic technologies to the integration and consolidation of real-life engineering design data has been demonstrated successfully. The application discussed in Section 5 has illustrated the advantages of ontology-based information modeling and of the reasoning capabilities of semantic technologies but also revealed its drawbacks. Still, we are convinced that the approach proves suitable for industrial use for the following reason. The application of semantic technologies as the core of the proposed CIB facilitates the generation of a coherent and consolidated data stock. For the time being, the integration and consolidation tasks mentioned in Section 5.2 are mainly performed manually in industrial practice. For instance, one designer typically spends several days on the consolidation of design data for electrical and instrumentation devices. Accordingly, there is no doubt that semantic technologies provide a unique benefit to information integration applications due to its powerful querying and reasoning capabilities. That way, highly skilled design engineers have to spend less time on manual integration and consolidation issues. Obviously, such functionality helps to further shorten the engineering process. This view is shared by our partners at Evonik Degussa. The downside of the use of semantic technologies certainly is the need for very powerful computer hardware to be able to reason within reasonable time. However, the reasoning often does not have to be performed in very short time. A feasible scenario for the integration or the consolidation task would be to perform the consistency check over night. Such time constraints allow for the usage of standard (powerful) computers. Besides, continuously decreasing prices of computer hardware will permit the application of higher computational power installed at moderate cost such that the encountered performance issues will possibly become irrelevant in the near future. 6. Related work The use of semantic technologies for the integration of distributed data is a long-standing research issue; an overview on the contributions introducing the fundamental concepts used in our work can be found elsewhere (e.g., Wache et al., 2001; Paton et al., 2000; Visser et al., 2000). More recent publications report on the application of such concepts mainly in the field of medicine and biology, e.g., Lister, Lord, Pocock, and Wipat (2009), Buitelaar, Cimiano, Frank, Hartung, and Racioppa (2008), Martin et al. (2008), or Kunapareddy, Mirhaji, Richards, and Casscells (2006). In all of these contributions, the originally proposed hybrid (or mediator-based) approach is extended in a similar way to the one proposed in this contribution. Originally, mediator-based approaches have viewed the purpose of a global ontology as simply a union of source ontologies, rather than as a semantically rich description of the application domain (e.g., Rousset & Reynaud, 2004; Wache et al., 2001). However, if a global ontology is defined merely as a model of a set of data sources, it becomes brittle with respect to the addition of new data sources and new formats. Thus, in most previous research attempts an intermediate ontology level has been introduced to address this issue. This intermediate level may be called differently, for instance ‘syntactic ontology’ (Lister et al., 2009) or ‘Simple Concept Organization System’ (Kunapareddy et al., 2006), but in each case, the underlying objective is to be more flexible in terms of adaptability and extendibility. Thus, we can claim that our approach complies with the state-of-the-art concepts published for semantic integration problems. Only very few contributions demonstrating the application of semantic technologies to engineering design problems have been published to date. Exemplarily, three of them are briefly sketched in the following, and the main differences to our work are pointed out. Peltomaa, Helaakoski, and Tuikkanen (2008) have developed an information system dealing with semantic information integration in the heavy industries. A typical production process is controlled by several information systems, each of them having different concept hierarchies, database structures, and information models. The project aims at the integration of three given information systems. The first contains failure and breakdown information, the second controls machines and devices, and the third incorporates the information from the previous two for further processing. The objective is (i) to integrate the data stemming from all information systems in order to offer a single view onto machine, device, breakdown, and failure information, and (ii) to enable proactive maintenance in the manufacturing process, including the planning of maintenance work and controlling the lead time of maintenance. The approach considers a so-called semantic layer for the specification of domain knowledge, which is done in a manner that is comparable to our approach, i.e., by means of global and local ontologies similar to our work. The implementation is also based on the ontology language F-Logic and the inference engine OntoBroker. However, contrary to our approach, Peltomaa et al. did not rely on an existing domain representation, which could have been customized to their own needs. Rather they used a ‘bottom-up’ approach by developing a proprietary, tool-specific ontology from scratch to fulfill the requirements of the information integration problem at hand. Adams, Dullea, Clark, Sripada, and Barrett (2000) address data integration of multiple data sources in order to establish a knowledge base for maintenance purposes in the aeronautical industry. The authors investigate the feasibility of applying a knowledge base to interface the information sources to the decision support system. The investigation relies on the commercially available knowledgebased development environment Cyc (Lenat, 1995), which provides an expressive knowledge-representation language (CycL), an efficient inference engine, and a large repository of common-sense knowledge. In analogy to this contribution, the existing domain knowledge is specified and extended to meet the requirements of the intended integration prototype. However, Adams et al. intend to develop a software tools that is tailored to the legacy systems encountered at their company and do not propose a general approach to integrate other data from other data bases or formats (e.g., XML). Finally, the MODALE project (Hefke, Szulman, & Trifu, 2005) aims at the development of an ontology-based reference model for semantic data integration for the manufacturing industries. The project is carried out in cooperation with various partners from the automotive industry. The ultimate goal is the integration of all data stemming from the various disciplines involved in the planning and operation of a complex manufacturing plant. The data required and generated by the multiple disciplines is captured by the reference model for the entire lifecycle. Thus, all required information can be extracted efficiently and consistently from the central reference model. The approach is based on the attempt to represent the content of the partners’ proprietary data sources in so-called partner ontologies, which are then integrated by a common reference model. The representation of the reference model is implemented in the RDF language (RDF, 2009). However, the approach failed in the sense that most of the partners preferred to work with their internal well-established tools and data formats instead of developing specific partner ontologies since the ontology management tools available at that time were considered to be too complex and too difficult to handle (Hefke et al., 2005). Generally, our work differs from those above in that we were able to build our approach on a field-tested ontology (i.e., OntoCAPE) with its extensive substrate of common-sense engineering design knowledge. This greatly facilitated the formation of a knowledge base for the given task. This advantage facilitates to focus on the development of a flexible concept for a prototype architecture which is suited for large-scale information integration and the processing of non-proprietary formats in engineering applications. With respect to chemical and pharmaceutical engineering, three other research groups have investigated the use of semantic technologies for information integration. Weiten and Wozny (2003, 2004) developed a system to archive the diverse results of research projects (publications, experimental data, mathematical models, etc.) in an integrated fashion. The system uses ontologies to annotate and link heterogeneous information sources. Similarly, the Process Data Warehouse (PDW, Brandt et al., 2008) uses ontologies to describe and interrelate the contents of chemical engineering documents and data stores. Within the PDW, ontologies are utilized for the annotation of electronic documents and data stores. That way, one obtains a consistent, integrated representation of the contents of these heterogeneous information sources. These content descriptions can then be processed and evaluated by semantic searching and browsing functions provided by the PDW, which support the navigation between and the retrieval of information resources. Both the PDW and the archival system developed by Weiten and Wozny allow to establish semantic relations between the information sources via content-specific metadata. However, both of these systems do not merge and reconcile the contents of the data sources as it is necessary for a consolidation of engineering data. Rather, they provide a cross-linked data repository, which allows for improved data retrieval. Also, both approaches only dealt with academic “toy” problems. The consolidation of large amounts of real-life distributed engineering design data were not tackled. Finally, Venkatasubramanian and co-workers have published several contributions on the topic of ontology-based informatics infrastructure in the field of pharmaceutical industries (e.g., Akkisetty, Reklaitis, & Venkatasubramanian, 2009; Hailemariam & Venkatasubramanian, 2010; Sesen, Suresh, Banares-Alcantara, & Venkatasubramanian, 2010; Venkatasubramanian et al., 2006). These contributions address different problems within the phase of product and process development regarding information integration. Similar to the approach proposed in this article, the authors have developed a framework which is based on the use of a domain (task) ontology (i.e., POPE and OntoReg respectively), which are implemented using OWL and partly SWRL. However, the focus of this work is on the extraction of pharmaceutical product information and mathematical model development knowledge, which is complementary to our approach. Unlike this work, Venkatasubramanian and co-workers have integrated their framework with the technical computing environment Matlab to use their knowledge for the generation of mathematical models. Finally, although the framework has been validated against industrial relevant examples, it has not yet been field-tested in a large industrial project. 7. Conclusions In this contribution, a novel approach for the integration and consolidation of distributed engineering design data has been presented and its implementation in the software prototype CIB has been described. The approach strives for two major goals: (i) a highly flexible and extensible structure in order to be generally applicable to heterogeneous and distributed data sets, and (ii) a concise but expressive knowledge base for resolving semantic heterogeneities in design data in order to shorten project durations and improve the data quality. The overall concept is based on a combination of XML and semantic technologies – the former is currently the technique of choice when in comes to data integration in industry, while the latter have only been applied in a rather explorative manner so far. The connection between the two technologies is established by defining an explicit representation of the XML data sources’ semantics by means of ontologies. By merging proven integration software and state-of-the-art technology, we combine the advantages of both solutions: Our approach and its realization within the CIB distinguishes itself by its generality, which allows for the integration of any design data expressed by XML due to its flexible, transparent, and extensible structure. The CIB is independent of any proprietary data format and thus widely applicable. This generality, however, manifests itself in a higher demand for computational power, compared to a tailored integration solution. The core of our approach is an expressive knowledge base, which is based on the formal ontology OntoCAPE. The generated knowledge base incorporates specific parts of OntoCAPE, which have been partly extended and customized for the task of integration and consolidation of design data. The predefined vocabulary within the applied ontologies serves as a stable conceptual interface to the engineering data. Also, the knowledge captured by the ontology is reusable and extensible (customizable) to other engineering data. This allows for the flexibility to adapt to CAE tools that were not envisioned at the time of generation of the ontology. Semantic technologies have the ability of advanced reasoning which is for instance enabled by means of rules and expressive queries. The reasoning itself is performed by the commercial inference engine OntoBroker, which is scalable and thus applicable to large data sets like they are arising in real industrial applications. For a first benchmark, the reliability of the CIB and its ability to handle large-scale industrial data sets have been evaluated by means of several integration problems provided by Evonik Degussa. The expressiveness of the knowledge base and the applicability of the integration prototype have been successfully tested in different scenarios with diverting industrial data stocks. The resolving of semantic heterogeneities includes the detection and visualization of design errors. In industry, this work still needs to be performed manually, at considerably effort and cost. With the rapid screening of design inconsistencies, as proposed here, the reconciliation task can be accelerated, and the data quality can be improved significantly. Overall, the concept performs the integration and consolidation of data from different CAE tools without the need of standardization. Thus, the CIB constitutes a highly flexible and very expressive integration platform, which can simply be linked to different XML in-house technologies and is thus reusable across companies and organizations. The existence of an integrated and consolidated data stock within the CIB builds a sound foundation for further, more sophisticated analysis, which might help to further improve the overall engineering design process. For example, our approach can be extended to address the management of revisions within the engineering design process. Instead of detecting unintended design errors, as it is done for the time being, we may use the same mechanisms to discover all inconsistencies that result from intentionally imposed deviations, typically occurring from revisions within the course of a design process. By rating potential inconsistencies with some economic quantity, an evaluation of alternative revisions become possible, allowing for the determination of the one change with the lowest impact on project costs. While this is still subject to current research, it further highlights the great potential of the application of semantic technologies in the engineering design process. Acknowledgements Most of the research described in this article has been performed in the context of CRC 476 IMPROVE8 and TC 61 (), funded by the German Research Foundation (DFG). The authors would like to thank their co-workers, Michael Wiedau, Alexander Sempf, Christian Bruns, Kalle Fischer, Sebastian Mauer, Sebastian Tobies and Stefan Arts, who helped to develop and implement major parts of the prototype. Also, the contribution of the industrial partners Evonik Degussa and Ontoprise is greatly acknowledged, with special thanks to Heiner Temmen, Hannes Richert and Felix Anhäuser (all working for Evonik Degussa) and Moritz Weiten, Mike Ulrich, and Joachim Redmer (all working for Ontoprise). 
Information integration in chemical process engineering based on semantic technologies. During the design phase of a chemical plant, information is created by various software tools and stored in heterogeneous formats, such as technical documents, CAE databases, or simulation files. Eventually, these scattered information items need to be merged and consolidated. However, there is no efficient computer support for this task available today. While existing technologies like XML are capable of handling the structural and syntactic differences between the heterogeneous formats, these technologies cannot resolve any semantic incompatibilities. For this reason, information integration is still largely performed manually – a task which is both tedious and error-prone. Semantic technologies based on ontologies have been identified as an appropriate means to establish semantic interoperability. This contribution presents an ontology-based approach for information integration in chemical process engineering. The underlying knowledge base, which is based on the formal ontology OntoCAPE, is presented, and the design and implementation of a prototypical integration software are described. Further, the application of the software prototype in a large industrial use case is reported. 1. Introduction The design process for a chemical plant is characterized by a sequence of design phases and involves different departments, disciplines, and contractors. In the different design phases, various software tools are used by the project team to generate typical engineering documents, like equipment lists and piping and instrumentation diagrams (P&ID), which capture the major engineering design information (cf. Fig. 1). Some of these software tools are of a domain-specific nature (e.g., process simulators, CAD or CAE systems, etc.); others are of generic type and independent of the requirements of engineering design processes (e.g., word processors, project management systems, etc.). Typically, the contents of the documents and data files resulting from the activities in the various design phases are heavily intertwined and overlapping, due to the strong dependencies between the information generated. Generally, the information models of domain-specific software tools suffer from a lack of a well-structured, standardized information representation (Bayer, 2003; Bayer & Marquardt, 2004). Thus, the exchange of data between the different tools is often hindered by the inherent heterogeneities of the underlying data sources (Embury et al., 2001). Regarding the engineering design phase, the heterogeneity of the data models of the applied CAE tools (cf. Fig. 1) is reflected in the diversity of the corresponding native export schemas of the respective tools. Typically, each tool has its proprietary data export schema, which is hardly related or linked to the export schema of any other tool. Also, a data export schema can produce different data files, which do not necessarily have identical structures in terms of scope or content: For example, a document may contain different engineering objects – e.g., apparatuses and machines – the characteristics of which are described by different attributes, basically representing the actual properties of the objects. Finally, one and the same CAE tool can be used for different tasks during the design phase and thus may export data to create different engineering documents in different versions – for example, a tool can be used to create P&IDs as well as technical specific sheets for apparatuses and machines. To date, no adequate solution is available in industrial practice to cope with all of these heterogeneities. Past approaches, mainly driven by the software vendors, addressed different slices of this overall problem: Data and information management issues were addressed separately, leading to stand-alone systems with limited capabilities and significant integration barriers. Accordingly, most of these tools are mainly applied in a stand-alone manner since a considerable implementation effort would be required to achieve interoperability with other dedicated tools. Consequently, the relations and dependencies existing between the different documents and data stores are often not captured explicitly and hence cannot be checked and managed easily. Thus, integration and management of this scattered data provides a real, yet unsolved challenge for the users and vendors (Marquardt & Nagl, 2004). As a result, the lack of appropriate tool interoperability and data integration are major cost drivers in the design phase. In this respect, the National Institute of Standards and Technology (NIST) in the U.S. recently reported that the lack of interoperability costs the U.S. capital facilities industry 15.8 billion dollars per year, compared to a hypothetical scenario where the exchange of data and the access to information are not restricted by technical or organizational boundaries (Gallaher, O’Connor, Dettbarn, & Gilday, 2004). Hence, there is an enormous potential for productivity gains, and consequently for cost reduction and quality improvement. Besides the general interoperability problem, engineering departments are also challenged by a more specific, self-imposed problem: The necessity to reduce time to market, forced by increasing global competition, calls for complex work processes like concurrent and distributed engineering. Accordingly, the different parties involved in a design project work in parallel and possibly even operate at geographically distributed sites. In particular, concurrent engineering (Kusiak, 1993) is a frequently applied concept to reduce project duration. However, the advantage of all these procedures, i.e., simultaneous work on related engineering issues, is at the same time its biggest disadvantage: In a fully sequential design process, team members in the downstream phases (i.e., the later phases of the design process such as detail engineering) work on finalized data sets that were created upstream (i.e., in the earlier design phases such as basic engineering). By contrast, in an overlapping, concurrent work process, the team members working downstream have to rely on preliminary information. Since the design processes in chemical engineering typically are of a creative and evolutionary nature, this preliminary information may frequently change substantially. Consequently, team members working downstream must readjust their work if colleagues from upstream change the design requirements or specifications in an unexpected way. Under these conditions, upstream engineering changes may cause significant downstream rework, potentially delaying the whole project (Eastman, 1980). For the efficient handling of such complex design processes, effective computer support is highly desirable to enable the integration and consolidation of the distributed design information. Such computer support has to fulfill two major requirements: (i) it has to provide a single point of access to the miscellaneous, heterogeneous data sources such that they appear to the user as a single, homogeneous data set, and (ii) it must determine the relations between the contents of these data sources in order to detect and reconcile possible inconsistencies in the distributed information. These relations can be both temporal and logical. Temporal relations result from versioning, where information item X is replaced by Y. Logical relations exist, if there is any conditional dependency between information items A and B. To our understanding, there is a clear mismatch between the urgency and significance of the concrete problem in industrial practice and the published solutions actually available and applicable in the chemical engineering domain. Due to the complex nature of the problem, it requires not only a thorough understanding of state-of-the-art concepts for information integration is required, but also a fundamental background on process engineering in order to grasp the problem in sufficient scope and detail. Solution concepts published by the computer science community tend to be highly generic and context free, and thus in fact non-applicable to a concrete chemical engineering problem. In the process engineering community, on the other hand, those few groups and organizations dedicated to this issue have published relatively little of their work due to the secrecy under which such (commercially valuable) systems tend to be developed. In order to close this gap, a research project was initiated as part of the collaborative research center CRC 476 (Nagl & Marquardt, 2008) at RWTH Aachen University, with the goal of tackling the aforementioned problem in close cooperation with partners from the chemical as well as the software industry (Evonik Degussa and Ontoprise). This paper summarizes the results of the aforementioned project. We present a prototypical ontology-based software tool for the integration and consolidation of distributed design data in chemical process engineering. Particular emphasis is put on the conceptual design and the implementation of the semantic information model of the tool. Furthermore, its applicability in an industrial use case is illustrated. Due to the close collaboration with our partners, all the developed concepts could be tested immediately in an industrially relevant setting to allow for a reliable proof of concept. The remainder of this paper is structured as follows: Section 2 introduces the necessary technical background and discusses the deficiencies of existing solutions. Section 3 summarizes the conceptual design of our approach and points to the important aspects. In Section 4, the implementation strategy for the integration prototype is introduced. In Section 5, the application of the integration prototype in an industrial use case is shown, including a discussion of its particular requirements, implementation details, and results. Section 6 gives an overview on related work available in literature. Finally, Section 7 summarizes the achievements and gives an outlook on future work. 2. Existing solutions and their deficiencies In order to enable efficient computer support of engineering design processes, essentially two aspects have to be taken care of: firstly the structural heterogeneity of data formats and stores, and secondly information integration.1 In the distributed database systems community, the problems arising from the heterogeneity of the data are classified as (i) structural heterogeneity (a.k.a. syntactic or schematic heterogeneity; e.g., Kashyap & Sheth, 1996; Kim & Seo, 1991) and (ii) semantic heterogeneity (Kim & Seo, 1991). While structural heterogeneity essentially means that different information systems store their data in different structures, semantic heterogeneity relates to the contents of information items and its intended meaning (Wache et al., 2001): A good example of semantic heterogeneity is the use of synonyms, where different terms are used to refer to the same concept. A comprehensive overview on the diverse types of semantic heterogeneities including their classification is given by Visser, Jones, Bench-Capon, and Shave (1998). Most of the approaches that have been published to date only address the problem of structural heterogeneity. Within recent years, several new technologies have become available for handling syntactic and structural heterogeneities. Particularly XML has gained wide acceptance as a way of providing a common syntax for exchanging heterogeneous data. However, the reconciliation of the semantic heterogeneities of the data models – i.e., the different meanings of the data within the context of the particular software data model – is still an unsolved problem (Cui, Jones, & O’Brien, 2001; Schneider & Marquardt, 2002; Tolk & Diallo, 2005). In particular, the identification of semantically identical data – i.e., data items with the same meaning – that are stored in different files or software tools is not possible so far. The ability of computer systems to communicate information and have that information properly interpreted by the receiving system in the same sense as intended by the transmitting system is also referred to as semantic interoperability. Particularly, the lack of semantic interoperability causes a significant overhead for the designers as they have to spend considerable time on the manual consolidation and integration of design data. 2.1. Traditional approaches to semantic interoperability There are three traditional approaches to achieve semantic interoperability: (i) brute-force, (ii) a global data standard, and (iii) interchange standardization (Gannon et al., 2005). While the bruteforce approach requires an enormous coding effort (for each pair of systems, an individual translation software must be created), the other two call for a common standard, with the effect that all the semantic differences disappear (at least from a data-exchange perspective) and there remains no need for translation. The global standard approach requires that all stakeholders (i.e., all application users and software vendors in a certain application domain) agree on a common format for the representation, processing, and storage of their data. The interchange standardization approach is less ambitious since the stakeholders only need to agree on a standard format for data exchange – the systems to be integrated may use their internal formats for data storage as long as each data sender generates the data according to the standard. Most of the industrially applied integration solutions are either using the brute-force or the interchange standardization approach. Commercial software systems for the integration and management of process design data typically employ a combination of both. Most of these systems are based on a central data warehouse (Jarke, Lenzerini, Vassiliou, & Vassiliadis, 2003), which can import and redistribute data created by application tools. These systems support the navigation and retrieval of data, and they provide some basic data management functions such as version and change management. A typical example is SmartPlant® Foundation (Intergraph, 2009). However, such off-the-shelf solutions can only process the proprietary data formats propagated by the respective vendors (nowadays often XML). Their applicability is thus limited to the data created by the vendors’ application tools, which normally represent only a minor subset of all data created in the course of a design project. Examples of such proprietary XML dialects are XMpLant (Noumonon, 2006) or cfiXML (EPlant, 2004). Extending these tools towards the handling of non-proprietary formats means that appropriate parsers and converters have to be implemented. More specifically, the engineering design data has to be mapped onto the internal data model of the integration tool. Since, in most cases, both the XML exchange schemas and the internal data models are poorly documented, this is not an easy task to accomplish. Numerous attempts towards an interchange standard have been made over the past years, not only in chemical engineering, but also in other engineering domains including STEP (Fowler, 1995), ISO 15926 (ISO 15926, 1999) or more recently CAEX (IEC 62424, 2006): STEP, also known as ISO 10303, covers information for the computer-interpretable representation and exchange of product manufacturing information. It consists of various parts, which can be used by different disciplines in many industries, such as the automotive, aerospace, and chemical industries. In spite of its detailed elaboration, it has never been fully accepted in the field of chemical engineering. Note that parts of STEP were originally implemented in the EXPRESS language, but have recently been translated into XML to meet the state-of-the-art implementation requirements. In the field of chemical engineering, one particular part of STEP has been further elaborated towards the ISO 15926, which has recently gained certain popularity. It is intended to incorporate all technical data in the life cycle of a chemical plant. Thus, it is supposed to facilitate the information exchange between all stakeholders starting from design to construction to operation. However, except in the oil and gas industry, the ISO 15926 has not found broad acceptance, due to the extreme complexity of the data model on the one hand (Bayer, 2003; Löffelmann, Zgorzelski, & Ahrens, 2005) and its rather narrow scope on the other hand (Bayer & Marquardt, 2003; Schneider & Marquardt, 2002). CAEX is a generic metamodel for the structuring of chemical engineering design data, which has been implemented in XML. It allows for the definition of specific modules, such as one for the description of a level control structure for a vessel, which can be reused and parameterized to a given task. An entire plant can then be composed of linked and parameterized modules. CAEX, however, does not force the modeler to use consistent elements, which may result in inconsistent representations of equivalent objects. The implementation of a global data standard is infeasible in engineering practice due to various reasons. The most important reason is the fact that in an interdisciplinary and international field, such as chemical process engineering, different standards are coexisting for the various disciplines, which makes it virtually impossible to agree on a single one which is obligatory. Principally, the idea of standardization has helped a lot to improve communication and syntax standards in various domains. Such standards incorporate the choice of language syntax, such as XML, or the choice of application layer protocols, such as http. However, standards regarding the information representation are much more difficult to agree on. Various ideas and approaches regarding information modeling have been proposed by academia within the last decades, including the work of Angus and Winter (1985),  ̃ Motard, Blaha, Book, and Fielding (1995), Banares-Alcántara (1995), Westerberg, Subrahmanian, Reich, and Konda (1997), Batres, Naka, and Lu (1999), Bayer and Marquardt (2003), and many more. A comprehensive overview has been provided by Bayer (2003). However, none of these ideas have found their way to industrial practice. Also, despite the tremendous efforts spent by the various initiatives (in particular STEP), a universal standard data models have not yet gained wide acceptance, neither in the chemical industry nor by the software vendors. Obvious reasons could be the incomplete coverage, which is always limited to parts of the life cycle, as well as the lack of coherence and consistency between the suggested standards stemming from (at least initially) independent initiatives. Experience has shown that it is hardly possible to agree on a life-cycle data model – not for technical, but rather for organizational reasons. Even agreement on the skeleton for a life-cycle data model on a coarse-grained scale can be difficult to achieve due to the divergent objectives of the various stakeholders (Schneider & Marquardt, 2002). Considering the diversity of the existing standards relevant to the different facets of process engineering, there is definitely a need for a framework into which they all could fit. However, since many of these standards have been developed from scratch, are based on different paradigms, and incorporate different legal (national) constraints, it seems unrealistic to find a framework that applies to all of them. But even if such a hypothetical framework had been developed, we would still face the problem that comprehensive standards, such as those briefly introduced previously, are not understood and used in a uniform manner. In fact, one major problem of existing data standards is the existence of different flavors: A flavor of a standard is evidenced when two different stakeholders interpret the same standard in two different ways and consequently find two ways for encoding the same piece of information. In other words: While an interchange standard can define an agreed set of labels for the exchange of design data, it does not solve the problem of semantic heterogeneity due to two reasons (Cui et al., 2001): First of all, it is very likely that more than one standard is applied in the future, and it cannot be assumed that these standards and their flavors will all be based on a consistent semantics. Secondly, it does not ensure a consistent use of a single terminology for the labeling the data contained in different files. Consequently, the problem of semantic heterogeneity would still exist if all data were exchanged using XML structures that are in accordance with standard schema-level specifications. Summarizing this section, we come to the conclusion that each of the three traditional approaches towards information integration has certain drawbacks that make them inappropriate, particularly if a large number of data sources are involved. In short, brute-force requires a large number of hand-coded converters that are costly to implement and difficult to maintain; a global standard is at least costly and may even turn out to be impossible to develop and to maintain. Also there are technological difficulties, and there is organizational resistance against a single uniform global data standard. Therefore, there will always be several competing and coexisting standards. Although interchange standardization certainly has advantages, such as local autonomy and the smaller number of converters required, it also has serious limitations: Comparable to a global standard, all stakeholders need to have a clear, common understanding about the domain in order to reach an agreement on the content. Furthermore, the exchange standard’s requirements will change over time. Continuous adjustments will obviously be necessary, which will affect and need to be harmonized between all systems. Finally, it still requires a large number of hand-written converters. In addition, all three approaches lack flexibility to adapt to changes because the data semantics are either hard-coded in the converters (i, iii) or in the standard (ii, iii). A suitable concept needs to overcome these shortcomings. As a consequence, the question whether or not the usage of an interchange standard is more efficient than working with the proprietary files is not relevant. There is no alternative to the usage of proprietary files or some in-house interchange standards, as the comprehensive application of universally applicable standards is simply not possible due to their absence. Accordingly, we are convinced that proprietary standards, partly accepted interchange standards, and in-house standards are the only available options in the near future. Compared to a ‘universal’ format, they have the advantage that they are de facto fast to agree on (although they often leave out debatable elements). However, they are flexible in terms of expendability and adaptability, and they are to some extent reusable. Thus, any solution to the problem of interoperability has to emanate from this situation. Finally, we can say that no accepted (global data or interchange) standard has been established to date, such that the companies are forced to work with various proprietary formats. As a consequence, many chemical and engineering companies are utilizing in-house solutions for data exchange, often based on XML schema specifications, such as PlantXML (Anhäuser, Richert, & Temmen, 2004). Such approaches can be seen as locally valid interchange standards. While these approaches are capable of solving the more basic problems of data exchange, they are less suitable for handling complex integration issues. 2.2. Ontology-based approaches to semantic interoperability Any appropriate solution of the problem of semantic heterogeneity has to formally specify the meaning of the terminology used by each system. Such a formal specification is a prerequisite for the computer to autonomously infer a translation between the different system terminologies. Unfortunately, the semantics assumed by a particular system are rarely documented – that is, there is no explicit representation of the semantics used by a particular data source (in the sense in that a data schema provides an explicit representation of the underlying data structure). Hence, to overcome this problem, the structured information stored in the XML document has to be connected to expressive domain knowledge, which relates meaning to the stored information within the context. To that end, ontologies have gained popularity as a convenient means for the representation of domain knowledge. Ontologies essentially constitute a structured framework for the storage of information and knowledge. More formally, the definition of Uschold and Gruninger (1996) states that “an ontology is an explicit specification of a conceptualization, typically involving classes, their relations and axioms for clarifying the intended semantics”. A well-structured ontology consists of reusable parts for different applications and captures consensual knowledge of a particular application domain. Accordingly, it can be used as the generic core of a knowledge base (Uschold & Gruninger, 1996). For an ontology-based solution to semantic interoperability, as it is proposed in this contribution, essentially two basic software components are required (Neches et al., 1991): a knowledge base, which contains generic domain knowledge (i.e., the ontology) as well as concrete facts about the case under consideration, and an inference engine (also known as reasoner), which processes the knowledge and facts stored in the knowledge base and autonomously inferences a solution for the case at hand. The term ‘semantic technologies’, as it is used in this contribution, refers to software systems that use ontologies as internal data models. A brief overview on related work and some exemplary ontology-based applications is provided in Section 6. 3. Information integration based on semantic technologies Our objectives have been (a) to develop a sound concept for integrating and consolidating semantically heterogeneous data across the different phases of a design process, and (b) to implement this concept in a software prototype. The software prototype should provide a convenient way of resolving semantic heterogeneities (requirement #1); moreover, it should be easily adaptable to different proprietary data formats (requirement #2). The prototype has been named Comprehensive Information Base, and we will refer to it as CIB in the following. The overall architecture of the CIB (conceptually as well as in the concrete implementation) consists of three basic modules (cf. Fig. 2): (i) a knowledge base, which holds the domain knowledge required for the integration and consolidation task; (ii) a peripheral software environment (PSE), which administrates and visualizes the integrated and consolidated data and provides functionality such as the import and export of XML data; and (iii) a powerful inference engine, which evaluates the knowledge modeled in the respective ontologies in order to reason about the facts. Regarding our requirements, the reconciliation of semantic heterogeneities (requirement #1) is performed by the semantic technologies (knowledge base, inference engine). As for the easy adaptability (requirement #2), the concept assumes that all data to be integrated is provided in the XML format – as long as this assumption is true, the CIB is very flexible in so far as it allows for the import of any XML dialect. In the following, we will primarily focus on the conceptual design of the knowledge base, putting special emphasis on the design of the underlying ontology; the characteristics of the PSE will only be sketched briefly. Overall, the development of the knowledge base was guided by two crucial aspects, which will be discussed in the following: the design of a flexible and intelligible structure, and the formulation of a concise but expressive knowledge base. 3.1. Design of the CIB as a mediation layer The goal of the design is to provide a flexible, open, and integrative framework for information integration, which meets the requirements of industrial practice. Therefore, it has to be able to process standard formats and it has to provide means to be easily adopted by engineering departments of chemical companies and EPC-contractors.2 Accordingly, the CIB basically represents a mediation layer, which is placed between the user and the data sources (Wiesner, Morbach, & Marquardt, 2008; cf. Fig. 2). It is organized by means of the so-called hybrid ontology approach (e.g., Lenzerini, 2002; Paton, Goble, & Bechhofer, 2000; Wache et al., 2001), which enables flexible manageability and easy expandability. The main idea is to use a shared vocabulary, which contains the basic terms of a domain and can be applied to all data sources in order to describe their respective semantics. That way, the concept is flexible in the sense that the vocabulary is generally applicable – i.e., independent of the syntax and structure of the data sources – and it does not need to be modified when a new data source is to be added to the CIB. In terms of easy expandability, any new data source can conveniently reuse the shared vocabulary. To that end, all mappings between the shared vocabulary and the previously integrated data sources have been conserved and thus can be reused for the alignment of the new data source. The hybrid ontology architecture consists of a so-called global ontology – i.e., the shared vocabulary – and several source ontologies, which are linked by means of mappings (Lenzerini, 2002) as shown in Fig. 3. Each data source (i.e., each XML document generated by some application tool) is described by a local source ontology, which basically replicates the document (its original structure as well as its data) in the ontology language of the CIB. In this context, the term ‘XML document’ denotes a set of XML files that (i) originate from the same source and that (ii) conform to the same XML schema. Such XML files are considered as different versions of the same XML document. The global ontology functions as a predefined vocabulary, by means of which a representation of the contents of the XML documents can be given. This neutral representation is independent of any native XML schema specific application tool. Thus, the global ontology provides the necessary vocabulary for a unified description of the data in the source ontologies. The entities of the source ontologies are linked to the entities of the global ontology via mappings, thus making the semantics of the source documents explicit. Thus, meaning is assigned to the data stemming from the different tools. The interactions between a source ontology and its corresponding XML document are handled by a bidirectional converter, which mediates between the XML representation and the ontology language of the CIB (Wiesner, Morbach, et al., 2008; cf. Fig. 3). There are different options for the realization of such a converter: The most common method directly translates the XML data into the representation of a “knowledge-carrying” ontology (which would correspond to our global ontology). However, our experiences so far show that the specification of generally applicable conversion rules for the realization of suchlike converters is a fairly complex task; moreover, such conversion rules are difficult to maintain due to the lack of transparency. Hence, we decided against such a type of converter. Instead, we built a converter that performs a pure syntax conversion: The data of the respective XML documents are represented as instances of intermediate schemas inside the knowledge base: The intermediate schemas comprise identical data structures and labels as the schemas of the XML document, but they are formalized in the respective formal ontology language. In the following, we will call these intermediate schemas ‘import ontologies’ (cf. Fig. 4). The extraction of the relevant data is conducted by mappings in the knowledge base. The user may interact with the CIB via the global ontology, assisted by a suitable user interface hiding all the complexity of the ontology (cf. Fig. 2). The global ontology serves as a global query schema3 : First, the user formulates a query in the terminology of the global ontology.4 Next, the inference engine transfers the query to the source ontologies via the existing mappings. That way, the inference engine retrieves and combines matching information from the different source ontologies. The results are then retranslated into the vocabulary of the global ontology and finally presented to the user via the user interface. The main advantage of this architecture is its ability to distribute the complexity of the integration and consolidation tasks to different steps, each of which are represented by distinct, generally applicable, and reusable modules within the global ontology. For the problem at hand, the global ontology is decomposed into different modules, which fulfill either integration or reconciliation tasks (cf. Section 3.3). For instance, data integration can be performed independently of data reconciliation if required. The use of these different modules for the diverse requirements of the CIB renders the approach highly flexible. 3.2. Global ontology requirements The above-described approach requires an appropriate (global) ontology that can (i) relate meaning to design data within the domain of chemical process engineering, and that can (ii) fulfill the task of information integration. Accordingly, a domain-task ontology had to be developed. Generally, a domain-task ontology5 is defined as an application-independent task ontology that is reusable in a given domain, but not across domains (Gomez-Perez, FernandezLopez, & Corcho, 2004). Hence, our global ontology corresponds to a domain-task ontology that has been created by extending an existing domain ontology and combining it with a given task ontology. The domain ontology OntoCAPE (Marquardt, Morbach, Wiesner, & Yang, 2010; Morbach et al., 2007) has been developed for applications in the domain of Computer-Aided Process Engineering and is thus particularly applicable to the integration of design data in chemical engineering. It is designed to be usable in different application contexts, and it is independent of a specific implementation (Morbach, Wiesner, & Marquardt, 2009). Thus, it could be easily reused for the purpose of our global ontology and adapted to the task at hand. To enable the task of information integration, specific OntoCAPE modules have been extended and customized in terms of adding relations, attributes, and axioms to enable, for example, the identification of design inconsistencies. Besides, a specific task-ontology, which is based on the ideas of the document model (Morbach, Hai, Bayer, & Marquardt, 2008), was added to the domain knowledge. Essentially, it is used to describe the formal structure and version history of the XML documents. By combining the document model and OntoCAPE, the contents of the engineering documents and their mutual dependencies can be modeled. Eventually, a global ontology has been derived from the domain ontology OntoCAPE, which incorporates also parts of the task ontology ‘document model’. Overall, the global ontology in its entirety is capable of fulfilling the following tasks. Firstly, redundant information has to be identified and rejected. In the first place, this concerns information objects that occur in multiple XML documents. These objects must be detected, and their attributes have to be reconciled within the consolidated data set. If the attributes of such an object are distributed across different XML documents, they have to be assembled, and outdated attributes have to be replaced by the current ones. Secondly, implicit dependencies existing between the different information objects have to be represented as explicit ones: Examples are topological relations between different plant items or the temporal and logical interrelations between diverse versions of these objects. 3.3. A knowledge base for information integration A concise and expressive knowledge base for information integration has to contain a global ontology as well as source ontologies as stated in Section 3.1. Fig. 4 represents all the sub-ontologies that constitute the knowledge base; these sub-ontologies belong either to one of the source ontologies or to the global ontology. Each source ontology essentially comprises two sub-ontologies – called document ontology and import ontology – which are specific to the XML schema of an application tool. As for the generic global ontology, three distinct parts can be distinguished, which were partly derived and customized from suitable OntoCAPE parts (namely the integration ontology and the consolidation ontology) and partly derived from the document model (namely the versioning ontology). Although all three parts of the global ontology are modeled as discrete modules, they all contribute to the overall shared vocabulary used for the task of information integration. In the following, the different ontologies will be described in detail, placing special emphasis on their contents and their specific roles within the knowledge base. 3.3.1. Import ontology As explained in Section 3.1, the source ontologies describe the contents of the XML documents in the ontology language of the CIB. The import ontologies simply replicate the data structures and labels of the XML documents in the ontology language (note that the information contained in the XML documents is only hierarchically structured, but does not capture any other dependencies). Every XML document belonging to a certain native XML schema is represented in a self-contained import ontology. Thus, for each XML schema, there is a single import ontology, which collects all the data that is formatted according to that particular XML schema. For example, all data stemming from the particular XML schema used by the CAE tool Comos® Feed (Comos, 2009) is gathered in an import ontology called ‘Comos Feed Schema’. 3.3.2. Document ontology In most cases, the data represented in the import ontologies cannot immediately be used by the global ontology. Typically, there are significant differences between the structures and contents of the diverse data sources (and thus of the import ontologies), which makes a direct mapping to the global ontology difficult. Therefore, an intermediate step is introduced, in which the so-called ‘document ontology’ is generated. This step basically comprises the following tasks: Not all data gathered in the import ontology are relevant to the task of information integration. The document ontology retains only those data which have been identified to be of special importance, in the sense of “technical master data”. Thus, by means of the document ontology, the complexity and quantity of the import data is significantly reduced. Further relations and attributes are added to account for import context information, which goes beyond the purely structural information. Typically, such information comprises relations crosslinking between information objects. A typical example would be a measurement device that belongs to a specific control loop: In XML, these links are typically realized only indirectly by means of internal identifiers. In the ontology, by contrast, a ‘measurement device’ and the ‘control loop’ would be represented as classes which are linked by the relation ‘has measuring device’. Clearly, the generation of such a document ontology requires expertise about the structures and contents of the respective engineering documents at hand. Thus, the required knowledge, in terms of classes, relations and axioms, can only be created by an expert. Since XML documents are typically bound to specific CAE tools, which in turn obey proprietary XML export schemas, such document ontologies and their linkage to the import ontology have to be built (only once) for each native XML export schema. Once the knowledge has been modeled, it applies to all successive instances stemming from XML documents obeying the same schema. The data contained in the document ontology constitute the basis for executing the semantic integration and consolidation. 3.3.3. Versioning ontology The versioning ontology is part of the global ontology and holds the basic concepts necessary for the versioning of engineering objects. Therefore, a uniform description of the structural composition of documents is required, such as the one proposed by Morbach, Hai, et al. (2008). The versioning, as it is applied here, assumes engineering documents, the contents of which can be represented by delimitable entities with definable properties and dependencies. For example, an engineering document may be composed of different (engineering) objects, which represent uniquely identifiable processing units or ‘plant items’. The characteristics of these objects may evolve within the course of a design project. Accordingly, different versions of each object may exist in different documents. Consider the example of a distillation column, which is given the unique identifier K-4001: In the course of the design project, this column is specified in various documents, e.g., in a process data sheet or a technical specification document. In order to describe the history of these objects their different versions and configurations have to be captured to follow their progress over time. To this end, specific attributes have to be assigned to all engineering objects of a document version, such as the creation time, the originating software tool, and the associated project. On the basis of this information, versioning can be performed. Versioning aims to satisfy two needs: Firstly, the most current entities captured by the respective document versions can be identified and possibly assembled in a single document for further processing. Secondly, the tracking of changes between different versions of a particular engineering object becomes possible. In particular, intentionally discarded or accidentally forgotten plant items can be determined. 3.3.4. Integration ontology The integration ontology holds knowledge about the constitution and description of complex technical systems. Such knowledge is captured by the OntoCAPE module ‘technical system’ (Morbach, Bayer, Wiesner, Yang, & Marquardt, 2008), which is applied and extended for the integration task. In doing so, it is not relevant what kind of function such a technical system fulfills – e.g., if it represents a reactor or an instrument device. For the integration, it is only important that each plant item can be clearly identified as an instance of a particular technical system (e.g., according to a unique identifier) and that all its subsystems (e.g., fixtures or nozzles) can be derived unambiguously from the application context. For example, the distillation column K-4001, its column internals KE-4001, and the diverse nozzles N1, . . . N20 can be identified from the respective document and related to each other. Note that, even though integration by means of unique identifiers may not seem to be a universally applicable option, it is in fact a far-reaching solution since such identifiers are widely applied in chemical and engineering companies for an unambiguous identification of plant items and their characteristics. The goal of the integration task is to merge the information items from disparate sources. By merging, we mean to gather and assemble all available information about a single engineering object (e.g., an apparatus or a pipeline), which may be scattered across different source ontologies. Different files of the respective CAE tools may represent different characteristics of identical real world objects. As a result of the integration task, one document version for each engineering discipline is produced, which includes all the relevant technical systems, subsystems (e.g., fixtures and nozzles), and properties (e.g., technically relevant attributes). In doing so, the individual information items from different documents and thus different CAE tools may be merged and summarized. Furthermore, an inventory of the plant items already specified in the CAE tools is obtained as a side effect. 3.3.5. Consolidation ontology The consolidation ontology eventually holds process engineering domain knowledge, which is provided by OntoCAPE and which is customized according to the prevailing requirements. Since the data captured by the engineering documents correspond either to a functional or a constitutional description of a chemical plant, this part of the global ontology holds knowledge about the function of the processing steps and their realization by means of plant items and their connectivity. Particularly, the mereological and topological concepts for the description of plant items, provided by the OntoCAPE modules ‘process’ and ‘plant’ (Wiesner, Morbach, Bayer, Yang, & Marquardt, 2008), have been reused. The concepts applied in the consolidation ontology basically represent a specialization of the concepts represented in the integration ontology, since specific features regarding the composition of plant items and their interconnections have to be considered. For example, equipments and pipes follow different rules with regard to connectivity: a piece of equipment may have connections to various pipes and instruments, whereas as a pipe typically connects exactly two pieces of equipment. An important principle, which is for instance applied for checking the consistency of interconnections, is the transitivity of properties. That way, e.g., the pressure levels within an integrated processing system or the kind of insulation of neighboring systems can be checked. As an example, a vessel has several nozzles by which it can be connected to other plant items (e.g., to instrument devices or pipes). In order to check if such a connection has been properly specified, the properties of the interfaces need to be assessed: e.g., if nozzle N1 has a nominal diameter of 50 mm and if N1 is connected to pipe R10, which also has a nominal diameter of 50 mm, the diameters of nozzle and pipe match. 3.4. Advantages of the proposed approach The proposed approach compensates for the most severe drawbacks of all of the established approaches presented in Section 2.1. In contrast to the brute-force approach, we only need to translate between the data sources and the global ontology: Thus, we need to create only one data converter for each data source (instead of one converter for each pair of data sources). Also, the semantic heterogeneities do not have to be resolved individually by the converter, but by a knowledge-carrying component of the CIB. Compared to the global data standard, the proposed hybrid method has the ability to lower or even eliminate the necessity of reaching a global agreement on the entire domain terminology. The global ontology is different from a data standard in that it contains rather universal, generally accepted concepts (e.g., technical system, instrument, and pipe). These general concepts show up in very similar form in any of the proprietary data models and typically do not need special consideration. According to experiences, differences in the conceptualization and representation of the domain knowledge only appear when it comes to details. The specific differences in the representation of the domain knowledge in the various tools are typically not only represented by special attributes in the data sources, but also by further refinement of the classes. This results in a large number of alternatives typically based on different classification principles in the various CAE tools. For instance, often special classes for pieces of equipment are introduced, such as a classification of pumps according to their function (e.g., centrifugal pump or hose pump). Even though the data models of such different pump classes only differ in few details, they are not entirely consistent. Applying only general concepts, as it is proposed here, constitutes the context for a more specific characterization of concepts without the need for a very fine-granular classification. For instance, a pump X in CAE tool A may correspond to a pump Y in CAE tool B, but it may not be classified identically and may not hold the identical set of attributes. However, since the most important attributes are captured by both representations, although in a slightly different manner, a matching of these crucial attributes and subsequent data integration is possible. In other words, focusing on these crucial attributes only facilitates understanding and mediating the specific meanings captured by the document ontologies: They represent the actual mediators in the system. They reduce the data to information by applying knowledge about the sources and user requirements (Wiederhold, 1992). Contrary to the approach of interchange standardization, the use of source ontologies and the specification of document ontologies allow for the autonomy and diversity of the data sources. In consequence, the data can be identified according to their context and can be classified according to the generally agreed basic domain knowledge in the global ontology, i.e. the problems imposed by flavors can be easily overcome. It is furthermore noteworthy that the knowledge captured in the global ontology is selectively reusable, as it is independent of the syntax and structure of the heterogeneous sources (Visser, Stuckenschmidt, Wache, & Vögele, 2000; Wache et al., 2001). If the structure of the existing sources is changed, or if other data sources are added from hitherto unconsidered CAE tools, only the source ontologies and the respective mappings between the source ontologies and the global ontology must be adjusted; all other semantic coherences are conserved. If additional aspects of the domain have to be modeled, the global ontology can be modified according to the domain ontology it is based on. Afterwards, it has to be made sure that the document ontologies capture and translate the required information, and the mappings to the source ontologies have to be extended and possibly adjusted if necessary. Finally, it should be mentioned that the global ontology provides a comprehensive view on the scattered data sets. It thus qualifies as a neutral point of access for the search and navigation within the data (Arens, Knoblock, & Hsu, 1996). 4. CIB implementation Ontologies can be modeled with different modeling techniques. Also they can be implemented in various kinds of languages (Uschold & Gruninger, 1996). For an overview on the various languages, including the corresponding underlying mechanism, the reader is referred to Neches et al. (1991) or Gomez-Perez et al. (2004). The tasks of merging and consolidating data require an ontology language and an associated inference mechanism which enables the reasoning of data given in rather complex contexts with many relations between the data objects. Also, a powerful inference engine is required to support the chosen mechanism to perform information integration even on an industrial scale. Two languages are basically qualified for such a task: the W3C standard Ontology Web Language (OWL) (Bechhofer et al., 2004) based on description logics, in combination with the Semantic Web Rule Language (SWRL) (Horrocks et al., 2004), which combines the rule language RuleML (Rule, 2009) and OWL, and the deductive ontology language F-Logic (Frame-Logic) (Kifer, Lausen, & Wu, 1996), which is an object-oriented and logic-based language originating from the deductive database community. Both languages, OWL in combination with SWRL on the one hand (Golbreich & Imai, 2004) and F-logic on the other hand (Maier, Aguado, et al., 2003), provide the expressiveness required to perform the integration tasks. However, according to Matheus, Baclawski, Kokar, and Letkowski (2005), particularly SWRL suffers from a number of drawbacks which renders it unsuitable for our application. The biggest disadvantage of SWRL is the need for checking for the consistency (i) within, (ii) across and (iii) between SWRL rules and OWL ontologies upon which they are built. Moreover, regarding practical implementation, further critical issues are reported including (i) the lack of negations, (ii) the need for procedural attachments, and (iii) the need for implementation of SWRL built-ins. By contrast, the deductive ontology language FLogic and the related inference engine OntoBroker (Angele, Kifer, & Lausen, 2009) have already been successfully tested in industrial applications (Maier, Schnurr, & Sure, 2003). According to Maier, Aguado, et al. (2003), the inference mechanism of deduction (i.e., the execution of production rules) is particularly useful for merging and consolidating distributed information. Also, more recently, Rector and Stevens (2008) reported on severe drawbacks of knowledge-driven applications based on OWL regarding usability and reliability. The deductive ontology language F-Logic and the inference engine OntoBroker have been chosen for implementation for the following reasons: Unlike most other ontology-based systems available today, OntoBroker is scalable and thus suitable for the processing of large data sets, as successfully demonstrated in industrial applications (Maier, Schnurr, et al., 2003). F-Logic (Kifer et al., 1996) allows the definition of rules for integration and mapping purposes as well as the formulation of expressive queries. It is a deductive, object-oriented database language, which combines the declarative semantics and expressiveness of a deductive database language with the rich data-modeling capabilities supported by the object-oriented data model. 4.1. Modeling in F-Logic For the modeling of the document ontologies and the global ontologies in F-Logic, the design environment OntoStudio (Weiten, 2009) has been chosen as an implementation basis. OntoStudio is a design environment for ontologies and semantic applications, which enables the representation of ontologies, the definition of mappings, and the formulation of queries. It further incorporates a graphical modeling interface for the creation of ontologies. Subsequently, these ontologies can be directly used as implementation models for the information integration tasks. Ontology engineering is furthermore supported by the inference engine, which checks the ontology for consistency, thus revealing logical errors and ensuring a reliable, validated data model. F-Logic not only supports the specification of mappings between individual classes and/or instances but also the definition of general integration rules. Rules encode generic information of the form: Whenever the precondition is satisfied, the conclusion also is. The precondition is also called the rule body and is formed by an arbitrary logical formula (e.g., using OR, NOT, AND, etc.). The conclusion, called the rule head, is derived from the facts stated in the rule body. Thus, rules may be used to represent declarative knowledge in the form “if A then B”, where A and B are statements about the extracted information expressed by means of ontological terms. The major advantage of using rules to formalize knowledge about concept relationships is that the knowledge can be expressed at the highest level of abstraction at which it is applicable. Thus, instead of establishing mappings between individual data items, the integration policy can be specified in more general terms. For instance, the assignment of all relevant properties (which are represented by attributes in the engineering documents) to plant items can be performed by a single rule statement within the CIB. Note that this rule applies to any part of any plant. Thus, the rule needs to be stated only once and can then be used for different tasks in the global ontology. This capability of advanced reasoning allows partial automation of the information integration process. Also, emanating from a given knowledge base, rules offer the possibility to derive new information – that is, to make implicit knowledge explicit and thus to extend the knowledge base incrementally. The newly derived information becomes part of the knowledge base and is thus available for the evaluation of subsequent queries. Rules have further advantages compared to conventional technologies regarding querying. The applicability of rules to the given data is automatically checked by the inference engine. Accordingly, the practicability of all queries using the particular rules is guaranteed. Also, rules are more intuitive and less error-prone with regard to querying than conventional database integration techniques (e.g., integration via SQL queries), especially in complex contexts with many relations between the data objects (Maier, Aguado, et al., 2003). Deductive logic enables to reduce the complexity of a query, since complex correlations between several classes may be expressed by a single relation. By contrast, the effort to realize SQL queries significantly increases with complexity. In the following, two examples of specific rules are given. The first example deals with the determination of a valid connection between a piece of equipment and the attached pipe. Therefore a couple of rules have to be considered for consistency checking. Typically, the pipe object specification holds the connection information for such an interconnection. It usually contains information about (i) the piece of equipment the pipe is connected to and (ii) to which particular part of the piece of equipment (e.g. nozzle) a connection exists; furthermore, (iii) a number of connection properties are listed. In order to check whether there is a connection between the pipe and a piece of equipment at all, only aspect (i) has to be considered. Yet for determining the validity of the connection, further checks have to be performed. These checks assess whether the right nozzle is used and whether the properties match, but also whether, by mistake, more than one pipe or instrument has been connected to the particular nozzle. When aspect (i) holds true, a relation of type ‘is connected to’ can be established between the instances representing the respective pipe and the piece of equipment.6 Then, after all further aspects have been verified, the connection between the pipe and the piece of equipment is finally regarded as valid. Exemplarily, the rule stating the validity of a nozzle-pipe connection (ii) is briefly explained. Informally, it reads as follows: IF THEN a piece of equipment AND a pipe belong to the same unit7 AND the piece of equipment possesses a nozzle AND the pipe has some connection information AND the identifiers of pipe and piece of equipment match the connection to the nozzle is classified as not inconsistent. A graphical representation of this rule generated by OntoStudio is given in Fig. 5. Essentially, it shows four different types of symbols: Ovals represent classes, rectangles denote predefined class-specific attributes, arrows indicate a relation interconnecting classes, and lines show which specific attribute (possibility including its value) are assigned to classes and may be further compared to other attributes. As a second example, the reconciliation of values belonging to major engineering attributes, is explained in the following. Any reconciliation is based on comparatively simple rules, which are formulated within the consolidation ontology as part of the global ontology. Such rules check, for instance, the relations between operating values, design values, and test values of an attribute – i.e., the operating value of a quantity must be less than or equal to the design value of that quantity, which in turn must be less than or equal to the test value. Another type of rules check if the value of a quantity is within a specified range, i.e., the min value must be less than or equal to the nominal value, which in turn must be less than or equal to the max value. The inference engine then determines all violations of these rules for all instances belonging to the respective attributes. A simplified representation of a rule comparing the minimal and the nominal value of an arbitrary physical quantity is as follows: the unique identifier of two plant items X and Y match AND X is assigned a min. design quantity AND Y is assigned a nominal design quantity AND the min. design quantity is greater than the nominal design quantity the min. design quantity of the plant item X is inconsistent. X and Y are variables which can represent all instances of the respective plant items. Since the F-logic rules obey first-order logic, the comparison includes the case that X and Y represent one and the same instance. Analogously, more specialized rules can also be stated for a combination of the previously mentioned value ranges and condition ranges, such as for a pressure checking as stated below: the unique identifier of two plant items X and Y match AND X is assigned a max. design pressure AND Y is assigned a max. operating pressure AND the max. operating pressure is greater than the max. design pressure the attribute max. operating pressure of the plant item X is inconsistent. Again, it should be stressed that the novelty of this approach does not consist in the formulation of these rather obvious rules; rather, it consists in the reduction of the required implementation effort: The designer/user only needs to specify a small number of rules at a high level of abstraction. The time-consuming work of applying the rules to the numerous application cases on the data level is then automatically performed by the inference engine. 4.2. Realization of a peripheral software environment The realization of the proposed CIB calls for an additional software environment to be of use in industry. Therefore, a peripheral software environment (PSE) has been developed, which enables the user-friendly application of the CIB by means of different query and management interfaces. The main purpose of the PSE is to hide the complexity of the ontology by mediating between the user and the underlying system. Ultimately, the project designer has to be able to operate the CIB without knowledge about the structure of the knowledge base and its underlying ontologies. However, if required, the knowledge base can be viewed and adjusted by means of the ontology editor OntoStudio at any time by an expert user. In order to gain acceptance in industrial practice, a graphical user interface (GUI, cf. Figs. 6 and 7) is mandatory. The GUI visualizes the results on demand in an intelligible manner and provides some additional data management functionality. This allows, on the one hand, for the visualization of the integrated and consolidated data inferred by the inference engine. On the other hand, it facilitates tasks like the import and export of the respective XML documents or the adjustment of the scope of data. Besides, the software system has to support collaborative work on different computers. To that end, data redundancy has to be avoided to the extent possible, which can be realized by concepts enabling shared access to centrally stored consistent data sets for every user. The demand for collaborative and distributed work without data redundancy requires a client–server system where the data is stored on a central server. For our purposes, OntoBroker has been chosen as the server software. As an additional benefit, this architecture allows to run the inference engine on a server machine with high computing power. Accordingly, the user interface and some other parts required for client–server communication constitute the client. All parts of the ontology – i.e., rules, queries and facts – are stored in F-Logic within the OntoBroker system. OntoBroker can be configured to store all data either in CPU-memory or persistent, i.e., in a relational database (Oracle, H2DB, etc.). The development of the GUI has been guided and tested by expert project engineers from Evonik Degussa. In order to achieve a platform-independent realization of the GUI, the Standard Widget Toolkit (SWT) has been used in combination with the development environment Eclipse (Budinsky, Brodsky, & Merks, 2003). Eclipse facilitates the realization of different ‘perspectives’. These perspectives can be customized by arranging window areas called ‘views’. These views can be filled with different SWT elements like buttons, drop-down boxes, tables, and trees (cf. Fig. 6). 5. Industrial application of the CIB Fig. 7. Screenshot of the GUI. Semantic inconsistencies are marked with red color. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.) The integration prototype has been applied for solving a specific information integration problem at Evonik Degussa. In this particular integration problem, the engineering data is represented in a specific in-house XML schema called PlantXML (Anhäuser et al., 2004). It should be noted that, although PlantXML has been chosen as the first application case, the CIB is essentially designed such that it can handle arbitrary XML formats. Evonik Degussa Engineering is the in-house engineering department of Evonik Degussa, the world’s biggest producer of specialty chemistry. The engineering department is responsible for such different tasks as feasibility studies, conceptual front-end engineering, basic and detail engineering, and construction of complete chemical plants. However, the engineering department mainly focuses on basic engineering and extended basic engineering, respectively. For detail engineering, external contractors are typically commissioned. PlantXML is an existing in-house solution for the information exchange between application tools used by the engineering department of Evonik Degussa. It defines specific XML schemas for the different main engineering objects: (i) XML-EQP for the design of machines and apparatuses, (ii) XML-EMR for the design of instruments and control systems, (iii) XML-RLT for piping engineering, and (iv) XML-SiAr for the design of safety-related fittings and safety valves. Custom-made converters handle the import and export of the XML files to and from CAE tools by mediating between the in-house standard and the internal data models of the application tools. To allow for an evolutionary and non-deterministic workflow, partial exports of documents are supported by PlantXML. PlantXML does not claim to be a completely defined, generally applicable data model. Instead, it represents a pragmatic solution tailored to the specific needs of Evonik Degussa Engineering. Thus, only task-specific engineering data are modeled and transferred; specific data of the software tools are not considered. Also, interrelations and dependencies between the data stemming from different schema are not explicitly modeled. Hence, although the respective schemas are structurally similar, they are not harmonized with respect to content. Hence, the contents of the four XML schemas are partially overlapping. The structure and nomenclature of specific plant item attributes are organized according to the datasheets, which were traditionally used for manual data exchange between the respective disciplines. Though the main purpose of PlantXML is to establish an overall exchange layer for engineering design data, it also provides a good starting point for a thorough data integration as it supports the access to structurally homogenous data sets. In the course of a typical project, different data sets of real world objects are generated. These versions arise for two different reasons: either they describe different functionality, or they are frequently modified over time. To shorten processing times, often partial exports of PlantXML files are transferred, which do not contain a full specification of a particular plant item. Also, PlantXML supports the exchange of subsets of data – that is, the contained objects may be fully specified, but represent only a part of the entire data stock of a design project, e.g., the part of the project which is assigned to a particular sub-contractor. Due to this kind of data exchange, no stringent versioning of the documents is possible. Accordingly, the project engineer is not able to determine the current status of the overall data stock on the basis of a simple version number of a document. The use case consisted of different scenarios with varying scope and complexity. It included different sets of PlantXML files containing data of different complexity, reaching from simple test data to real project data related to a design of a mid-size chemical plant, which comprised up to several hundred electrical and instrumentation devices and pipes, and up to 30 pieces of equipment. 5.2. Implementation As a first step of each information integration scenario, the engineering data distributed across the various PlantXML documents have to be assembled in the CIB. Subsequently, they can be merged and consolidated. Within the CIB, the data sets stemming from different XML documents are kept separately and are individually prepared for integration. A distinct import ontology is created for each PlantXML schema. The elaboration of the required document ontologies has been carried out in close cooperation with the respective experts from Evonik Degussa. 5.2.1. Versioning The main focus of the case study is the retrieval of the overall current status of the design data. In this context, the term ‘XML document’ denotes a set of XML files that (i) originate from the same source and (ii) conform to the same XML schema (e.g., XMLRLT). Such XML files are considered as different versions of the same XML document. As an example, all data stemming from the CAE tool Comos® Feed, which can be either converted to the EQP schema or the EMR schema, are separately prepared. Preparation includes the detection and elimination of outdated and redundant information as well as the resolution of versioning conflicts. Such preprocessing is necessary since there may be several versions of an XML document, as mentioned previously. In other words, each version may include both current information as well as outdated and/or redundant information. The resulting versioning conflicts need to be resolved before integration. The current status of the XML documents obeying one PlantXML schema and stemming from one source (CAE tool) is referred to as sum-file in the following. After the generation of a sum-file, it can be exported into the repository it came from, and it can be integrated in the design process if required. 5.2.2. Integration On the basis of the sum-files, the semantic integration of the design data can be performed. It essentially aims at two goals. On the one hand, it determines which plant items are specified in the different CAE tools. This is required since, for instance, data belonging to the EMR schema are processed by different CAE tools such as Comos® Feed, SmartPlant® P&ID (Intergraph, 2009) and others. As a result of this determination, conclusions can be drawn regarding the processing status within the respective CAE tool. Thus, it is identifiable, which plant items are not yet specified in a particular CAE tool and hence whether the project’s schedule could be delayed. On the other hand, a classical integration of the data sets can be accomplished, such that plant items, which are present in different files of the respective CAE tools, can be merged into a single file. Consequently, the semantic integration generates one version of each PlantXML schema, which contains all relevant plant items, its fixtures and nozzles as well as all relevant technical properties. In doing so, the plant items stemming from different CAE tools can be combined in any order, as long as they obey the same schema. The set of all integrated plant items is denoted as master-file, which can be exported to the respective CAE tools like the sum-file. 5.2.3. Consolidation Eventually, based on the current and integrated data (masterfile), the semantic consolidation can be performed. In the case study considered, only the realization perspective of plant items has been of interest. The focus has been on the following two aspects: (i) consistency checking of topological and mereological information, such as the matching of the inner diameter of a nozzle and the connecting pipe, or the existence of similar fixtures and nozzles in different specification of identical plant items; (ii) reconciliation of “crucial” design properties such as temperature, pressure, or construction material. The consistency check is performed in two steps: First of all, semantically equivalent properties and plant items are consolidated within each PlantXML schema, by checking for equality. For example, the minimal design temperatures of a particular apparatus should be identical, regardless of the CAE tool the data originates from. With respect to mereological consistency, a temperature loop should comprise the same sensors and actors, independent of the CAE tool it has been specified in. In addition, semantic interdependencies between plant items on the one hand and properties on the other hand can be reconciled in a second step. For instance, a rule is formulated for checking the design properties, stating that the minimal design pressure of an apparatus always has to be less than its maximum design pressure. Regarding the consistency check of the connectivity between plant items, we refer to the previously mentioned example of a connection between a nozzle and a pipe. The prototype would detect that the pipe is connected to a nozzle of a vessel, and thus establish a link between the pipe and the vessel. In summary, the following functions are realized within the CIB: Redundant information can be detected and removed by means of the integration. This applies particularly to data entities which occur in different versions describing an identical real world object. Those are identified and are represented by the most current entity within the merged data stock. In case the attributes of such an object are distributed over different PlantXML files, only the most current attribute values are considered within the CIB. Implicit dependencies between the entities can be identified and presented in an explicit manner by means of the integration prototype. This applies to, e.g., topological relations between different plant items of the chemical plant as well as to the linking of logically correlated information items and its different specifications. 5.3. Visualization of integration and consolidation results In order to demonstrate how the results of the integration and consolidation tasks are visualized, we have chosen the example of a data set describing electrical and instrumentation (E&I) devices. The results have been taken from the data of a midsize project, which comprises 521 devices. Before the results of the specific case are discussed, a general overview on the structure of the GUI is given. The implemented perspectives are divided into two main views: On the left, the selector-view allows for navigating the entire data imported in the prototype, sorted by the different types of engineering objects (e.g., equipment, E&I, piping). On the right, the main tables holding the engineering information are shown, separated by columns and distinguished by their respective data sources. In Fig. 6, a screenshot of the integration perspective of the CIB is given. The CIB provides several means to specify the project scope such that the user may only view and work on the relevant data. For instance, the project data displayed belong to project P471104712-00 (cf. input mask (1) as well as the message at the bottom right of screenshot). The selector-view comprises all E&I data which are contained in the most current sum-file (2). As an example, the selected project data comprises 468 instrument devices, a member of which is the control loop 8110.06 which in turn holds the mass flow sensor FT1 8110.06. The table view then directly states from which CAE tool (data source) the actual data were imported. The exemplary E&I data (3) were originally generated in two CAE tools, partly in Comos® Feed and partly in SmartPlant® P&ID (SPPID). The master-file, which contains in fact a combination of E&I items from each source, is shown in the ‘Master’ column (4). The collection of the items constituting the master-file, can simply be assembled by using the buttons ‘master column manipulation’ (5). The ‘Master’ column (4) holds all master data together with reference to the CAE tool the data was originally generated from. Furthermore, the GUI indicates consistencies and inconsistencies between the engineering data of the respective data sources by means of color and symbols to allow for a rapid screening and general support for the user. Whereas the color in the table view (6) refers to the consistency of the processing state (integration), the symbols used in the selector-view (2) refer to the consistency with respect to the consolidation. For instance, data highlighted in green are consistent in the sense that they have been specified in both sources as required (e.g., item 8110.01). Item 8110.12, on the contrary, is not yet consistent in terms of processing state – data from SPPID is missing – and hence it is highlighted in red. Regarding the symbols, the tick (2) in front of item 8110.03 means that there are no conflicting attributes and no conflicting mereological relations attached to this particular instance data item. By contrast, an X and the respective key (7) (e.g., FT1 8110.06) give some indication of the type of inconsistency. For the ease of inconsistency recognition – e.g., which kind of inconsistency problem occurs for the E&I-item – a simple “traffic-light” system has been introduced. Each icon in front of an object or its attributes has three “lights”. These can be either green (consistent) or red (inconsistent). These three lights are used to indicate (i) value inconsistency, (ii) unit of measure inconsistency or (iii) semantic inconsistency. Rudimentary information about the processed plant items within a project is visualized for the convenience of the project manager (8). Principally, it is indicated how many of the currently processed plant items are not yet fully specified or may contain attribute and mereological inconsistencies respectively. Eventually, the consolidated, possibly integrated data listed in the ‘Master’ column can be transferred back to the corresponding software tools (message “Create master-file” (9)), and thus the consolidated data can be directly reintegrated in the standard workflow. As an example for an intelligible visualization, we consider a scenario the results of which are shown in Fig. 7: In the course of the specification of a temperature sensor for a specific piece of equipment, the designer has made a mistake and has set the average temperature (tempUsual) to be higher than the minimum temperature (tempMin) of the temperature control sensor. Due to the vast amount of sensors specified in a typically project, this error was not detected directly. The undetected error would eventually lead to incorrect measurements in the worst case and hence would affect process control later on. However, for the final review of the design data, the designer uses the CIB, and so an inconsistency is displayed in the GUI as shown in Fig. 7. In the selector-view (left hand side of Fig. 7), the X marks control loops, (e.g., loop 4500.01), which has an inconsistent instrument (e.g., the temperature sensor TT4500.01). Fig. 7 also shows on the right how a semantic inconsistency is highlighted on the table view. By means of the CIB, the time necessary for carrying out the reconciliation task can be shortened significantly. Since the potential inconsistencies are presented concisely, the designer can quickly pick the marked items, interpret the inconsistencies reported, and eventually perform the occurring correction. In the particular case, a predefined rule has detected the inconsistencies, the data set has been labeled to be inconsistent, the properties tempUsual and tempMin are highlighted in red and the icon indicates a semantic inconsistency. 5.4. Computational effort Our experience with semantic technologies clearly reveals very high computational load when it comes to reasoning, particularly for large instance data sets. For the computation of the mid-size test data stemming from the use case provided by Evonik Degussa, a fairly powerful computer was used (Intel XEON 1.6 GHz 8 CPUs, 12 GB RAM). Unfortunately, it still turned out that CPU-memory has not been sufficient for a rapid reasoning (the data of which are shown in Fig. 6). For the reasoning of the data of the mid-size project, the installed OntoBroker uses a permanent storage implemented by means of a H2 database (H2DB, 2009), such that OntoBroker can store its data in database files inside the file system. For the mid-size project, about 22 GB of CPU memory are used at run time and about 50 GB of hard disk space of temporary inferencing data have to be saved locally in the database during the inferencing process. Overall, the run of 4 queries required for integration took about 8 h. However, applying a RAM-based storage configuration instead enables access to the temporary inference data faster and thus increases the system’s performance significantly. Hence, a more powerful computer (SGI Altix 4700 64 CPUs, 128 GB RAM) with capabilities to store the temporary inference data in the RAM has been used in a second test. The second machine obtained the results of the same 4 queries on the identical data set after about 2 min of computation time. 5.5. Lessons learned from the application example The applicability of semantic technologies to the integration and consolidation of real-life engineering design data has been demonstrated successfully. The application discussed in Section 5 has illustrated the advantages of ontology-based information modeling and of the reasoning capabilities of semantic technologies but also revealed its drawbacks. Still, we are convinced that the approach proves suitable for industrial use for the following reason. The application of semantic technologies as the core of the proposed CIB facilitates the generation of a coherent and consolidated data stock. For the time being, the integration and consolidation tasks mentioned in Section 5.2 are mainly performed manually in industrial practice. For instance, one designer typically spends several days on the consolidation of design data for electrical and instrumentation devices. Accordingly, there is no doubt that semantic technologies provide a unique benefit to information integration applications due to its powerful querying and reasoning capabilities. That way, highly skilled design engineers have to spend less time on manual integration and consolidation issues. Obviously, such functionality helps to further shorten the engineering process. This view is shared by our partners at Evonik Degussa. The downside of the use of semantic technologies certainly is the need for very powerful computer hardware to be able to reason within reasonable time. However, the reasoning often does not have to be performed in very short time. A feasible scenario for the integration or the consolidation task would be to perform the consistency check over night. Such time constraints allow for the usage of standard (powerful) computers. Besides, continuously decreasing prices of computer hardware will permit the application of higher computational power installed at moderate cost such that the encountered performance issues will possibly become irrelevant in the near future. 6. Related work The use of semantic technologies for the integration of distributed data is a long-standing research issue; an overview on the contributions introducing the fundamental concepts used in our work can be found elsewhere (e.g., Wache et al., 2001; Paton et al., 2000; Visser et al., 2000). More recent publications report on the application of such concepts mainly in the field of medicine and biology, e.g., Lister, Lord, Pocock, and Wipat (2009), Buitelaar, Cimiano, Frank, Hartung, and Racioppa (2008), Martin et al. (2008), or Kunapareddy, Mirhaji, Richards, and Casscells (2006). In all of these contributions, the originally proposed hybrid (or mediator-based) approach is extended in a similar way to the one proposed in this contribution. Originally, mediator-based approaches have viewed the purpose of a global ontology as simply a union of source ontologies, rather than as a semantically rich description of the application domain (e.g., Rousset & Reynaud, 2004; Wache et al., 2001). However, if a global ontology is defined merely as a model of a set of data sources, it becomes brittle with respect to the addition of new data sources and new formats. Thus, in most previous research attempts an intermediate ontology level has been introduced to address this issue. This intermediate level may be called differently, for instance ‘syntactic ontology’ (Lister et al., 2009) or ‘Simple Concept Organization System’ (Kunapareddy et al., 2006), but in each case, the underlying objective is to be more flexible in terms of adaptability and extendibility. Thus, we can claim that our approach complies with the state-of-the-art concepts published for semantic integration problems. Only very few contributions demonstrating the application of semantic technologies to engineering design problems have been published to date. Exemplarily, three of them are briefly sketched in the following, and the main differences to our work are pointed out. Peltomaa, Helaakoski, and Tuikkanen (2008) have developed an information system dealing with semantic information integration in the heavy industries. A typical production process is controlled by several information systems, each of them having different concept hierarchies, database structures, and information models. The project aims at the integration of three given information systems. The first contains failure and breakdown information, the second controls machines and devices, and the third incorporates the information from the previous two for further processing. The objective is (i) to integrate the data stemming from all information systems in order to offer a single view onto machine, device, breakdown, and failure information, and (ii) to enable proactive maintenance in the manufacturing process, including the planning of maintenance work and controlling the lead time of maintenance. The approach considers a so-called semantic layer for the specification of domain knowledge, which is done in a manner that is comparable to our approach, i.e., by means of global and local ontologies similar to our work. The implementation is also based on the ontology language F-Logic and the inference engine OntoBroker. However, contrary to our approach, Peltomaa et al. did not rely on an existing domain representation, which could have been customized to their own needs. Rather they used a ‘bottom-up’ approach by developing a proprietary, tool-specific ontology from scratch to fulfill the requirements of the information integration problem at hand. Adams, Dullea, Clark, Sripada, and Barrett (2000) address data integration of multiple data sources in order to establish a knowledge base for maintenance purposes in the aeronautical industry. The authors investigate the feasibility of applying a knowledge base to interface the information sources to the decision support system. The investigation relies on the commercially available knowledgebased development environment Cyc (Lenat, 1995), which provides an expressive knowledge-representation language (CycL), an efficient inference engine, and a large repository of common-sense knowledge. In analogy to this contribution, the existing domain knowledge is specified and extended to meet the requirements of the intended integration prototype. However, Adams et al. intend to develop a software tools that is tailored to the legacy systems encountered at their company and do not propose a general approach to integrate other data from other data bases or formats (e.g., XML). Finally, the MODALE project (Hefke, Szulman, & Trifu, 2005) aims at the development of an ontology-based reference model for semantic data integration for the manufacturing industries. The project is carried out in cooperation with various partners from the automotive industry. The ultimate goal is the integration of all data stemming from the various disciplines involved in the planning and operation of a complex manufacturing plant. The data required and generated by the multiple disciplines is captured by the reference model for the entire lifecycle. Thus, all required information can be extracted efficiently and consistently from the central reference model. The approach is based on the attempt to represent the content of the partners’ proprietary data sources in so-called partner ontologies, which are then integrated by a common reference model. The representation of the reference model is implemented in the RDF language (RDF, 2009). However, the approach failed in the sense that most of the partners preferred to work with their internal well-established tools and data formats instead of developing specific partner ontologies since the ontology management tools available at that time were considered to be too complex and too difficult to handle (Hefke et al., 2005). Generally, our work differs from those above in that we were able to build our approach on a field-tested ontology (i.e., OntoCAPE) with its extensive substrate of common-sense engineering design knowledge. This greatly facilitated the formation of a knowledge base for the given task. This advantage facilitates to focus on the development of a flexible concept for a prototype architecture which is suited for large-scale information integration and the processing of non-proprietary formats in engineering applications. With respect to chemical and pharmaceutical engineering, three other research groups have investigated the use of semantic technologies for information integration. Weiten and Wozny (2003, 2004) developed a system to archive the diverse results of research projects (publications, experimental data, mathematical models, etc.) in an integrated fashion. The system uses ontologies to annotate and link heterogeneous information sources. Similarly, the Process Data Warehouse (PDW, Brandt et al., 2008) uses ontologies to describe and interrelate the contents of chemical engineering documents and data stores. Within the PDW, ontologies are utilized for the annotation of electronic documents and data stores. That way, one obtains a consistent, integrated representation of the contents of these heterogeneous information sources. These content descriptions can then be processed and evaluated by semantic searching and browsing functions provided by the PDW, which support the navigation between and the retrieval of information resources. Both the PDW and the archival system developed by Weiten and Wozny allow to establish semantic relations between the information sources via content-specific metadata. However, both of these systems do not merge and reconcile the contents of the data sources as it is necessary for a consolidation of engineering data. Rather, they provide a cross-linked data repository, which allows for improved data retrieval. Also, both approaches only dealt with academic “toy” problems. The consolidation of large amounts of real-life distributed engineering design data were not tackled. Finally, Venkatasubramanian and co-workers have published several contributions on the topic of ontology-based informatics infrastructure in the field of pharmaceutical industries (e.g., Akkisetty, Reklaitis, & Venkatasubramanian, 2009; Hailemariam & Venkatasubramanian, 2010; Sesen, Suresh, Banares-Alcantara, & Venkatasubramanian, 2010; Venkatasubramanian et al., 2006). These contributions address different problems within the phase of product and process development regarding information integration. Similar to the approach proposed in this article, the authors have developed a framework which is based on the use of a domain (task) ontology (i.e., POPE and OntoReg respectively), which are implemented using OWL and partly SWRL. However, the focus of this work is on the extraction of pharmaceutical product information and mathematical model development knowledge, which is complementary to our approach. Unlike this work, Venkatasubramanian and co-workers have integrated their framework with the technical computing environment Matlab to use their knowledge for the generation of mathematical models. Finally, although the framework has been validated against industrial relevant examples, it has not yet been field-tested in a large industrial project. 7. Conclusions In this contribution, a novel approach for the integration and consolidation of distributed engineering design data has been presented and its implementation in the software prototype CIB has been described. The approach strives for two major goals: (i) a highly flexible and extensible structure in order to be generally applicable to heterogeneous and distributed data sets, and (ii) a concise but expressive knowledge base for resolving semantic heterogeneities in design data in order to shorten project durations and improve the data quality. The overall concept is based on a combination of XML and semantic technologies – the former is currently the technique of choice when in comes to data integration in industry, while the latter have only been applied in a rather explorative manner so far. The connection between the two technologies is established by defining an explicit representation of the XML data sources’ semantics by means of ontologies. By merging proven integration software and state-of-the-art technology, we combine the advantages of both solutions: Our approach and its realization within the CIB distinguishes itself by its generality, which allows for the integration of any design data expressed by XML due to its flexible, transparent, and extensible structure. The CIB is independent of any proprietary data format and thus widely applicable. This generality, however, manifests itself in a higher demand for computational power, compared to a tailored integration solution. The core of our approach is an expressive knowledge base, which is based on the formal ontology OntoCAPE. The generated knowledge base incorporates specific parts of OntoCAPE, which have been partly extended and customized for the task of integration and consolidation of design data. The predefined vocabulary within the applied ontologies serves as a stable conceptual interface to the engineering data. Also, the knowledge captured by the ontology is reusable and extensible (customizable) to other engineering data. This allows for the flexibility to adapt to CAE tools that were not envisioned at the time of generation of the ontology. Semantic technologies have the ability of advanced reasoning which is for instance enabled by means of rules and expressive queries. The reasoning itself is performed by the commercial inference engine OntoBroker, which is scalable and thus applicable to large data sets like they are arising in real industrial applications. For a first benchmark, the reliability of the CIB and its ability to handle large-scale industrial data sets have been evaluated by means of several integration problems provided by Evonik Degussa. The expressiveness of the knowledge base and the applicability of the integration prototype have been successfully tested in different scenarios with diverting industrial data stocks. The resolving of semantic heterogeneities includes the detection and visualization of design errors. In industry, this work still needs to be performed manually, at considerably effort and cost. With the rapid screening of design inconsistencies, as proposed here, the reconciliation task can be accelerated, and the data quality can be improved significantly. Overall, the concept performs the integration and consolidation of data from different CAE tools without the need of standardization. Thus, the CIB constitutes a highly flexible and very expressive integration platform, which can simply be linked to different XML in-house technologies and is thus reusable across companies and organizations. The existence of an integrated and consolidated data stock within the CIB builds a sound foundation for further, more sophisticated analysis, which might help to further improve the overall engineering design process. For example, our approach can be extended to address the management of revisions within the engineering design process. Instead of detecting unintended design errors, as it is done for the time being, we may use the same mechanisms to discover all inconsistencies that result from intentionally imposed deviations, typically occurring from revisions within the course of a design process. By rating potential inconsistencies with some economic quantity, an evaluation of alternative revisions become possible, allowing for the determination of the one change with the lowest impact on project costs. While this is still subject to current research, it further highlights the great potential of the application of semantic technologies in the engineering design process. Acknowledgements Most of the research described in this article has been performed in the context of CRC 476 IMPROVE8 and TC 61 (), funded by the German Research Foundation (DFG). The authors would like to thank their co-workers, Michael Wiedau, Alexander Sempf, Christian Bruns, Kalle Fischer, Sebastian Mauer, Sebastian Tobies and Stefan Arts, who helped to develop and implement major parts of the prototype. Also, the contribution of the industrial partners Evonik Degussa and Ontoprise is greatly acknowledged, with special thanks to Heiner Temmen, Hannes Richert and Felix Anhäuser (all working for Evonik Degussa) and Moritz Weiten, Mike Ulrich, and Joachim Redmer (all working for Ontoprise). 
Merging domain ontologies based on the WordNet system and Fuzzy Formal Concept Analysis techniques. 1. Introduction While the development of World Wide Web has allowed people to easily access information, but still have many drawbacks remain. Due to large quantities of information available, locating the desired information can be time consuming. The next generation of the web requires a convenient and efficient way to improve this situation. In 1998, Berners-Lee proposed the notion of Semantic Web [1]. The core technology of a Semantic Web is an artifact called ontology, and the Semantic Web relies on formal ontologies to structure data for machine understanding. In other words, the Semantic Web is a mode of communication between machine and users. The construction of domain ontologies relies on different experts, different tools, different techniques, and different languages. Domain ontologies include many differences and conflicts even though ontologies exist in the same domain. Moreover, the extant domain ontologies may need to be updated to solve domain problems. Thus, the manner of integrating ontologies is a very important issue. Gruber indicated that an ontology is an explicit specification of a conceptualization [2]. Hendler defined an ontology as a set of knowledge terms, including vocabulary, semantic interconnections, and some simple rules of inference and logic for a particular topic [3]. Ontology is a formal conceptualization of the real world. In general, an ontology consists of concepts, attributes, relations, operations, axioms, and instances. However, an ontology lacks Many different contents and structures exist in constructed ontologies, including those that exist in the same domain. If extant domain ontologies can be used, time and money can be saved. However, domain knowledge changes fast. In addition, the extant domain ontologies may require updates to solve domain problems. The reuse of extant ontologies is an important topic for their application. Thus, the integration of extant domain ontologies is of considerable importance. In this paper, we propose a new method for combining the WordNet and Fuzzy Formal Concept Analysis (FFCA) techniques for merging ontologies with the same domain, called FFCA-Merge. Through the method, two extant ontologies can be converted into a fuzzy ontology. The new fuzzy ontology is more flexible than a general ontology. The experimental results indicate that our method can merge domain ontologies effectively. information to deal with uncertainty in problems. Fuzzy ontology is better suited to describe domain knowledge for solving uncertainty reasoning problems [4–10]. An example is shown in Fig. 1. The concept of Ball has the attributes of color and size; the ball has the operation of throw and kick to be controlled; the ball has an axiom of entity; the ball has the instances of baseball, basketball and volleyball; and the ball has relations to each element. If the gray boxes have fuzzy information in the part-of relationship between the Equipment and Game Equipment, the Ball might be similar to the Game Equipment. Fenza et al. [10] presented a hybrid framework for achieving a fuzzy matchmaking of Semantic Web services. They indicated the matchmaking activity exploits a mathematical model, the fuzzy multiset to suitably represent the multi-granular information. Lee et al. [6,7] presented two methods: an ontology-based computational intelligent multi-agent system and an ontologybased intelligent decision support agent (OIDSA) for Capability Maturity Model Integration (CMMI) assessment. They used ontology model to represent the CMMI domain knowledge that will be adopted by the computational intelligent multi-agent. So, the CMMI ontology is predefined by domain experts, and created by the ontology generating system. Their experimental results indicate that the ontology-based computational intelligent multi-agent can effectively summarize the evaluation reports for the CMMI assessment [6]. They also utilized the fuzzy inference agent computes the similarity of the planned progress report and actual progress report, based on the CMMI ontology, the project personal ontology, and natural language processing results. Their experimental results show that the OIDSA can work effectively for project monitoring and control of CMMI [7]. Reformat and Ly proposed an application of ontology, in the sense of the Semantic Web, for development of computing with words based systems capable of performing operations on propositions including their semantics. The ontology-based approach is very flexible and provides a rich environment for expressing different types of information including perceptions. It also provides a simple way of personalization of propositions [9]. Calegari and Farina presented a concept network based on the evolution of a dynamical fuzzy ontology. A dynamical fuzzy ontology can manage vague and imprecise information. Fuzzy ontologies were defined by integrating Fuzz Set Theory into ontology domain, so that a truth value is assigned to each concept and relation. They examined the case where the truth values change in time according to the queries executed on the represented knowledge domain [8]. The integration of ontologies has become an important research subject in recent years. Many tools [11–13] and techniques [14,15] have been used to construct domain ontologies, however various ontologies are constructed using different tools or techniques despite existing at the same domains. Nevertheless, experts use various tools and languages to create an ontology that will have different architectures even though having the same or similar concepts in the domains. Therefore, ontology integration becomes an important task. Choi et al. [16] have divided the ontology into global ontology, local ontology, and domain ontology. In their definition, a global ontology has a domain topic schema lacks detailed content. It also names upper ontology such as Cyc (Cycorp) [17], SUMO (Suggested Upper Merged Ontology) [18] and WordNet [19]. The scale of local ontology is smaller than that of global ontology. Local ontology has topic schema in a certain specific field, and the local ontology may belong to different domains. Domain ontology has plentiful or particular information to apply to specific task. In this paper, we focus on the merging of domain ontologies having the same domain topics. The rest of the paper is organized as follows. Section 2 outlines related research. Section 3 describes the FFCA-Merge method. Experiments and discussions are presented in Section 4. Finally, Section 5 presents conclusions and future work. 2. Related research 2.1. Domain ontology merge and alignment The field of domain ontology merging and alignment generally includes the following six strategies [20]. (1) Strategies based on linguistic matching: They complete an integration task according to the linguistic meaning of words. (2) Structure-based strategies: They adopt the structural information relating super-concepts and sub-concepts. (3) Constraint-based strategy: They complete the integrative task according to the constraints in each concept. (4) Instance-based strategy: They complete the integrative task according to the external instance from a database or the Internet. (5) Auxiliary-based strategy: They use auxiliary information such as WordNet to complete the integrative task. (6) Hybrid-based strategy: They combine different strategies to complete the integrative task. Noy and Musen [21] proposed a SMART system to match similar class names based on linguistic characteristics. The system is semiautomatic and makes suggestions to users to resolve conflicts. In addition, Noy and Musen [22] proposed a PROMPT system based on linguistics and structural knowledge to determine similarities between terms. PROMPT is a semi-automatic system that generates a list of suggestions for users. Next, Noy and Musen developed an Anchor-PROMPT system [23] to identify pairs of related terms from source ontologies. The system then compares the terms to identify similar terms and to generate a set of new pairs of similar terms. Anchor-PROMPT is a semi-automatic method. After analysis of terms, it can display the results, allowing users to merge source ontologies. Chalupsky’s OntoMorph system [24] provides a powerful rules language for ontology merging and generating a knowledge-base translator. OntoMorph is also a semi-automatic system. However, users may require more background knowledge to use the powerful rules language for ontology mapping as it does not facilitate use of the initial list. McGuinness et al. proposed a Chimaera system [25]. The system is an ontology merging tool based on an ontolingual ontology editor. If the linguistic match can be found, ontology merging can be achieved automatically. The system is difficult to implement automatically because it is not easy to find all linguistic matches from uncertain data. Ichise et al. have developed a HICAL (Hierarchical Concept Alignment system) system [26] that provides a concept hierarchy management for ontology merging and alignment. The method is semi-automatic and uses machine-learning techniques to align multiple concept hierarchies; it also exploits data instances in overlap mapping. The drawback of the system is that categorizing different words under the same concept may result in ambiguity. FCA-Merge (Formal Concept Analysis Merge) [27] consists of the following three steps: (1) Extract instances from documents and generate formal contexts. (2) Output the pruned concept lattice by the Titanic algorithm. (3) Establish the merged ontology based on the pruned concept lattices. FCA-Merge is also a semi-automatic method. The system translates concept lattices into the merged ontology, but a domain expert must make revisions manually. The CMS (CROSI Mapping System) system [28] consults external linguistic resources, feature selections, multi-strategy similarity aggregators, and similarity evaluators. The system is automatic but suffers from the drawback that while it provides many techniques for the user, and user can choose one or more techniques for mapping ontologies, the user may not know how to choose suitable techniques for specific situations. The ontologies merging and aliment, mentioned above, have two drawbacks: (1) Such techniques are hard to achieve automatically. (2) The operation of merging ontologies lacks fuzzy information. In this paper, we use FFCA [29,30] and WordNet techniques to merge and align domain ontology automatically. After the merging operations, the system translates the merged ontology to a fuzzy ontology. The membership values of the fuzzy concepts can be used in flexible applications and can process uncertain information. FCA-Merge is an ontology merging method depending on the FCA, and our system improves upon FCA-Merge to generate a fuzzy ontology. The experiments indicate our method is useful. 2.2. Related techniques Two major techniques, WordNet and FFCA, are used to implementation of this system. WordNet contains much information concerning the English language can be reused. FFCA technique comes from FCA method which is the conceptual framework applied in our system. WordNet, FCA and FFCA techniques are introduced below. 2.2.1. WordNet WordNet is a large semantic lexicon for the English language created and maintained by Princeton University by Professor George A. Miller since 1985. The contents of WordNet constitute the part of speech for each word – nouns, verbs, adjectives, and adverbs – and provides knowledge structure consisting of hypernym, hyponym, sister term, domain term, etc. In linguistics, words sounding alike with many meanings are known as homonyms, and multiple words having the same meaning are called synonyms (WordNet’s authors named it synsets). Through definitions and sample sentences glossing each term, homonyms and synonyms are both completely catalogued in WordNet. An example of WordNet query results is shown in Fig. 2. The upper ontology SUMO (Suggested Upper Merged Ontology) defines the conceptual mapping with respect to WordNet synsets [31]. The WordNet qualifies as an upper ontology as it includes the most general concepts and more specialized concepts, related to each other not only by the subsumption relations, but also by other semantic relations, such as part-of and cause [31]. 2.2.2. Formal Concept Analysis (FCA) FCA [32] is a conceptual framework. It has been applied to the structure and visualization of data analysis to make data more understandable [33]. FCA is based on lattice theory [34], a wellestablished mathematical discipline. It has been applied to many different fields, such as, sociology, psychology, medicine, linguistics, and computer science. The FCA method defines formal contexts to represent the relationships between objects and attributes in a domain. From the formal contexts, FCA can generate formal concepts and interpret the corresponding concept lattice so that information can be retrieved effectively. The FCA method is widely used for various applications, such as, text processing [35–37], ontology generation [29,30,43–45], ontology merging [29,38], email managing [39], e-learning [40], web navigation [41], and expert systems [42] in computer science. FCA includes two steps: (1) contributing context, and (2) translating context into the concept lattice. The definition of context contains three elements (G, M, I), where G and M are two sets of context elements called object and attribute, respectively, and I is a relationship between the object and the attribute. Objects stand for documents or entities and attributes identify terms or properties. The concept of context has two sets, E and I. E stands for extensional and I stands for intension of concept. They also signify extent and intent. The extent is a collection of objects to satisfy specific attributes. If B is a set of attributes and B ⊆ M, regard B as the extent of B (B is a set of objects) and it satisfies common attributes for all B. The definition is: B :={g ∈ G|(g, m) ∈ I for all m ∈ B}. The intent is a collection of attributes to satisfy common objects. Alternatively, if A is a set of objects and A ⊆ G, regard A as the intent of A (A is a set of attributes) and it satisfies common objects for all A. The definition is: A :={m ∈ M|(g, m) ∈ I for all g ∈ A}. Once the context has been contributed, it can be translated to the concept lattice based on the contextual information. The concept lattice contains information describing the inheritance relations ( ) between the concepts. If one chooses two concepts (E1, I1) and (E2, I2) from the context, one can find the inheritance relation (E1, I1) (E2, I2), (E1, I1), inherited from (E2, I2). Generally, such a highlevel concept (E2, I2) is called a super-concept, and the low-level concept (E1, I1) is called sub-concept. Moreover, the top node of formal concept is the maximum node (labeled with ‘ ’ and grouped from all objects) and the foot of the formal concept is the minimum node (labeled with ‘⊥’ and grouped all attributes). The purpose of a concept lattice is to determine the hierarchical structure and to visualize independent formal concepts. If an attribute has more objects, it belongs to the higher level. The example in Table 1 and Fig. 3 illustrates the technique of FCA below. In Table 1, we use a symbol (×) to represent the relationships between the attributes and objects. For example, D1 has attributes Football game and Soccer. In the Fig. 3, the larger the formal concept, the greater the number of objects in it, and the numbers and percentages are the documents numbers and the rate of all documents. Furthermore, the maximum node ( ) groups all five objects; the minimum node (⊥) respects all attributes and does not have any common object. If an attribute refers to more relevant objects, it will be located at higher level of the formal concept. For example, the concept of Football game is a super-concept to the concept of soccer; on the other hand, the concept of soccer is a sub-concept of the concept of Football game. 2.2.3. Fuzzy Formal Concept Analysis (FFCA) Quan et al. [29,30] proposed the FFCA technique which combines the fuzzy theory and FCA. The major divergence between FFCA and FCA is that FFCA has degrees of membership values between attributes and objects. The FFCA has three elements (G, M, I =  ̨(G, M)) of the context, where G, M and I are equal to the definition of FCA, but the value of I is a membership degree which represents the degree of relevance among attributes and objects. For example, the membership value of an FFCA-context is listed in Table 2. Let  ̨-cut be 0.6; which means a matrix value greater than 0.6 possesses more significance and retains it, as shown in Table 3. FFCA possesses the information of membership value. By setting up a  ̨-cut, one can remove the irrelevant formal concept. Each formal concept includes a membership value between each object, and this information can classify each formal concept flexibly and correctly. Moreover, one also can use the values to calculate the similarity between formal concepts. Fig. 4 shows the hierarchical relationship of concepts. 3. Ontology merging based on WordNet and FFCA techniques The flowchart of FFCA-Merge is indicated in Fig. 5. First, there are two extant domain ontologies, base ontology and revision ontology. The base ontology will not be changed so we select the manual ontology as the base ontology. In the other side, the revision ontology is used to add its new concepts to the base ontology resulting in a new ontology. Both use the pre-process operation to extract their concepts. Next, the ontologies are merged, using: class name merging, WordNet alignment, and FFCA alignment. Finally, the system will generate a new fuzzy ontology. 3.1. Pre-processing Pre-processing includes two processes. (1) Enrich each concept from the source ontology: In this process, the system will store the information obtained from WordNet, such as synsets and definitions, and reuse this information to compare the base ontology concepts to the revision ontology concepts. The enrichment step will enable WordNet mapping automatically. (2) Perform word stemming: According to the SIL International (initially known as the Summer Institute of Linguistics), the definition of stem is the root or roots of a word, together with any derivational affixes to which inflectional affixes are added. In other words, we consider that all of inflected variants of a word represent the same concept. For example, the stem of the verb ‘test’ includes all inflected variants, such as ‘tests’, ‘tested’ and ‘testing’. For word stemming, various algorithms exist and can be applied, such as the Paice/Husk algorithm [46], Porter algorithm [47], Lovins algorithm [48], Dawson algorithm [49], and Krovetz algorithm [50]. The purpose of this process is to find the original form of word, and takes inflected variants into account once the FFCA mapping process is complete. However, our method is not the same with them. We use a vocabulary to identify all the changes. For example, a word “test”, the system will identify all the changes: test, tests, tested and testing. These words appear in our calculation of frequency, are satisfied to the same regulatory concepts, such as shown in Fig. 6. If we only converted to the original root, it may cause some confusion, due to different meaning of words may have the same roots. In this paper, we write a program to find the original form of word and its inflected variants by ourselves. Fig. 6 shows the framework of pre-processing and lists an example once the pre-processing has been done. In Fig. 6, the pre-processing step consists of stemming and enriching. On the right side of Fig. 6, the contents include the slot names of “stem and inflected variants,” “synsets,” and “definition,” that have been appended to each concept. 3.2. Ontology merge In this phase, ontology merging is described. The ontology merging has three steps: class name merging, WordNet alignment, and FFCA alignment. This method is a top-down design of merging ontology in which mapping proceeds from the root node to leaves nodes. This approach can decrease the complexity of the merge process. Furthermore, two ontologies, structure and content are unequal, even if they are in the same domain. The selection of the base ontology in the merging operations will affect the merging results, thus the system must choose an ontology as a base, and merge the other ontology. On this premise, the system will not change the base ontology. Then, the system inserts the concepts of the revision ontology into the base ontology. The first step of class name merging is to determine the concepts having identical names and to merge them into a single coherent concept. The purpose of WordNet alignment and FFCA alignment is to determine the locations of the concepts in the base domain ontology and to insert the concepts into the appropriate positions. 3.2.1. Class name merging After enriching each concept using WordNet, the base ontology and revision ontology will possess concept names merging automatically. The class name merging is to merge the concepts of base and revision ontology with equal concepts. According to WordNet, we define an equal concept as follows: Definition 1. Equal(ci1 , cj2 ):= Equal concept {(ci1 , cj2 )|ci1 ∈ O1 ∧ cj2 ∈ O2 ∧ O1 = O2 ∧ / ((ClassName(ci1 ) = ClassName(cj2 )) ∨ (Synsets(ci1 ) ∩Synsets(cj2 ) = ))∧ / (Definition(ci1 ) = Definition(cj2 ))}. The O1 and O2 are two different domain ontologies; ci1 and cj2 are concepts of O1 and O2 , respectively. O1 is the base ontology. O2 is the revision ontology. ClassName(c), Synsets(c) and Definition(c) indicate the class name, synsets, and the definition of concept c, respectively. Definition 1 defines the concepts between base ontology and revision ontology having equivalent class names or overlapping synsets. For example, if a pair of concepts “test” and “tryout” has overlapping synsets and own equivalent definitions that satisfy the equal concept definition, the system will merge them. 3.2.2. WordNet alignment In this step, the remainders of concepts of revision ontology will be used to search WordNet to determine the hierarchical relations in the base ontology’s concepts. There are three definitions that explain the relations of hyponym, hypemym, and sibling. Definition 2. Hyponym concept Hyponym(ci1 , cj2 ):= {(ci1 , cj2 )|ci1 ∈ O1 ∧ cj2 ∈ O2 ∧O1 = O2 ∧ ((cj2 / Definition 3. ci1 ) ∈ W )}. Hypernym concept / Hypernym(ci1 , cj2 ):= {(ci1 , cj2 )|ci1 ∈ O1 ∧ cj2 ∈ O2 ∧ O1 = O2 ∧((ci1 Definition 4. Sibling(ci1 , cj2 ):= cj2 ) ∈ W )}. Sibling concept {(ci1 , cj2 )|ci1 ∈ O1 ∧ cj2 ∈ O2 ∧ O1 = O2 ∧ (SisterTerm(ci1 , cj2 ))∨ / (SuperConcept(ci1 , d) ∩ SuperConcept(cj2 , d) = )}. / The symbol “ ”stands for inheritance such as ci1 cj2 is c1 inherited from c2 . W is the contents of the WordNet structure. SisterTerm(ci1 , cj2 ) indicates ci1 has a sister term cj2 . In other words, they have equivalent super-concepts in WordNet, and the SuperConcept(ci1 , d) is the distance d from super-concept ci1 , in WordNet, to super-concept ci1 in base ontology. Definitions 2 and 3 indicate the relations of hyponym and hypernym in WordNet contents. Comparing the concepts by WordNet, the concept is mapped to the suitable position of the base ontology. The hyponym relation will become the sub-concept of the base ontology and the hypernym relation will become the super-concept of the base ontology. For example, a pair of concepts “inquiry” and “experiment” has a hyponymic relation in WordNet, so the “inquiry” is the concept from the preferred ontology; the “experiment” will become the sub-concept of inquiry. On the other side, “inquiry” and “problem solving” have a hypernymic relationship, the “problem solving” must be a super-concept of “inquiry”. Definition 4 has two situations, one is that a pair of concepts have sister term relation in WordNet, and the other is that a pair of concepts have common super-concept over more than one level. The difference between them is shown in Fig. 7. The concept c1 is extracted from the base ontology and the concept c2 is extracted from revision ontology. In Fig. 7, Part 1 and Part 2 are the concept c1 and concept c2 relationships in the WordNet structure. Part 1 indicates that the concepts c1 and c2 have a sisterterm relationship in WordNet; Part 2 indicates c1 and c2 are not sister terms; they still satisfy the sibling concept because they have a common super-concept on the upper level; in this case, the distance of super-concept d is 3. Furthermore, Fig. 8 shows an example of class-name merging and WordNet alignment; finally, the system generates a new ontology. 3.2.3. FFCA alignment WordNet is an upper ontology that possesses the domain topic schema but does not have the detailed contents. Domain ontology should include complete knowledge and may possess specific terms regarding this domain that even WordNet lacks. Therefore, the FFCA alignment will be used to overcomethat concepts would not be found in the WordNet. The overview of FFCA alignment is shown in Fig. 9. First, the system uses the base ontology and the revision ontology to construct the context tables of FCA. In the context table, the attributes are the concepts of the ontologies, and the objects are documents extracted from the web pages. The two context tables can be merged into a new context table. Then, the new context table is translated into a concept lattice. After class-name merging and WordNet alignment, most of concept relations can be located. The remainders of the concepts which cannot be merged in the revision ontology must be mapped to the base ontology. For example, in Fig. 9, if context table A is a base ontology after merging and alignment and context table B is a revision ontology that possesses only the remainders of concepts, context tables A and B can be unified as context table A. Next, the fuzzy membership values of context table have a token account of TF (term frequency) and TF-IDF (term frequency-inverse document frequency), listed in formulas (1) and (2), which is used to determine the degrees of relevance between the objects and the attributes. The function of frej (ai ) is the term frequency of attribute ai in the document j. TFj (ai ) is the TF value of attribute ai in the document j. N(Di ) is the number of all documents and N(ai ) is the number of attributes ai appearing in all documents. The fuzzy membership value can be calculated by formula (1) or (2). In other words, the greater the number of the concepts in documents, the more important these concepts are. According to the fuzzy membership value of cutoff threshold, the concept having lower related value should be removed. A suitable threshold can be found based on experimental results. Let BCm,k be the concept m on the level k of the base ontology. RC indicates one of remainder of concepts in revision ontology, and n is the summation of the number sub-concepts and the concept m in the base ontology. k is the index of the concept which belongs to the sub-tree of concept m. Based on the fuzzy operation, which union operation selects maximum value and intersection operation select minimum value, calculating the similarity is listed in formula (3): Similarity(BCm , RC) = Moreover, we have two strategies to map revision ontology with base ontology. (1) The lattice calculation strategy: According to the top-down design of merging ontology, the remainders of concepts of revision ontology only must determine the maximum similarity of top-level concepts based on formula (3) and translate all similar concept pairs into the concept lattice to determine the exact mapping position. The advantage of this strategy can mitigate the influence of noise documents. (2) The direct mapping strategy: This strategy calculates the similarity of remainders of concepts of revision ontology and the concepts of base ontology, and regards the maximum similarity as exact mapping. The advantage of this strategy is that it decreases the complexity of computation. Furthermore, we compare the two strategies in our experiments. For example, we utilize formula (3) to calculate out the similarity of formal concepts, as shown in Fig. 10. These calculations of the similarity between the concepts in base ontology and the remainders of concepts using the fuzzy membership values in the context list are as follows: In Fig. 10, the system how to calculate the similarity value between “Brazilian Jujitsu” and “Football game” is described as follows. Due to the node “Football game” has two children. So the value of ‘n’ is 3 based on formula (3); the union operations between “Brazilian Jujitsu” and “Football game” are to find the maximum values, so the denominator is the summation of {0.9 + 0.8 + 0.7 + 0.35}. In addition, the operation of intersection is to find the minimum value. The node Brazilian Jujitsu has only one intersection document D3 with Football game, so its value is min{0.7, 0.15} which the numerator is equal to 0.15. However, it has not intersection with nodes “Soccer” and “American football”. So their values are zeros. After summation the three terms, the system get the value of 0.01 and so did “Brazilian Jujitsu” and “Martial art” nodes values were found. Brazilian Jujitsu (RC) is most similar to Martial art. The Concept Explorer software was obtained from FCA homepage is used to translate those concepts into a concept lattice [51]. Concept Explore provides a framework to construct and to translate the concept lattice. We select the maximum similarity of concept pairs between base ontology and revision ontology to translate concepts into the concept lattice. As many concepts will be translated into the concept lattice, the lattice will be very complex and hard to understand. If too many concepts must be translated, the order of translation must be adopted level-by-level to avoid the confusion of too many documents and concepts. Our experiments account for this possibility. The concept lattice can be used to determine the hierarchical structure of concepts merged from base ontology and revision ontology. For example the lattice result indicates that Brazilian Jujitsu will be inserted under Martial art, and has a sibling relation to Judo. If Brazilian Jujitsu is aligned under Judo and Judo has too many sub-concepts, Brazilian Jujitsu will be translated to the concept lattice on the next level until the useful information cannot be found, that is done level-by-level. The maximum similarity of concepts of Martial art, Judo, and Brazilian Jujitsu are shown in Fig. 11. 3.2.4. Generation fuzzy ontology An ontology has elements: concept, attribute, operator, instance, relation, and axiom. Our fuzzy ontology has membership values between the concept and documents. Furthermore, there is information concerning membership values between superconcepts and sub-concepts called sub-sethood. For example, the concept Football game contains attributes such as events, rules, competition time, and competition date. The membership value consists of documents, such as, D1(0.9) and D2(0.8); the numbers represent the membership values in each document. Utilizing this membership value between documents can be calculated with membership values among the sub-concepts, such as basketball. In addition, the relations between concepts can be defined as various types such as Belong-to, Consist-of, and Instance-of; and the subsethood defines the related value between concepts. The structure is not altered to carry out the merging procedure, so the abovementioned attribute, operator, instance, relation, and axiom will be retained totally. After merging the concept layer, the membership values can be directly added to base ontology. Moreover, each concept has the membership values to each relevance document, and the relationship between each concept also possesses the fuzzy information in fuzzy ontology. According to the fuzzy concept lattices, the system can build up the concept hierarchy. The membership values between superconcepts and sub-concepts can be calculated by formula (4) [30,32]: subsethood(Subconcept, Superconcept) = |Subconcept ∩ Superconcept| |Superconcept| (4) According to Figs. 10 and 11, the Super-concept Martial art (C1) and the Sub-concept Judo (C2) will be calculated as follows: subsethood(C1, C2) = 0.6 + 0.7 = 0.49 0.85 + 0.85 + 0.95 After calculating the sub-sethood, the fuzzy ontology can be constructed; it has more flexibility to deal with uncertain information. The complete information of each fuzzy concept is shown in Fig. 12. The attributes generally describe the concept characteristics and content, and the operations are used for controlling or operating the concept. When the attributes and operations are added into the class layer, the instances must link with the class layer, such as an umpire in a football game, famous athletes and stadiums of a competitive sport, etc. Moreover, the relationship-type between concept and instance are regarded as Instance-of. Fig. 13 shows a complete fuzzy ontology that contains four layers: domain layer, category layer, concept layer, and instance layer. 4. Experiments and discussions In the section, two different domains of ontologies are utilized to evaluate the ontology merging and to generate a fuzzy ontology. The experimental environment and data source are introduced first. Next, some factors affected the system are tested which include number of documents, mapping strategy, and different domains. Finally, the experimental results of the two integration tasks are shown. 4.1. Experimental environment and data sources In this paper, we perform two experiments. One is a baseball domain and the other is an academic domain. There are two ontologies in baseball domain ontologies, one is refer to from the DAML ontology library [52] which the number of concepts is 98 and the other refer to our previously published paper [45]. In Ref. [44], we collected domain-related web pages from the Internet. Then, we use the HTML tag labels to choose meaningful terms from the web pages. Next, we use these terms to construct the domain ontology by calculating a TF-IDF to find the weight of terms, using a recursive ART network (Adaptive Resonance Theory Network) to cluster terms. Each group of terms will find a candidate keyword for ontology construction. Boolean operations locate individual keywords in a hierarchy. In reference [45], we used the labels of the HTML tags to select keywords, and then used WordNet to determine the meaningful keywords. Next, calculate the Entropy value to determine the weight of the terms. Projective Adaptive Resonance Theory (PART) Neural Network was used to cluster web pages and found the representative term of each cluster of web pages using the Entropy value. The system then uses a Bayesian network to insert the terms and complete the hierarchy of the ontology. We have used method [45] to construct baseball ontology automatically where the numbers of concepts is 75. There are many differences between the two ontologies. The former is construed by experts on the DAML ontology library and the latter is created automatically by a projective PART neural network [45]. The domain ontology from the DAML ontology library was verified by experts, so we take it as a base ontology and our automatic generation ontology as revision ontology. For academic domain ontologies, both two domain ontologies were extracted from the DAML ontology library. The ontologies were named academic department and academic position. The numbers of concepts are 45 and 30. The details of the ontologies are shown in Tables 4 and 5. In the experiment, we focus on the FFCA alignment step since most clear concepts in the revision ontology have been merged after class-name merging and WordNet alignment. The baseball and academic domain ontologies have obvious differences. (1) The baseball domain ontologies include concept names composed of a single word, so it is clear and can be explicitly finished by WordNet. Only the concept “baseball ground” and its sub-concepts are not mapped to the base ontology. (2) The concepts of academic domain ontologies have multiple words that cannot be located within WordNet. So the academic domain ontologies have many concepts necessarily finished at the FFCA alignment step. Table 6 shows the scale of the domain ontologies. Moreover, the mapping results were based on the WordNet contents which could deal with most problems of merging and alignment when the concept names were included with explicit words and composed with single words. Table 7 shows the number of concepts necessarily merged or aligned. In FFCA alignment step, we collect 3243 and 1524 related web pages from Google using “baseball” and “academic” as search keywords, respectively. (1) In the baseball domain ontologies, according to FFCA technique is an extension strategy to find the relations between concepts which greatly relied on the quality of documents, and the web pages related to the baseball domain contain some noise data, so we must filter them. The filter method includes following steps: First, set a multiple of two to increase the number of documents progressively until the accuracy is exceeded a threshold. Next, the interval of the final input documents between last input documents will be retained and filter other documents. In the experiments, 1600 web pages noise documents are filtered out. Thus the system filtered them and retained 1643 web pages. (2) In the academic domain ontologies, all web pages are retained. Each concept calculates its fuzzy membership values by the context table. The threshold is set 0 to 1 by step 0.1 to test the system. Formula (5), Formula (6), and Formula (7) are the definition of pre4.2. The influence factors for ontology merge In this section, three elements of this experiment are discussed: influence of number of documents for generated ontology, influence of direct mapping strategy and lattice calculation strategy for generated ontology, and influence of different domains. The number of documents is an important feature of information retrieval because it may improve or worsen the experimental result. We will compare the direct mapping and lattice calculation strategies influence for our system and use two different domains to test our system as follows. 4.2.1. Influence of number of documents In the case of the baseball ontology, while the number of documents is increased, it is not certain to improve accuracy because the web pages in the baseball domain contain much noise data. The result is shown in Fig. 14. Baseball is a popular topic, and a web page may contain many terms concerning baseball even though the article has no relation to the sport. Therefore, the unrelated terms in the web pages of baseball domain have a high frequency, which affects experimental results. Thus, the quantity of web pages is not crucial to improving performance in this domain. Moreover, our proposed algorithm is applied to filtering unrelated documents. Thus quality is more important than quantity. Nevertheless, in the case of academic department ontology, accuracy is very stable even if the documents are increased. The result is shown in Fig. 15. This can be partially explained by the fact that the terms relating to academic departments are specific; web pages containing such terms also usually contain related information. The terms appearing in the web pages are regular because the terms in this topic are more specific than those of the baseball topic. 4.2.2. Influence of direct mapping and lattice calculation strategies We proposed two strategies which we compare and discuss. As can be seen in our experimental case, the results of two-domain ontology depend on direct mapping and lattice calculation strategies, as shown in Figs. 16 and 17. In these figures, direct mapping strategy is named DMS and lattice calculation strategy is named LCS. According to the experimental results, the accuracy of LCS strategy is better than that of the DMS strategy. This is because the LCS strategy can avoid the influence of many confused terms, and determining suitable mapping depends on the structural information. In Fig. 17, the academic department results are very obvious, DMS strategy could not determine any correct mapping in this case. As such, the merge tasks assume that all synsets can be found in the class-name merge step, but actually the specific synsets cannot be determined by the WordNet. Moreover, DMS strategy cannot solve this problem because the DMS strategy only aligns the concepts even though they are synsets; but the LCS strategy can identify synsets even though they contain specific terms not extant in the WordNet. Therefore, the mapping steps necessarily apply the concept lattice to support our method. 4.2.3. Influence of different domains The baseball ontology includes many of unrelated documents that must be filtered, but the concepts of the baseball ontology are general terms extant in the WordNet. Therefore, most of the integration tasks can be finished in the class-name merge and WordNet alignment steps. The better results in the baseball ontology is that the class-name merge and WordNet alignment steps can guarantee the integration tasks are correct, so the experiment can obtain a good result. The academic ontology has many related documents, but many specific terms cannot be found out in the WordNet. Therefore many tasks must be finished by the FFCA alignment step. FFCA technique adopts the extension strategy to examine useful information from documents; so the result relies on the quality of documents, which is not stable enough to obtain good accuracy. Therefore, in the experimental results, the academic department ontology has less accuracy than the baseball ontology, as discussed in the next section. 4.3. Experimental results Figs. 18 and 19 show the experimental results of F-measure on the baseball domain and the academic domain, respectively. In the baseball domain, the F-measure value is 1 or is 0 because this case has only one super-concept “baseball ground” that must be mapped by FFCA alignment. From the experimental results, the FFCA alignment step using TF formula sets up the membership value at only 0.1 so that system can determine the exact mapping. The membership value of TF-IDF formula is set at 0.2 to 1 so that the system can determine the exact mapping. For the academic domain, the F-measure values of TF-IDF exceed the TF formula. The suitable fuzzy  ̨-cut is 0.6. According to the two-domain ontology experiment, the measurement of TF-IDF is better than TF to avoid the unambiguous words. Moreover, in the revision academic ontology has two concepts, “staff” and “teaching faculty”, and they have many sub-concepts needing to be translated into the concept lattices. So, the order of translation must adopt level-by-level to translate into concept lattices for information mining to find the appropriate position in the base ontology. The domain concept academic we regard as level 0 and the concept of person is on level 1 by the same token. Both staff and teaching faculty are aligned to the concept of person and they are below level 1; therefore the order begins on level 2 and it will translated to the next level until the useful information cannot be found. The result of baseball ontology is shown in Fig. 20. The results of the academic department ontology which adopt level-by-level for information mining are shown in Figs. 21–24. After producing the ontology, we then invited five domain users who had at least 1 year of experience relevant to the domain to evaluate our ontology results. Two forms of precision, Concept Precision (C P), and Concept Location Precision (C L P), are used to evaluate our ontology. We defined the two kinds of precision evaluation methods in Table 8 [6]. Formula (8) defines concept precision, which demonstrates the precision of the concepts the system generates. Formula (9) gives concept location precision, which not only demonstrates the precision of the generated concepts but also shows the precision of the location in the hierarchy relations. We have identified and checked by five users to decide the merge accuracy the baseball ontology is 92% and academic department ontology is 84%. The results are shown in Table 9. In Figs. 21 and 22, the staff is determined to be a sibling with worker and student on level 2, and staff and faculty were found to be siblings, as well as Administrative staff and AdministrativeStaff on level 3. In Fig. 23, teaching faculty is determined to be siblings with worker and student on level 2, but had no relations on level 3. Through concept lattice, not only conceptual structure could be determined but also the relationships between concepts could be defined. The relationship types of synsets, hypernym, hyponym, and sibling also could be determined at the FFCA alignment step. Next, all of the relationships between concepts in the two domain ontologies are determined by WordNet or FFCA techniques and stored in the final ontology. The relationships are shown in Table 10. 5. Conclusions and future works This paper proposes a novel ontology merging method based on the WordNet and FFCA techniques. The processing steps include pre-processing, class name merging, WordNet alignment, FFCA alignment, and generating a fuzzy ontology. One ontology is a base ontology and the other is the revision ontology. The system uses the revision ontology to update the base ontology. From the experimental results, we can find many concepts located within WordNet. Using WordNet to align ontology structure can yield good results. Finally, according to the FFCA-Merge information, the fuzzy ontology can be generated. The framework of this method can create a single coherent ontology of a high standard, and construct a more flexible fuzzy ontology. The experimental results indicate our method can merge domain ontologies effectively. In the future, other features such as attributes, operations, or axioms should be integrated into our system. The other features also have conflicts which must be solved. The WordNet still includes much detailed information which can be utilized, such as Antonym, Member holonym, Substance holonym, Part holonym, Member meronym, Substance meronym and Part meronym. Reuse of these factors can determine more relationships among elements,. In the FFCA step, we collect web pages as documents from the Internet. The information contained in web pages is rough. If we can reuse or redefine the contents of documents from reliable sources such as domain lexicons and data warehouses, the FFCA step will obtain better results. Furthermore, the importance of each term in a given document is different, so we can use relevant information derived from HTML, XML tags, and user behavior such as the frequency of clicking, copying, and cutting to set-up weight values automatically. In addition, we also will apply the method to a larger scale domain ontology. Finally, we will apply our system to different domain ontologies to test its performance. Acknowledgements The authors would like to thank the anonymous reviewers for their constructive and useful comments to improve the quality of the paper and also would like to thank the support sponsored by National Science Council, Taiwan, with number: NSC 97-2221-E324-033. 
Supporting domain experts to construct conceptual ontologies: A holistic approach. A recent trend in ontology engineering research aims at encouraging the active participation of domain experts in the ontology creation process. Ontology construction methodologies together with appropriate tools and technologies, such as controlled natural languages, semantic wikis, intelligent user interfaces and social computing, are being proposed to enable the direct input from domain experts and to minimize the dependency on knowledge engineers at every step of ontology development. The time is ripe for consolidating methodological and technological advancements to create intuitive ontology engineering tools which can make Semantic Web technologies usable by a wide variety of people without formal knowledge engineering skills. A novel, holistic approach to facilitate the involvements of domain experts in the ontology authoring process is presented here. It integrates (i) an ontology construction methodology, (ii) the use of a controlled natural language, and (iii) appropriate tool support. The integrated approach is illustrated with the design, implementation and evaluation of ROO – a unique ontology authoring tool which combines intelligent techniques to assist domain experts in constructing ontologies. The benefits and limitations of the proposed approach are analyzed based on user studies with ROO. A broader discussion is provided pointing at issues to be taken into account when assisting the involvement of domain experts in ontology construction. 1. Introduction Public organizations and businesses hold rich data sets whose large scale integration and sharing can be enabled with Semantic Web (SW) technologies. This requires ontology-based architectures, and includes an important stage dedicated to the development of ontologies, ranging from small domain ontologies to large ontologies linked to legacy datasets [1,12,14]. The time and effort required to create ontological structures is one of the major reasons for the reluctance of large organizations and businesses to utilize SW technologies [1,34]. This is aggravated by the fact that most ontology construction tools are designed to be used by specialists with appropriate knowledge engineering skills but who may lack the necessary domain expertise to create the relevant ontologies [20]. Finding knowledge engineers competent in the specific domain is a luxury; the most common case is to ask domain experts to provide relevant knowledge sources, or apply knowledge elicitation techniques to discover information directly from the expert, while knowledge engineers encode the ontology using available SW tools. Apart from creating an extra layer of bureaucracy in the development cycle [34], this approach can hinder the ontology construction process and may have a negative impact on the quality of the resultant ontology (e.g. poor documentation, inconsistency of terminology used, incorrect or incomplete knowledge constructs). However, if best practice for ontology construction in large organisations which have successfully adopted SW technologies is exploited, a step change in the wider deployment of the SW may be achieved. An example of such an approach – drawn upon extensive experience in creating topographic ontologies at Ordnance Survey, the national mapping agency for Great Britain [29] – is described here. The Ordnance Survey is developing a semantic reference system that includes several foundational domain ontologies and empowers the integration of heterogeneous topographic data and their reuse by third parties [14]. At the heart of ontology development at Ordnance Survey is the active involvement of domain experts (e.g. geographers, ecologists, emergency planners) [36]. They construct the conceptual form of an ontology, hereafter called a conceptual ontology, which records domain knowledge in an abstract way which is human understandable and independent from the logical formalism used at machine level. The conceptual ontology is translated to a machine-interpretable logical form, represented with appropriate logical formalisms, e.g. description logic [24,27]. The distinct characteristic of the proposed ontology engineering approach is that the development of the logical form of the ontology is not just rooted in, but seamlessly integrated with the construction of the conceptual ontology. This strengthens the connection between domain expertise and knowledge engineering, giving domain experts a direct and more prominent role in ontology authoring. This paper presents a systematic approach for supporting domain experts to construct the conceptual form of an ontology by incorporating three essential aspects: (i) a tailored methodology, called Kanga, for involving domain experts in the ontology authoring process; (ii) a controlled natural language (CNL), called Rabbit, to enable knowledge encoding in a human understandable form; and (iii) a user-friendly tool, called ROO, to assist the authoring of a conceptual ontology in Rabbit and to convert it to a logical form. Details of Kanga, Rabbit and ROO, presenting how the approach was derived and describing specific aspects of it have been published in [13,26,28,36]. This journal paper connects all aspects in a coherent integrated framework which shows how the different components fit together, forming a holistic approach for involving and supporting domain experts in ontology authoring. The novelty of this paper is the presentation of a bird’s eye view of the approach, accompanied by a comprehensive description of how the different components are linked and complement one another; crucially, this enables us to assess the capabilities and limitations of the overall approach, as opposed to the evaluations of separate aspects in previous publications. Furthermore, the paper examines and reports the application of the holistic approach, and facilitates its utilization for making SW technologies usable by people without formal knowledge engineering skills. The work presented in this paper addresses an important problem (i.e. finding intuitive ways for domain experts to engage in knowledge engineering) for the realization of the SW, and makes the following contributions: - proposing a way to complement controlled natural language interaction with an appropriate ontology methodology to develop innovative tools for ontology authoring suitable for users who lack knowledge engineering background; - illustrating how intelligent support can be provided to facilitate domain experts’ involvement in ontology authoring by both lessening their effort in defining ontology constructs and improving their understanding of the knowledge engineering process; - identifying benefits and limitations of the approach, based on user studies; - drawing implications for the design of tools to support the involvement of domain experts in ontology authoring. The next section reviews related work and positions our research in the relevant literature. We outline ontology methodologies which stress the importance of involving domain experts in ontology engineering, review existing approaches and tools to facilitate this involvement, and compare our work with relevant controlled natural language-based approaches. Section 3 presents our approach to facilitate domain experts’ involvement in ontology construction. This provides the reader with an overview of the Ordnance Survey’s methodology for engaging domain experts in ontology authoring; introduces the Rabbit controlled natural language; and presents the ROO tool for authoring OWL ontologies using Rabbit. We then illustrate the overall approach with example user interactions following scenarios from ontology authoring tasks at Ordnance Survey. Section 5 discusses the benefits and limitations of the approach based on a user study in Geographical domains (Hydrology and Water Pollution), as well as feedback from the current usage of the ROO tool. We draw the reader’s attention to broader aspects related to supporting domain experts’ involvement in ontology authoring. The paper concludes by summarizing the main findings of our work and pointing at future research directions. 2. Related work 2.1. The role of domain experts in ontology construction methodologies The active involvement of domain experts in the ontology construction process is one of the main goals of the approach presented in this paper. Promoting this involvement has been an important trend in recent research efforts in Ontology Engineering. Our work was inspired by two methodologies which suggest possible ways to achieve the involvement of domain experts in ontology engineering. The DynamOnt project [25] pointed out that existing methodologies did not support domain experts because they lacked appropriate support for communities and collaboration. The project aimed at producing a methodology where domain experts could create lightweight ontological models that could be used as part of an ‘‘evolving conceptual model’’. To achieve this, DynamOnt proposed to reuse existing methodologies (Uschold and King [52] and CommonKADS [43]) while (i) adding guidance for domain experts so they can act as knowledge engineers; (ii) encouraging collaboration; and (iii) grounding the resultant ontologies using foundational ontologies such as DOLCE [22]. Similar to DynamOnt, the HCOME methodology [35] argued that traditional ontology construction methodologies such as METHONTOLOGY [16], Uschold and King [52], On-to-Knowledge [47], rely too much on the knowledge engineer for development, maintenance and evolution of ontologies and minimize the role of the domain experts. The HCOME methodology proposes to support individual domain experts by enabling them to collaborate in the construction of ontologies with a community of knowledge workers. DynamOnt and HCOME have both pointed out limitations of traditional ontology construction methodologies and suggested involving domain experts by (i) considering ontology construction as a joint process involving both domain experts and knowledge engineers and (ii) providing domain experts with suitable guidance to ensure their active involvement in ontology authoring. In practice, it is very difficult to achieve effective domain experts’ involvement without appropriate tool support. The approach presented here embarks on this challenge and illustrates a systematic way to design an ontology authoring tool geared towards domain experts by integrating an appropriate ontology methodology in the design of the tool. In the approach described in this paper we use the Kanga methodology for ontology construction. Kanga has been derived empirically, based on experiences at Ordnance Survey when building several ontologies in the topographical domain. Kanga adds to the existing ontology methodologies focusing on domain experts’ involvement by clearly identifying the assumptions about domain experts and distinguishing the phases where domain experts or knowledge engineers should be involved. Most other methodologies also explicitly include the domain expert. Where Kanga differs is in the emphasis it places on the domain expert and the central role that the expert plays. Kanga requires the domain expert to take the lead role, guided by the knowledge engineer but nevertheless in control. So where Kanga differs in not in that it involves the domain expert where others do not, but rather the degree to which it involves them. Additionally, Kanga does not sacrifice the expressivity of the resultant ontologies: it describes how domain experts can be involved in the construction of highly expressive and interconnected ontologies by using a Controlled Natural Language interface. Thus Kanga has adapted best practice to place greater emphasis on the domain expert and therefore the novelty in Kanga results principally from this shift of emphasis. An overview of Kanga is given in Section 3.1. 2.2. Relevant approaches and tools Several approaches and tools to involve domain experts in the ontology construction process have been proposed in recent years. We discuss these approaches here and position our research with respect to the relevant work. Ontology Engineering tools that improve collaboration focus on supporting a community of people (including domain experts) to build ontologies, e.g. HCONE [35] and Web Protégé [50]. These tools provide communication and Web 2.0 techniques – such as discussion forums – to aid users to propose, document and implement changes to the ontology. The main advantage of this approach is that it encourages the formation of a community of both domain experts and knowledge engineers to collaborate in building the ontology. These tools improve the communication between domain experts and knowledge engineers, which may motivate domain experts to provide more input into the ontology construction process. However, the means to edit the ontology are similar to traditional tools, e.g. Protégé, which makes domain experts heavily dependent on knowledge engineers to formalize the ontologies. Recent studies explore customised interfaces that domain experts can be comfortable with (predefined forms or Excel sheets) and can be converted into OWL [42,49]. This clearly has potential for facilitating domain experts’ involvement in ontology engineering, however their participation is currently restricted to discussions and the population of ontologies with specific instances and subclasses, without being directly involved in the addition of new formal definitions. Hence, the ontology constructs are actually composed by a group of knowledge engineers (who may or may not be domain experts), while the domain experts without knowledge engineering experience mainly provide the knowledge sources and are involved in the verification of the ontology. Further experimental studies are needed to examine the effectiveness and wider applicability of this approach. Semantic Wikis [23] are extensions allowing the wiki manager to define a broad ontology structure that corresponds to wiki pages. Users then refine the ontology by editing and semantically tagging wiki pages. The wiki interface hides the ontology formalisms from the users, in this case domain experts, who can add information to the ontology model by editing wiki pages. Note that to make the interaction intuitive, an initial ontology needs to be created with input from both domain experts and knowledge engineers (e.g. to create semantic forms in Semantic Media Wiki). Semantic wikis offer a flexible approach for lightweight ontology engineering. However, they are inappropriate for heavy weight ontology engineering which requires more expressive logical formalisms, such as description logic and OWL. Ontology Maturing [2,5] aims to reuse semi-structured data produced by knowledge workers, such as emails, tags and existing schemas and classifications, to produce lightweight ontologies and eventually heavyweight ontologies. This approach looks at ways for users to add formal semantics to existing data one layer at a time. Proposed tools, e.g. SOBOLEO [5], for extracting a lightweight ontology based on a set of tags or existing schema provide intuitive ways for domain experts to encode their knowledge of the existing data. However, this work is still in progress and more research is required to allow domain experts to insert more complex relations following the maturing approach. Further experimental studies are needed to examine how people without knowledge engineering skills contribute to the ontology maturing process. Ontology Understanding aims to make it easier to understand what type of knowledge is represented by a particular ontology, for example, by extracting the main concepts in an ontology [41] or by showing relevant metadata [30]. Ontology Visualizations are also commonly used to show and gain insights into the structure of ontologies and linked data [40] and to provide visual interfaces for editing ontologies [37]. Domain experts can benefit from these approaches by getting a high-level understanding of existing ontologies that they can reuse or extend. However, the reuse of extension of the ontologies requires domain experts to be able to understand and edit the ontologies at the axiom level, which requires logical modeling skills lacking in many domain experts. Recently, the use of Controlled Natural Language (CNL) interfaces [19] to perform ontology engineering has been explored. Because the approach described here falls in this category, we will review related tools and CNLs in more detail in the next subsection. 2.3. Controlled natural language tools A ‘‘controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-specialists’’ [18]. CNLs that can be used to view, create and edit ontologies are: CLOnE [20], SOS [8], ACE [33], and CPL[7]. CLOnE only supports a small number of OWL constructs – currently, only lightweight ontologies can be built using CLOnE. We are not aware of any tool support for the Sydney OWL Syntax (SOS), although there is some tool support for PENG [46] that forms the basis for SOS. ACE is the most mature CNL having originally been created to translate into First Order Logic. A subset of ACE is now used to drive the ACE View application [32] where the resulting sentences are translated into OWL and SWRL. ACE View was developed at the same time as the tool described in this paper. Although ACE View provides a CNL interface, it still requires users to have some knowledge engineering background, since there is no guidance through the ontology construction process. A comparative evaluation study showed that a CNL interface alone is not enough to properly support domain experts who lack knowledge engineering experience [13]. Another mature CNL is CPL developed at Boeing and used by the HALO project [3,7]. HALO improved over other CNL approaches by providing a more holistic approach: the CNL is provided in conjunction with support of the ontology construction process and not just as a standalone tool for entering knowledge into the system. HALO focuses on query answering and uses a wide variety of techniques such as information extraction to build the ontology. As a result, it is difficult to identify to what extent the CNL-based interaction in HALO facilitates domain experts to abstract the domain and formulate their knowledge in a logical form. Such analysis was possible in our research because the interaction support was focused around the formulation of ontology constructs in a CNL. CNL interfaces have also been proposed for query answering [3,4,6,48]. These languages make it easier for domain experts to evaluate an ontology, as they can pose questions that need to be answered based on the ontology and a set of instances. These languages cannot be used to enter new knowledge into the ontology. They are mainly used for ontology validation, not ontology construction, which is the focus of the approach presented here. In our approach, we use Rabbit [28], which has been designed based on extensive experimental work with domain experts and focuses on usability. Rabbit aims to resemble English in order to feel natural for domain experts and enable them to compose and read Rabbit sentences. At the same time, Rabbit has an expressiveness comparable with OWL 2, as there is a one-to-one correspondence between Rabbit sentences and OWL 2 constructs. This allows for easy conversion and limits the number of allowed Rabbit constructs, making it easier to learn. We refer the reader to [45] for a detailed comparison between ACE, SOS and Rabbit for OWL editing. 2.4. Design decisions based on related work Table 1 shows an overview of the major approaches to involve domain experts in the construction of ontologies. The work presented in this paper promulgates an approach where tool support enables domain experts, with no knowledge engineering experience and without the direct help of knowledge engineers, to build both lightweight and heavyweight ontologies. The table shows that CNL-driven and ontology maturing approaches are good candidates as they do not require ontology engineering experience and are easy to learn. The CNL-driven approach has the added advantage that the domain expert can produce ontologies that use complex OWL constructs, given appropriate tool support. Our research intentionally avoids the collaborative aspects of building ontologies (present in the collaborative and semantic wiki approaches in Table 1) in order to learn how domain experts can create ontologies that represent their knowledge of their domain. Note that the CNL-driven approach presented in this paper is compatible with and could improve collaborative approaches as it allows communities to be less dependent on knowledge engineers and to produce more complex ontologies if necessary. The approach presented in this paper improves on other mature CNL-driven approaches and tools which had the following negative usability issues: requiring some familiarity with the logical representation used [17,19]; assuming that the user has knowledge engineering skills to complete the ontology authoring process (all tools suffered from this to an extent); distracting frequently the user from the main task to enter basic lexicon entries [44]; lack of immediate feedback and error messages [17,20,44]; requiring installation of external libraries and appropriate user customization [20]. Our approach is distinguished from existing CNL approaches for ontology engineering, and makes a contribution to this research, by providing intelligent support to assist domain experts who may lack ontology engineering skills to engage in the ontology development process. Such support, which is based on an ontology construction methodology developed by the Ordnance Survey (this methodology is outlined in Section 3.1), is a vital improvement as it enables the domain experts to assume ownership of the ontology construction (from defining the requirements, to documenting and formalising the ontology), instead of merely providing knowledge sources and verifying the final product. Building on previous work in CNL, we have developed a novel tool that overcomes key usability limitations by integrating several features: a simple version of look ahead to provide suggestions by guessing what constructs the users might enter [46]; show the parsed structure to help the user recognize correct sentence patterns [44]; provide a flexible way to parse English sentences using robust language technologies [20]; automatically translate to OWL [8,20,33]; use templates to facilitate the knowledge entering process [39]; maintain a text-based glossary describing parsed concepts and relationships [44]; and distribute the CNL tool as a Protégé plug-in [32]. Besides integrating these features, our CNL tools improves on existing tools by providing intuitive error messages designed to be understood by domain experts building an ontology (see Section 3.3). 3. Supporting domain experts’ involvement in ontology construction Our main goal is to facilitate the involvement of domain experts in ontology authoring. We define domain experts as people who have highly specialized knowledge in a particular domain. They typically do not have any knowledge engineering experience and are unwilling to learn technical details about OWL or RDF. They are not trained in logic formalisms and do not have background in description logics or programming languages. They are used for using high level concepts and relationships to make sense of their domains, and have a vocabulary that they use to communicate with colleagues. This vocabulary is usually informal and may contain ambiguities or it may not be shared across a large population of domain experts (not standardized). We assume domain experts are used to using popular software packages such as web browsers, office packages and more specialist software packages specific for their own domains, and will be willing to learn a new software package as long as it is intuitive to use and does not require extensive training. In order to enable domain experts to build ontologies with a minimum of training, we propose a holistic approach that combines: - a methodology that puts the domain expert in the leading role of the ontology construction process; - a CNL to make knowledge formalization more intuitive; - a tool tailored to domain experts that offers guidance through the methodology and the input of CNL constructs. This section presents our approach outlining the ontology methodology (Section 3.1), controlled natural language (Section 3.2), and tool to support domain experts’ involvement in ontology editing (Section 3.3). 3.1. Kanga: a methodology for engaging domain experts in ontology construction The Ordnance Survey has developed Kanga, a methodology for authoring ontologies [36] that combines two aspects. The first is the human-readable or conceptual aspect, which is written by one or more experts in the relevant domain. The second component is the computer-parsable or logical aspect, which is created by man- ual or automatic conversion of the conceptual ontology into a SW language such as OWL. Kanga is centered on the domain expert, who, as the human source of the knowledge, should retain control of the ontology and the authoring process. The authors do not claim that Kanga introduces much new in terms of methodological steps or techniques. This was not the point of developing Kanga, rather it was to emphasise the role of the domain expert and the central position that this person is given. Hence the development of Rabbit, which itself is designed to make ontology development and interpretation more accessible to the domain expert. Indeed with simple modifications there is no reason to suppose that ROO could not be adapted to other methodologies, allowing ontol- ogies to be expressed in Rabbit but following a different methodol- ogy such as METHONTOLOGY [16]. Using Kanga, ontology engineering is conducted in several iter- ative steps (see Fig. 1): - The scope, purpose and other requirements of the ontology are identified. - Source knowledge and documents are gathered, including ontologies identified for reuse. - The ontology content is captured in a knowledge glossary. - Core concepts and relationships between concepts are formally defined using structured English sentences and then converted into OWL. - The ontology constructs are verified and validated. The first three steps are performed by domain experts, without involvement of knowledge engineers. The conversion from structured English sentences into OWL in the fourth step and the validation of the ontology of step 5 requires formal knowledge of OWL and ontology engineering and are usually performed by knowledge engineers with some assistance from domain experts. The knowledge glossary consists of lists of core and secondary concepts, along with relationships and any instances of concepts necessary to describe the core concepts. Each concept, relationship and instance is supplied with a natural language description, the source of which is documented. Core concepts are ones which are considered to be central to the domain, for example, in a Hydrology ontology, the concept Waterfall will be considered a core concept, and will be defined using structured English sentences (as described in Section 3.2). Secondary concepts, for example Cliff in a Hydrology ontology, will only be included in the ontology in order to describe core concepts, and will not themselves be further elaborated on – that is, they are not described with sentences of their own. The Ordnance Survey methodology was developed through the process of authoring two fairly large (approximately 600 concepts each) and expressive ðALCOQÞ ontologies within topography, namely Hydrology and Buildings and Places,1 constructed with the active involvement of domain experts from Ordnance Survey. Since domain experts did not have knowledge engineering skills, using the then available ontology construction tools was not appropriate. Consequently, the knowledge glossary, which included lists of concepts and relationships with corresponding textual descriptions from knowledge sources and structured sentence to define relation ships between concepts, was stored in a spreadsheet. The structured sentences were then manually converted to OWL by a team of knowledge engineers. This process is now automated using the controlled natural language and the tool described next. 3.2. Rabbit: a controlled natural language for defining ontology constructs In response to a need for domain experts to be able to understand and define ontological constructs, a CNL was developed [26]. Rabbit is designed to be easy for domain experts to understand and produce, allowing them to express what they need to in order to describe their domain. We have named this language Rabbit, after Rabbit in Winnie the Pooh, who was actually cleverer than Owl. To this end, we have involved domain experts from the outset in the core language design decisions, and modified our original design based both on the experiences we have had with domain experts building topographic ontologies in Rabbit, which are later converted to OWL, and also on experimental results of testing user understanding of Rabbit sentences. The main principles underlying the design of Rabbit are: - To allow the domain expert to express their knowledge as easily and simply as possible and in as much detail as necessary. It is designed to appear like writing statements in natural English. - To have a well defined grammar and be sufficiently formal to enable those aspects that can be expressed as OWL to be systematically translatable. - To recognize that the domain expert alone cannot produce an ontology and that a knowledge engineer is also necessary. - To be used in conjunction with tools which help to enforce an authoring method but not to the point where Rabbit is only readable through tools. - To be domain independent. Table 2 gives an overview of some of the main sentence structures, although Rabbit covers most constructs in OWL 2 [26].2 Ordnance Survey has performed user tests to investigate Rabbit sentence comprehension and ease of authoring without tool support [15,28]. 3.3. ROO: a tool to augment the ontology construction process ROO (Rabbit to OWL Ontology authoring) is a novel ontology construction tool that has been designed specifically to support the involvement of domain experts in the ontology construction process. Hence, the target users are domain experts who have limited or no previous experience in ontology building. However, ROO can also be used by knowledge engineers. ROO compensates for a lack of knowledge about ontology construction, ontology languages and formal logics, by providing users with appropriate support, as specified below. ROO guides the user following an appropriate ontology construction methodology by providing a user interface that reflects the phases of the Kanga methodology where each tab corresponds to a phase in the methodology. For example, when a user creates a new ontology or opens an existing ontology, the first tab is for Purpose and Scope. The interface components encourage entering an annotation for the ontology purpose and a different annotation for the ontology scope. The annotation URIs have already been defined for the user as they are not expected to learn the OWL annotation system. This has the advantage that the encoding of the purpose is standardized by the tool, so it is easy to check whether an ontology has defined its purpose and scope. This is used by ROO s Guide Dog, a component which contains an internal model of the Kanga methodology to check the progress of the user in building the ontology. The user can ask the guide dog for advice regarding building the ontology. When this happens, the guide dog inspects the state of the ontology to determine the current phase in the Kanga methodology and suggests a task to the user that is appropriate for the current phase. The following types of tasks are suggested: scope and purpose definition, knowledge source definition, declaration of concepts and relationships (OWL entities), free text definition of OWL entities, CNL definition of OWL entities. Examples of Guide Dog suggestions are given in Section 4. ROO uses a terminology that encourages the creation of a conceptual model and avoids terminology that is OWL specific. ROO’s interface shows lists of concepts and relationships as opposed to Protégé’s and OWL’s terminology of classes, instances and properties. The rationale behind this is that domain experts are not familiar with OWL and should not be required to learn its terminology. We assume that the terminology of concepts and relationships is easily understood by domain experts as they already conceptualize their domains in order to communicate with colleagues. ROO provides an intuitive way to describe relevant concepts and relationships of the domain by using a CNL interface. We have implemented a parser for the Rabbit CNL, as well as an editor that provides support for writing correct Rabbit sentences by (i) using syntax highlighting to show keywords, concepts and relationships; (ii) providing help files to introduce the Rabbit language, users can learn about Rabbit by learning about the meaning of the Rabbit keywords or by seeing examples of Rabbit sentences; (iii) providing a list of Rabbit sentence patterns; (iv) showing invalid Rabbit sentence error messages explaining errors in terms of missing concepts and relations; (v) detecting ambiguity and showing the user possible alternatives. 3.4. Technical overview of ROO ROO is based on Protégé 4. Protégé 4 is built around a plug-in architecture based on the OSGiTM dynamic module system for Java3 and can be used for viewing and editing OWL ontologies. Due to the plug-in architecture of Protégé, ROO takes advantage of the available third party plug-ins (e.g. for ontology visualisation, reasoning, verification). This extends the functionality and makes ROO attractive not only for domain experts (who use ROO features to author the conceptual ontology) but also for knowledge engineers (who benefit from ROO’s features for creating/changing the conceptual ontology but also have the extensive power of Protégé tools for verifying/ altering the ontology generated by ROO). ROO’s user interface reuses some components provided by Protégé, but simplifies the terminology and hides options that require a deep understanding of OWL by choosing default values. For example, when adding an annotation in Protégé 4, the user has to specify an annotation type, an XML type and a language in order to enter the annotation. In ROO, we reuse the same GUI component as Protégé 4, but we only allow users to use XML String as the XML type, ‘en’ as the language code and we choose the annotation type depending on the type of information that the user is entering (e.g. Natural language definition of a concept or scope of the ontology). Note that all the plug-ins that work with Protégé can also be used in ROO, but may be hidden in the default configuration, i.e. a knowledge engineer can configure ROO to behave in the same way as Protégé. ROO also inherits Protégé’s functionality to read, edit and write OWL ontologies using the OWL API. This is used for converting parsed Rabbit sentences to OWL and to compare the state of the ontology with the Kanga methodology. The Guide Dog functionality in ROO is driven by a rule-based task planner implemented in JBoss Drools. The task planner contains rules for determining when a set of ontology construction tasks should be carried out according to the Kanga methodology. Each rule has the form when LHS then RHS, where LHS is a condition expressed in terms of the ontology being constructed. Typical conditions are: whether the ontology contains a specific annotation(e.g. scope annotation); whether the ontology defines more than a specific number of OWL classes or whether an OWL entity in the ontology contains a specific annotation (e.g. related_rabbit_sentence annotation). These LHS conditions are checked using the OWLAPI to inspect the Ontology, its axioms and its annotations. The RHS is only triggered when the LHS condition is met. The RHS adds a task to a list of tasks that the Guide Dog will suggest to the user. As an example, the prescription made by Kanga that: ‘‘Users should enter a natural language description for each concept in the glossary’’ is encoded in the following rule: rule ‘‘Enter free-text definition for Entity X’’ when IOntologyWrapper( hasScope = true, hasPurpose = true, numberOfKnowledgeSources > 0) ew: IOWLEntityWrapper( hasFreeTextDef = false, numOfSent:numberOfRabbitSentences) then ntc.add(NextTaskSuggestionType. EnterFreeTextDefinitionForAEntity, ew); end When a user invokes the Guide Dog, all the rules are triggered, populating the Guide Dog with a list of tasks that can be presented to the user. Before presenting the tasks to the user, the Guide Dog sorts the tasks to give priority to tasks that are related to the currently selected OWL Entity. The Rabbit Language Processor is implemented using GATE4 and is inspired by the parsing in CLOnE [20]. During parsing, we use GATE to perform natural language processing tasks such as tokenizing, sentence splitting and part-of-speech tagging. The NLP processing is vital in order to allow users to write Rabbit sentences that feel natural because it allows us to recognise that strings with different morphologies refer to the same OWL entity (e.g. ‘‘has part’’ and ‘‘have parts’’). We use the NLP annotations to find the structures defined by the Rabbit grammar using JAPE5 transducers, resulting in a Rabbit AST(abstract syntax tree). Finally, the Rabbit AST can be used to give appropriate feedback to the users in case of errors or warnings and it can be used to generate OWL axioms and annotations that can be added to the ontology. A more detailed description of the implementation of the Rabbit parser is given in [11]. 4. Example interaction with ROO This section shows a typical user interaction with ROO, illustrating how the combination of the Kanga methodology, Rabbit CNL and the tool support provided by ROO assist a domain expert to construct OWL ontologies. We also show how the resultant ontology benefits an experienced user, due to the good readability and provenance support provided by Kanga and ROO. The interaction described below combines steps performed by different domain experts as observed during our evaluation studies (see Section 5), although we have changed or ommited details in order to give a narrative that reflects a typical interaction with ROO.6 Hayley is an expert in water pollution at the UK Environment Agency. She is involved in a team examining the location and likely impact of pollution events in different parts of the river network in the UK. Hayley has to describe her knowledge of river networks and the flow of water in an ontology that can help in pollution assessment tasks. She has not come across SW technologies before, is not familiar with ontologies, and has not been previously involved in any knowledge engineering tasks. Although Hayley has extensive domain knowledge, she has no idea how to encode this knowledge in a computer processable form. Hayley has been advised to use ROO because the tool can assist her in defining and documenting the knowledge required for her impact-assessment task, which can in turn help with automating her work and will enable other organizations to use the same knowledge. 4.1. Creating a new ontology with ROO Hayley opens ROO7 and is first presented with the ROO welcome screen. Since Hayley does not know anything about ROO she clicks on ‘‘Show me an introduction to ROO’’ which runs a video about ROO, the user interface and the steps required for building an ontology. The video also introduces the ‘‘Guide Dog’’, which Hayley can use whenever she is stuck and needs a suggestion about what to do next. Hayley then starts building her ontology. The first tab in ROO is for defining the ontology purpose and scope; Hayley learns from the help documentation what she is supposed to enter here. She enters: Purpose: To be used for answering questions on location and likely impact of pollution events in different part of the river network. Scope: Describes water pollution in the context of river networks. Water pollution in oceans and underground water is not covered. 4.2. Defining knowledge sources Hayley then selects the next tab in the user interface, which takes her to the second step in the Kanga methodology—gathering source knowledge and relevant documents. Here, Hayley specifies sources that provide definitions of general water pollution concepts which she intends to included in the ontology.8 Hayley also enters her name as a knowledge source since she intends to provide her own definitions of some specific concepts. Later on, whenever Hayley needs to refer to more sources, she adds them by coming back to the Knowledge Sources tab. She will be asked to specify the corresponding knowledge source every time she enters a natural language description of an entity. This provides important meta-data which can be used by other ontology authors for changing/verifying the conceptual ontology. Hayley becomes aware that specifying knowledge sources is an important step during ontology construction. 4.3. Adding concepts and relationships Hayley now switches to the Concepts and Relations tab. She consults the Guide Dog, which suggests to ‘‘add concepts to the ontology’’, prompting Hayley to click on the button to add a new concept. She sees a dialog with the sentence: A new concept is a concept. Where ‘A new concept’ has already been selected. She types the name of the concept Organophosphate and sees that the concept is highlighted and that she can click on the ‘OK’ button. When she does this, Hayley sees that the concept Organophosphate now appears on the list of concepts. The Guide Dog now suggests an ontology construction task that Hayley has not seen before: to provide a natural language text to describe the concept Organophosphate. Hayley does this by taking a definition from the UK Government Pesticide Safety Directorate which she specified as a knowledge resource. At this point the natural language description of the concept has been filled in. 4.3.1. Describing concepts using Rabbit After entering more concepts, the Guide Dog starts suggesting a new task to Hayley: concepts need to be defined using Rabbit sentences. She selects Water Pollution as the concept to define and sees that it is missing a natural language description. By now, Hayley has learned that this is a step in the ontology creation process, thus she enters her personal definition (i.e. she indicates herself as the knowledge source): ‘‘Water pollution is the contamination of bodies of water as a result of human activities. Water is polluted when it can no longer be used for its intended use.’’.9 Because Hayley is not familiar with Rabbit, she enters the same text in the Rabbit editor. This results in error messages telling Hayley that the text is not a valid Rabbit sentence and that she should read the documentation about Rabbit to find a suitable sentence. In the documentation she sees several examples of valid sentences and Hayley is instructed to break down long definitions into multiple simple sentences. Using example sentences, Hayley enters a simplified statement: Water pollution contaminates bodies of water. This time, the parser recognises a valid Rabbit sentence and shows the message: CONCEPT ‘bodies of water’ has not been added to the ontology yet. INSTANCE ‘water pollution’ has not been added to the ontology yet. RELATIONSHIP ‘contaminates’ has not been added to the ontology yet. Using the templates offered in ROO for adding new concepts and relationships, Hayley enters: Bodies of water is a concept. Contaminates is a relationship. Hayley is surprised by the suggestion that Water Pollution is recognised as an instance. She checks the existing Rabbit patterns and realises that there is a missing word Every at the beginning of the sentence. She then enters: Every water pollution contaminates bodies of water. The sentence is accepted by the parser, and she is allowed to enter it (the OK button for entering the Rabbit construct is allowed). Hayley moves to the additional part of her natural language sentence. She follows the same Rabbit pattern and enters: Every water pollution is result of human activities. The parser recognises the Rabbit construct – Water Pollution and Human Activities are colored in blue, and is result of is colored in green. Hayley is given a message that the concept Human Activities and the relationship is result of are not in the ontology yet. She enters Human Activity. When entering the relationship is result of, Hayley realises that she may need to use this relationship in other sentences and decides to enter is caused by instead. As a result of this interaction, Hayley has entered the following Rabbit sentences corresponding to the first sentence in her natural language description: Every Water Pollution contaminates Bodies of Water. Every Water Pollution is caused by Human Activities. Hayley has more difficulty translating the second sentence of her natural language description as it contains a self reference (i.e. ‘‘its intended use’’). She solves this by introducing the concept Contaminated Water. Following the previous Rabbit sentences, she enters: Every Water Pollution results in Contaminated Water. 4.3.2. Refining concept definitions As Hayley defines more concepts (and as she shares her ontology and gets feedback from other people), she gets used to the Rabbit language and starts exploring and using more complex constructs. She revisits her definition of Water Pollution as shown in Fig. 3 in order to introduce an equivalence relation. The Rabbit editor displays Water Pollutant in blue to show that this is a concept, but underlines the concept using squiggly lines. The editor also shows a message stating that the concept Water Pollutant is not defined in the ontology. Hayley realizes that she needs to add concept Water Pollutant and writes the sentence: Water Pollutant is a concept. This adds the new concept and the previous error message disappears. Similarly, she adds the relationship causes problems for and accepts the sentence defining Water Pollution. The appropriate knowledge is encoded and the new concept and relationship are now added to the glossary. In a similar way, Hayley enters additional sentences to encode the knowledge in the textual descriptions of Organophosphate and Water Pollutant. The Guide Dog reminds Hayley that she introduced concepts such as Contaminated Water, but she has not given natural language or Rabbit definitions for that concept. An example of the ROO interface showing part of the ontology defined by Hayley is given in Fig. 2. When Hayley exits ROO, she is prompted to save her work. ROO saves the ontology as an OWL file that can be shared with others using ROO or any other OWL editor such as Protégé. The Rabbit sentences are stored as annotations which can be presented to other domain experts for inspection. As Hayley continues working on her ontology she learns the capabilities and limitations of Rabbit and is able to write complex definitions with advanced constructs such as disjoint classes, concept negation, equivalence relations and nested definitions, for example: A Rill is anything that: flows in exactly one Gully; has part exactly one Gully that contains Water; has Linear Form. Rill and Pools are mutually exclusive. No Rill has a Circular Current. 4.4. Ontology reuse The ontology built by Hayley can be reused and extended by more advanced users. Anjit is an expert at Ordnance Survey specializing in geographical information systems. He has some experience of ontologies and SW technologies, having used Protégé to author an ontology to describe his organization’s knowledge of Hydrology and how Ordnance Survey describes the locations of different water features. His current task is to combine Hayley’s Water Pollution ontology with the Ordnance Survey’s Hydrology ontology, to allow Hayley to access the locations of her pollution sites using Ordnance Survey topographic data. Anjit is not familiar with water pollution, and finds the Rabbit sentences and accompanying free text definitions of the concepts and relationships easier to understand than decoding the OWL. He also finds it helpful that the ontology is documented and indicates the knowledge sources used. This enables him to look up the original documents or contact the original authors of the ontology when he needs to clarify details of the ontology. Anjit uses ROO in combination with existing Protégé plug-ins to inspect Hayley’s ontology. For example, he can use the OWLViz plug-in to inspect the water resources Hayley has defined. Anjit decides to import concept Ditch from the Hydrology Ontology into Hayley’s Water Pollution ontology. He does this by entering the following Rabbit sentences: Use ontology: OS Hydrology from http://example.com/v1/Hydrology.rbt. Refer to Ditch [OS Hydrology] as Ditch. Both scenarios given above are based on real ontology engineering tasks at Ordnance Survey and one of their clients – the UK Environment Agency. Hayley’s interaction was also part of the tasks performed by users with background in environmental studies during an experimental study with ROO. The results of this study, as well as the regular feedback from current users, enabled us to identify benefits and limitations of our approach. 5. Evaluation: potential and limitations of the approach This section discusses how well our approach works based on user experiences with the ROO tool. We will examine the benefits of added features in ROO to assist domain experts’ involvement in ontology authoring, and will point out how these features can be integrated in relevant approaches. We will also outline open issues and point at further development. 5.1. User studies with ROO Usability studies were conducted during the development of ROO while new features were still being added. Three small usability studies (with 3–4 users) were conducted. Music was chosen as the domain due to the availability of users with subject knowledge. Users were asked to build an ontology of musical instruments, corresponding to the material in a UK A-level syllabus.10 All participants had studied this specialized level and had played different musical instruments. The users did not have experience and knowledge in ontology engineering, their computer background varied from programming to general computing literacy skills. The sessions were recorded using CamStudio, and a member of the ROO team would always be present at the first session for a user to observe how newcomers would start using the tool. This enabled us to tune the interface and, most importantly, to polish the user guidance and the support provided with the CNL interface. It also allowed us to elicit interaction patterns, which were further examined in follow up studies. The second source of user experience with ROO is a comparative user study performed to evaluate (i) the suitability of the tool for domain experts with no ontology engineering background; (ii) whether users gain any understanding of ontology construction and knowledge engineering; and (iii) the quality of the resultant ontologies. The study followed a task-based, between-subjects experimental methodology, comparing ROO with a baseline system. We chose ACE View [32] as the baseline system, as it is a CNL-based ontology authoring tool that is similar to ROO. ACE View is a Protégé plugin that provides similar features as ROO: (i) input in a controlled natural language; (ii) error messages for sentence composition and (iii) automatic translation into OWL. ACE View does not provide any other type of support to domain experts besides its CNL interface, making it a suitable baseline system to gather evidence of the impact of ROO’s holistic approach compared to a CNL-only interface.11 We did not choose Protégé as the baseline, because the users in our study did not have knowledge engineering skills (required for interacting with Protégé). The reader is directed to a previous evaluation study [20] which shows advantages of a CNL-based interface compared to Protégé. The participants in the comparative user study were 16 students (at Bachelor, Master, and PhD levels) and 2 staff members from the departments of Geography and Environmental Studies at the University of Leeds. The participants had no previous experience with ontologies, knowledge engineering, or computer programming. Their direct interaction with ROO and ACE View was restricted to a single session of one hour where the participants built an ontology from scratch. The domain of the ontology was specified in advance depending on the area of expertise (Water Pollution or Hydrology) and a list of representative concepts had been specified (e.g. River, Catchment, Sediment). The participants were asked to bring free-text definitions of these concepts before the session, in order to maximise the time of interaction with the tools. Another hour was used to explain the study, gather questionnaire answers, introduce the domain modeling task, introduce the tool and CNL to be used, and get feedback about the system. The interaction records from each session, including video, log files from ROO, the resultant ontologies and pre- and post-experiment questionnaires were analyzed to find statistically significant trends that result from using ROO. A detailed description of this user study is presented in [13]. As the final source of experience with ROO, several staff members and students at the University of Leeds, UK have used ROO to build ontologies and have provided their feedback about the tool and the overall approach. Currently, the ROO distribution provided at sourceforge has more than 700 downloads.12 Based on the user experiences with ROO, we will address three questions crucial for research and projects focusing on domain experts’ involvement in ontology engineering: - What are the benefits and limitations of CNL-based interaction for ontology authoring? - What is the quality of the resultant ontologies? - How can ontology authoring tools support domain experts’ involvement in ontology construction? 5.2. Benefits and limitations of CNL-based interaction for ontology authoring 5.2.1. Efficiency The main aim of exploiting CNL-based interaction for ontology authoring is to allow domain experts to produce ontologies without extensive training beforehand. The user studies with ROO confirmed this. In the comparative user study domain experts were able to create simple ontologies after about 10 min introduction to the CNL (Rabbit or ACE) and the corresponding authoring tool (ROO or ACE View) [13]. In one hour, the participants who used ROO produced an average of 21.9 class definitions and 8.25 object properties. The one hour included time for initialization tasks, such as the definition of the ontology scope and purpose and the definition of knowledge sources. Time was also spent for entering the natural language descriptions for the key domain concepts (usually copying the definition from a document or a website). The participants who used the baseline system ACE View produced an average of 28.1 class definitions and 11.9 object properties. Although this productivity metric is higher than that of ROO users, note that ACE View did not require users to enter scope, purpose, knowledge sources or natural language descriptions. The results give positive evidence that CNL-based interaction makes it possible to quickly involve domain experts in ontology construction. The time users spent familiarizing with CNL interfaces (both in ROO and in ACE View) was relatively short. This can be due to the fact that CNL sentences are intuitive to understand [15]. Furthermore, CNL provides a unified way for defining all knowledge constructs, such as entering concepts/relationships, specifying hierarchical links, and formulating axioms. Our evaluation does not compare CNL versus non-CNL ontology authoring (e.g. direct editing of OWL statements, using forms, or visual interfaces). Hence, we have no direct evidence that CNL-based ontology engineering is faster than non-CNL. Some work in this direction has been done elsewhere, for example [20] reports that a CNL interface is more efficient for performing simple ontology construction tasks than traditional tools such as Protégé. 5.2.2. Abstraction Defining ontological constructs in a CNL requires some level of abstraction, albeit that the CNL reduces the cognitive complexity of this process. Typical sentences for defining a concept in a natural language tend to be information rich. For instance, a text definition of Flood Plain given by a participant in our comparative study is ‘‘Flood plain is the area of land surrounding a river, which is usually flat, and is prone to flooding’’. The domain expert has to learn that this text is not suitable for the ontology and has to be broken down into several sentences. In our study, this was explained at the start of the session when introducing the tool and CNL. However, participants used help material such as example CNL sentences and the tool feedback (error messages) to explore the CNL and learn the limitations of the language. For example, a participant broke the above natural language definition of Flood Plain into the following Rabbit constructs: Our studies showed that the process of breaking down natural language definitions into CNL sentences is a crucial step when using a CNL for building ontologies. In most of the cases, the participants performed this breaking correctly after an initial phase when they learn the restrictions imposed by the CNL and staying within those limits while describing concepts from the ontology. All ROO users, as well as the ACE View users in the comparative study, were able to quickly decide how to rephrase most natural language definitions. Although the reduced complexity of abstraction from natural language to ontological constructs is a key advantage of CNL-based ontology authoring, it also brings a crucial limitation. Making the formulation of ontological statements fairly easy can be misleading. In our studies, the participants without knowledge engineering background would focus mainly on the formulation of the CNL constructs. None of them questioned what was logically entailed by what they had entered. In contrast, users with previous ontology engineering experience not only managed to quickly formulate Rabbit sentences but were more dubious about the exact meaning in OWL terms. These users would often open the Protégé Class Description View to check the OWL translation of the entered sentences. This points at the need for offering intuitive ways for ontology validation, as discussed below. 5.2.3. Ambiguity The syntax and semantics of CNLs are usually clearly defined. However, to a user the CNL-based interaction resembles natural language interaction which can hide the logical precision and may introduce ambiguity. During the user studies with ROO, we observed three types of ambiguity. Firstly, ambiguity was introduced by some standard constructs embedded in the CNL. Commonly confused were is a kind of and is a in Rabbit – the former is used to enter subclasses, while the latter is used for defining instances. Users without knowledge engineering background did not realize the difference, and often mixed instances and classes, see Section 5.3. Such ambiguity problems can be anticipated and corresponding prompts added. In the most recent version of ROO, appropriate error messages are added when a possibility for mixing is a kind of and is a is detected (e.g. when a partial pattern of class definition is recognized but an is a relationship is used, the user is reminded that is a is used for defining instances). The second ambiguity type observed was caused by inability of the CNL parser to determine the part-of-speech for some words, most commonly when a word could be tagged as either a verb (hence, corresponding to a relationship) or a noun (hence, corresponding to a concept). For example in a Hydrology domain, a user stated that Flow is a concept, when trying to describe the Water Flow of Rivers and other bodies of water in terms of their flow of water. However, the Rabbit parser (at the time of the study) tagged Flow as a verb (e.g. to flow) instead of a noun, and the sentence was not accepted. ACE View had similar problems that were solved by enabling a user to extend the glossary of terms. A solution for this ambiguity type is to make the authoring tool aware of such cases and to allow the user to override the part-of-speech tagger of the parser at runtime. The tool should help the user realize that there is a danger of introducing ambiguity, e.g. the user could always state that flow is a relationship, which results in an ontology having an object property and a class with potentially the same name. The latest version of ROO includes corresponding warnings when this type of ambiguity is recognized. The third ambiguity type observed was associated with parsing compound noun or verb phrases when the boundary between such phrases was hard to determine. Consequently, the CNL parsers reported ambiguity and asked the user to re-phrase the sentence. In ACE View, users worked around the problem by using dashes to get an ACE sentence accepted (e.g. ‘‘Catchment is an area-thatcollects-water.’’ where area-that-collects-water is translated as a single OWL class, which is not further defined). ROO does not allow this type of merging, which requires the user to formulate two sentences to relate Catchment to an area and to things that collect Water. This type of ambiguity was the most confusing for users without knowledge engineering skills [13]. The latest release of ROO includes a context-aware Rabbit sentence parsing which uses the current ontology to recognize the most probable Rabbit pattern, see [11] for a detailed description of the Rabbit parser. 5.2.4. Coverage Learning to use a CNL resembles learning a new language – starting from basic constructs and gradually adding more complex statements. This was confirmed in the experimental studies with ROO – most domain experts utilized only a subset of the full set of Rabbit (and ACE) sentences. The resultant OWL ontologies varied from ALE to ALCO and ALCOQ . That is, the resultant OWL statements included definition of subclasses, anonymous classes (concept union and intersections) universal and existential restrictions and qualified cardinality restrictions. Domain experts rarely used or did not use at all CNL sentences that translated into disjoint, equivalence and negation axioms as well as role hierarchies and role inclusions. The reason for this might be that users tried to follow natural language descriptions of concepts – one rarely describes a concept in terms of what it is not (disjoint classes or complex concept negation). These axioms are crucial for the quality of the resultant ontology as they are vital when using OWL reasoners for automatic classification. One possible way to empower the reasoning is to ask knowledge engineers to inspect the ontology and add the connecting axioms. Another way is to help domain experts use more expressive sentence types. The Guide Dog feature in ROO would be a good way to do this. For instance, by scanning the created taxonomy for suitable candidates it can generate connecting statements that will enable the reasoning, and can ask the user to confirm or reject these statements. Furthermore, domain experts can be directed to use a more systematic approach to combine axioms following ontology design patterns [21], which can be a crucial feature in ontology authoring tools geared towards domain experts. To sum up, our evaluation showed that CNL-based interaction (in both ROO and ACE View) enabled domain experts to build ontologies from scratch in a short period of time. This suggests that CNL-based interaction can reduce the cognitive complexity associated with the move from natural language sentences to formal ontological statements. CNL shortens the abstraction path by providing an intermediate level of abstraction which helps people without formal logical background to formulate knowledge constructs. The task of converting CNL constructs to OWL, which requires formal knowledge engineering skills, is performed automatically by the CNL parsers. However, we also identified several drawbacks: (i) partial usage of the full spectrum of CNL sentences; (ii) omission of axioms that enable reasoning upon the ontology; (iii) existence of ambiguity which may confuse users with limited logical background. A possible approach to address the drawbacks is to provide intelligent support embedded in the tools, as discussed in Section 5.4. 5.3. Quality of the resultant ontologies The comparative user study described in Section 5.1 allowed us to analyze and compare the quality of the ontologies produced by domain experts using ROO and ACE View. Here, we summarize the main findings and explain how we obtained those findings. 5.3.1. Readability Since both Rabbit and ACE are easy to understand, including the CNL sentence as annotations improves the readability of the resultant OWL ontologies; both ROO and ACE View do this. Ontologies created with ROO were slightly more readable because ROO encourages users to provide natural language descriptions for both concepts and relationships, which are also included as annotations. 5.3.2. Fitness-for-purpose A domain expert who is also a knowledge engineer at Ordnance Survey produced benchmark ontologies in Hydrology and Water Pollution, following the purpose and scope of the ontologies defined in the comparative user study. Each ontology produced by a user in the study was compared to the corresponding benchmark ontology: 1 point was added for every valid axiom (axiom in the user ontology which also existed in the benchmark ontology or was accepted as valid by the analyzing expert), while 1 point was deducted for any invalid axiom (axiom from the benchmark ontology missing in the user ontology or a user axiom containing some modeling error). The overall average score for the ontologies created with ROO was 2.5, while the average score for the ontologies created with ACE View was À4.25 (Mann–Whitney U = 9, p 0.01). ROO out-performed ACE View with marginal significance which can be attributed to the intelligent support provided in ROO, see Section 5.4 for a discussion of the main differences between the tools. Indeed, the analysis showed that ACE View ontologies included a high number of invalid axioms, averaging 8 invalid axioms per person more that ROO users. ACE View ontologies had an average of 0.4 errors per axiom, compared to 0.13 errors for ontologies created with ROO.13 None of the ontologies produced during the comparative user study would have been usable without modification; this is not surprising because the users in this study had only a limited amount of time to build the ontologies, had no knowledge engineering background, and may have not realised the importance of choosing appropriate knowledge sources. Our experimental design followed closely real situations when domain experts enter knowledge constructs. This, however, led to reliance on domain experts’ judgement in choosing the natural language definitions and deciding what statements should be entered. A more controlled experimental design, e.g. providing the correct natural language definitions and asking users to formulate corresponding ontological statements, could be used to further examine the effect of the tools on the ontology quality. Our observations have key implications for involving domain experts in ontology authoring. Firstly, having domain experts more engaged in the ontology authoring process enables key knowledge constructs to be articulated but does not exclude the involvement of knowledge engineers who further check the formal meaning and validate the encoded statements. This gives strong support for collaborative ontology engineering where ontology contributors’ roles are identified and the provenance of each ontological statement is recorded [50]. Secondly, there is a need for intuitive approaches to help domain experts validate to what extent the axioms entered fall in the ontology scope. While the use of CNL provides feedback about whether entered knowledge adheres to the OWL standard, it does not provide feedback about whether the ontology is fit for purpose. This issue is not directly addressed by the work presented in this paper; however we are doing some initial work in this direction. For example, a Verification and Extension Toolkit (VET) for ROO was developed to help users verify the concepts and relationships in the ontology they enter against a text corpus [51]. Using text mining techniques, the Guide Dog was extended to point at redundant concepts and axioms (i.e. concepts and relationships which are present in the user ontology but are not represented in the text corpus), as well as to suggest missing concepts and relationships (i.e. concepts and relationships discovered in the text corpus but missing in the ontology). Very preliminary tests of VET have been conducted. However, it has not been evaluated with users, and is not integrated in the ROO release yet. Thirdly, domain experts should be made aware of the logical implications of their assertions. The first author of this paper is working on a feedback-loop mechanism that will inform the user about some logical consequences of adding new axioms to the ontology being built; this feedback mechanism is part of a dialogue system that aims to capture an OWL representation of ontology purposes [10]. Other possible approaches include providing explanations about inferences [31] or visualizing what has been stated [38]. 5.4. Intelligent tool support Based on the findings from the user studies with ROO, we can identify key features to be embedded in tools that facilitate domain experts’ involvement in ontology engineering. 5.4.1. Intuitive interface The initial usability studies with ROO made it clear that many of the GUI components used by Protégé 4.0 were not suitable for domain experts. Protégé provides an interface that enables knowledge engineers to use all the features that OWL provides. For example, there is a GUI component for adding annotations to OWL entities. When we asked domain experts to use this component, they would be confused by the options provided: the type of annotation URI (custom, built in, dublin core, etc.); the type of annotation (constant, individual or property value); if it is a constant annotation, the xsd type and ‘‘Lang’’. Our target users are not interested in learning all the details about OWL, but are interested in building a conceptual model that can be used in tools that make their jobs easier. The ROO GUI uses terminology that is closer to the domain experts’ perspective instead of using OWL terminology, e.g. concept instead of OWL class and relation instead of object property. By providing a simplified GUI, users are able to construct ontologies without needing to learn about too many OWL specific details. 5.4.2. Assistance with the ontology authoring process To compensate for the lack of knowledge engineering skills in users, ontology authoring tools can incorporate certain knowledge engineering expertise. In ROO, this was done by following an ontology methodology both in the interface and in the guidance provided via the Guide Dog, see Section 3. The user experience with ROO confirmed that this was beneficial not only for reducing the complexity of ontology authoring but, most importantly, for helping users gain an understanding of ontology engineering. During the comparative study between ROO and ACE View, the participants answered questions to test their knowledge about ontologies, concepts, relationships and the steps required to build conceptual models [13]. The users answered the same questions before and after their session with ROO and ACE View. Each answer was marked with: À1 (if the understanding has worsened); 0 (no change to the user’s understanding on the questions), +1 (correct aspects are added but gaps exist), and +2 (the understanding has improved, and now is correct and complete). The maximum score, if a user had not had any ontology modeling knowledge and has become an expert, would have been 12, while the worst score meaning a user was an expert and became totally confused would have been À6. The ROO users scored significantly higher (l = 5, r = 2.78) than the ACE View users (l = 0.38, r = 2.97); U (Mann– Whitney) = 8.5, p 0.01. Hence, with ROO the users’ understanding in ontology modeling improved significantly more than when using ACE View. The positive results are attributed to the main difference between both tools – ROO provides assistance with ontology authoring following an ontology methodology, while ACE View does not (note that the CNL-based interaction in both tools is fairly similar). This advocates in favor of using an appropriate methodology to ground the interaction and to provide intelligent guidance in ontology authoring tools aimed for people with limited knowledge engineering background. 5.4.3. Assistance with CNL-based interaction The limitations of CNL-based interaction pointed out above can be addressed with appropriate intelligent support integrated in the ontology authoring tool. This includes assistance with: - Composing CNL sentences: the assistance can include intuitive warnings and error messages, providing appropriate help, offering examples. - Transition from natural language to CNL: providing help with breaking a natural language definition into CNL statements. - Resolving ambiguity: the CNL parsers can recognize possible CNL patterns to automatically disambiguate between alternatives. - Ontology validation: intuitive ways to help users understand the logical implication of the CNL statements they have entered. - Connecting statements: domain experts need to be encouraged to define concepts with sentence patterns that are required for certain reasoning tasks with ontologies (e.g. define equivalence relations, disjoint classes), because these constructs are not common in natural language knowledge sources such as dictionaries, encyclopedia. - Ontology reuse: domain experts can be enabled to see existing ontologies in a way they can understand. A bidirectional CNL approach [9,33], where existing OWL can be automatically converted to Rabbit may help in this respect. Also, high-level summaries, metadata and visualisations as described in Section 2 can help to achieve this. 5.4.4. Individual differences and multiple perspectives Ontology authors differ based on their domain background, linguistic knowledge, and previous ontology engineering experience. For example, two participants found the formulation of CNL sentences very challenging. This may be caused by the lack of a good command of English (neither participants was a native speaker) or poor linguistic knowledge (these participants found it challenging to identify potential concepts and relationships in natural language descriptions). Ontology purpose is fundamental for taking ontology modeling decisions and is a key factor in ontology perspectives. It appears fruitful to seek ways to model purpose and clarify with users that domain axioms correspond to the intended purpose. Our initial study in modeling ontology purpose in a Geography domain is presented in [10]. In our current research, we consider modeling multiple perspectives in ontology authoring and taking these perspectives into account for providing individualized user support. 5.5. Generality and wider applicability The approach presented here has been followed for the creation of ontologies in several domains, as part of ongoing projects: music,14 water pollution, hydrology, buildings and places,15 dissertation writing,16 fire and rescue services,17 ontology purpose description.18 In addition, existing ontologies from Protégé, such as pizza, wine and cameras,19 were re-created with ROO. This enabled us to identify factors which affect the applicability of our approach to a range of domains, and to identify possible future improvements. Firstly, the applicability is affected by the CNL-based interface. Some domains may contain highly specialised terms that standard parsers cannot identify correctly (i.e. the part-of-speech is identified incorrectly). For example, in the camera domain, camera instances have names such as ‘Canon PowerShot SX200 IS’. Recognizing that the phrase is a possible concept or instance is a significant challenge for CNL parsers (the ROO parser could not recognise such constructs). In such cases, the interface can be frustrating since the users have to find a way round to enter these constructs, which make the ontology authoring cumbersome. Furthermore, problems with accurate part-of-speech tagging may arise when gerunds are used to define concepts such as activities, actions or processes (e.g. Data Processing, Housing, Trading). Although we have built some ontologies using this type of concepts, we lack evidence for the robustness of parsing large ontologies where the majority of the concepts describe processes. Another constraint of CNL-based interaction is that in order to avoid ambiguity, CNL sentences can be verbose, e.g. when combining more than one conjunction or disjunction with cardinality restrictions. This may occur in cases where complex relationships need to be captured between classes in a heavy-weight ontology. Although such occurrences were noted in our studies, we do not have enough cases for a systematic analysis of how domain experts cope with complex CNL constructs. This is a subject of future research. Secondly, the wider applicability of our approach, and ROO in particular, is hindered by technical constraints of the tool. There is a memory overhead on top of Protégé because we keep track of the CNL sentences on top of the OWL representation. The largest ontology we have built contains around 250 concepts; we did not encounter any performance problems with this ontology. However, much larger ontologies may cause performance problems. Currently, ROO does not support the translation of an existing OWL ontology into Rabbit, which means the tool is not suitable for editing existing ontologies. This is not a limitation of the approach, as other CNL tools such as ACE View and CLOnE provide roundtrip ontology editing [9]. Finally, the ontology methodology followed, Kanga, does not provide sufficient description of the collaborative aspect of ontology construction. For instance, Kanga does not specify what collaborative activities several domain experts can be engaged in while constructing the ontology. Currently, ROO does not provide ways to collaborate with others or to share the ontology, leaving this to the users’ discretion. We plan to extend ROO to include collaborative activities as part of our work on a recently started EU project.20 6. Conclusions This paper has presented our experiences with an approach for ontology construction that combines a controlled natural language to formalize conceptualizations and provides guidance through the ontology construction process by following an ontology construction methodology. This work is fundamental for ontology engineering as it pushes the boundaries of what a single domain expert with no previous knowledge engineering experience can do without needing to undergo training on OWL and ontology editing tools. The key novelty of our approach compared to other attempts to actively involve domain experts in ontology authoring is in offering a holistic way that complements a CNL-based interaction with an appropriate methodology to provide innovative tools for ontology authoring suitable for users who lack knowledge engineering background. The paper described the basic building blocks of our approach: the Kanga ontology construction methodology, the Rabbit controlled natural language and the ROO tool. All three have been designed based on previous experiences on ontology construction methodologies, CNLs and tools, but focus on the active involvement of domain experts. Kanga adds to the suite of ontology methodologies tailored for domain experts. It contributes by showing a methodology that has been derived empirically, based on extensive experience at a large organization (the UK mapping agency – Ordnance Survey), clearly defining the involvement of domain experts and knowledge engineers, stressing the importance of domain experts in the construction of heavy weight ontologies and showing how this can be addressed by using a CNL. Rabbit is a controlled natural language designed following extensive experimental work with domain experts at Ordnance Survey. It looks quite natural and close to English constructs. Its expressiveness is comparable to OWL 2 – there is direct match to OWL constructs which enables easy conversion to OWL. Rabbit focuses on usability, i.e. how sentences can be composed and read by domain experts without a formal logics background. ROO is a novel ontology authoring tool geared towards domain experts. ROO enhanced the usability of CNL-based interaction, integrates ontology methodology and compensates for the lack of knowledge engineering skills by offering intelligent assistance. Based on experimental studies with ROO, we have discussed the advantages of using a combined approach that is tailored to domain experts over a CNL-driven approach that provides a layer between Protégé and the user. Our experimental studies show that: (i) ontology authoring with a CNL is easily possible; (ii) providing CNL tool support results in higher quality ontologies; (iii) providing ontology construction methodology guidance enables education of the domain experts in basic knowledge engineering. However, we also identified pitfalls of CNLs, which indicate that further intelligent tool support is essential. We also pointed out limitations of our experiences with ROO, such as not having enough data about users building large complex ontologies with it, so the expressivity used by domain experts is still limited. These limitations will be addressed in future work. Finally, our experiences with ROO also showed that tailoring ROO around Kanga and providing support based on that methodology improves the user’s understanding of the ontology construction process.
TextOntoEx: Automatic ontology construction from natural English text. Most of existing ontologies construction tools support construction of ontological relations (e.g., taxonomy, equivalence, etc.) but they do not support construction of domain relations, non-taxonomic conceptual relationships (e.g., causes, caused by, treat, treated by, has-member, contain, material-of, operated-by, controls, etc.). Domain relations are found mainly in text sources. TextOntoEx constructs ontology from natural domain text using semantic pattern-based approach. TextOntoEx is a chain between linguistic analysis and ontology engineering. TextOntoEx analyses natural domain text to extract candidate relations and then maps them into meaning representation to facilitate constructing ontology. The paper explains this approach in more details and discusses some experiments on deriving ontology from natural text. 1. Introduction The most widely quoted definition of ‘‘ontology’’ was given by Tom Gruber in 1993, who defines ontology as (Gruber, 1993): ‘‘An explicit specification of a conceptualization.’’ Ontologies have proved their usefulness in different applications scenarios, such as intelligent information integration, knowledge-based systems, natural language processing. The role of ontologies is to capture domain knowledge in a generic way and provide a commonly agreed upon understanding of a domain. The common vocabulary of ontology, defining the meaning of terms and relations, is usually organized in taxonomy. Ontology usually contains modelling primitives such as concepts, relations between concepts, and axioms. Ontologies have shown to be the right answer to the structuring and modelling problems arising in Knowledge Management. They provide a formal conceptualization of a particular domain that can be shared by a group of people (in and between organizations). Most of existed ontologies construction tools support construction of ontological relations (e.g., taxonomy, equivalence, etc.) but they do not support construction of domain relations, non-taxonomic conceptual relationships, (e.g., causes, caused by, treat, treated by, has-member, contain, material-of, operated-by, controls, etc.). Domain relations are found mainly in text sources. TextOntoEx constructs ontology from natural domain text using semantic pattern-based approach. TextOntoEx is a chain between linguistic analysis and ontology engineering. TextOntoEx analyses natural domain text to extract non-taxonomic relations of specific domain from natural text using semantic pattern-based. TextOntoEx enriches shallow ontology with non-taxonomic relations, relation may hold between two concepts or more by a verb. Shallow ontology could be constructed using DOATool (Dahab, Hassan, Rafea, & Rafea, 2004). We developed shallow semantic parser which uses the semantic pattern. Semantic pattern is a generic formal representation for natural text fragments, each fragment represented with its meaning Next section presents a brief review on related work. In Section 3, a detailed description of the semantic pattern is given. Section 5 presents an overall architecture of TextOntoEx. Finally, Section 6 is devoted to conclusions and future work. 2. Related work A number of systems have been proposed for ontology  ́ extraction from text, e.g.: ASIUM (Faure, Nedellec, & Rouveirol, 1998.), TextToOnto (Maedche & Staab, 2000), Ontolearn (Navigli, Velardi, & Gangemi, 2003). Most of these systems depend on shallow text parsing and machine learning algorithms to find potentially interesting concepts and relations between them. The OntoLT (Sintek, Buitelaar, & Olejnik, 2004) approach is most similar to the ASIUM system, but relies even more on linguistic/semantic knowledge through its use of built-in patterns that map possibly complex linguistic (morphological analysis, grammatical functions) and semantic (lexical semantic classes, predicate-argument) structure directly to concepts and relations. Other tools depend on semi-automatic approach like SOAT tool (Wu & Hsu, 2002). SOAT tool allows a semiautomatic domain ontology acquisition from a domain corpus. The main objective of the tool is to extract relationships from parsed sentences based on applying phrase-rules to identify keywords with semantic links like hyperonym or  ́ synonym. TERMINAE (Aussenac Gilles, Biebow, & Szulman, 1999 Szulman, Biebow, & Aussenac-Gilles, 2002) has been developed in the Laboratoire d’Informatique of ParisNord at the University of Paris-Nord (LIPN). It integrates linguistic tools and knowledge engineering tools. The linguistic tool allows defining terminological forms from the analysis of term occurrences in a corpus. The ontologists analyze the uses of the term in the corpus to define the meanings of the terms. 3. Semantic patterns The term ‘‘semantic pattern’’ has different definitions in different domains but we define Semantic pattern in this work as ‘‘a generic format for natural language expression, to declare a specific meaning’’. Recognition of these semantic patterns are not straightforward since natural languages may have different lexical items that can be used to make reference to the same situation as well as different syntactic realization of the same arguments. Simple natural text expressions may have more than one semantic pattern; each semantic pattern adds a specific meaning. The correct identification of all possible patterns of particular situation and their arguments is the essence of an accurate ontology extraction. Using Semantic pattern technique employs ontological and linguistic knowledge of how different kinds of ontological classes are combined to represent meaning (e.g., hPlant Parti hBecomes.Verbi hColori). These classes may be found in an ontology or simple taxonomy. Matching natural text with semantic pattern differs from simple key word matching because the patterns used in the semantic matching contain ontological classes (e.g., hPlant Parti). Ontological classes can be substituted by any class subsumed by the upper class (e.g., ‘‘Plant Part’’) . Also, any verb that can play the same role of the main verb can be substituted and so on. We attempt to match every expression in a document, contains a domain knowledge, to the pattern library. We may use in this process an ontology or a simple taxonomy and a data dictionary to determine if a particular word is a member of the class that appears in pattern (e.g., that an ‘‘arm’’ is a hbody parti). 3.1. Semantic patterns elements To represent text expressions in a generic format, we have developed semantic pattern which contains a combination of the following elements: 1. Abstract ontological class (e.g., Plant Part, Color, Shape, etc.). These classes are obtained from top level ontologies. 2. Verb group (e.g., Change group which includes turn, change, become, etc.). This group can be extracted from any semantic lexicons like WordNet. 3. Text constant expression(s), (e.g., prepositions and conjunctions). 4. Optional elements do not give meaning on its own but modify the ontological class like pale in ‘‘pale green’’ and dark in ‘‘dark brown’’. All these elements are non-terminal elements except the third element, it is terminal element. 3.2. Symbols used for semantic patterns 1. We refer abstract ontological class as a word between ‘‘hi‘‘ signs (e.g., hColori, hShapei, etc.). 2. We denote a group of verbs as group name followed by ‘‘.Verb’’ all in between ‘‘hi’’ sign (e.g., hBecomes.Verbi). 3. We designate to an optional elements as its type followed by ‘‘.POS’’ all in between ‘‘hi‘‘ sign (e.g., hOrdinal numeral.POSi). For example ‘‘pale green’’, ‘‘dark brown’’, pale and dark do not give meaning on their own and are not colors but used for describing colors. 4. List of one of the above elements, we put them in the ‘‘[]’’ sign. For example ‘‘spots, stripes, and mottle emerge on leaves’’ matches with ‘‘[hAbnormalAppearancei ] hAppears.Verbi on hPlantParti’’. 3.3. The benefit of using semantic patterns Patterns can be used to acquire taxonomic as well as non-taxonomic relations. The hindrance of semantic pattern based approaches is the necessity to define the required semantic patterns, which is excessive and time consuming but often very valuable task. The primary goals of utilizing semantic pattern-based approach are: To extract implicit and explicit knowledge from natural text. To resolve the interpretation ambiguity. To declare a simple method to understand natural text. To authorize generation of knowledge bases to natural languages. Using semantic pattern guarantees that a vast number of sentences may be matched. Taking this simple semantic pattern ‘hPlant Parti hBecomes.Verbi hColori’ as an example, if we have M as possible plant parts, N as different synonymous of the verb ‘‘Becomes’’, and P as possible color of all plant parts, then we will have M · N · P possible matched sentences with the mentioned semantic pattern. Table 1 shows an example to illustrates most of the possible sentences that match the semantic pattern ‘hPlantParti hBecomes.VerbihColori’: 3.4. Types of semantic patterns We define four types of semantic patterns for the purpose of ontology extraction as follows: (1) Simple unit pattern The phrase matched with this type of semantic patterns includes single semantic unit, for example (hPlantParti hBecomes.Verbi hColori ) this semantic pattern suggests that the color plant part has been changed into other color and it will match these phrases ‘‘leaves turn brown’’, ‘‘veins become purple’’, ‘‘leaves become ashy colored’’, etc. We consider the sentence ‘‘spots, stripes, and mottle appear on leaves’’ has single meaning type because it describes only the appearance on leaves. Therefore, we accept mixed words of the same semantic role, for example, we undertake ‘‘yellow green’’, ‘‘pale green’’, ‘‘dark brown’’, ’’yellow to brown’’ as one color. (2) Compound unit pattern The phrase matched with this type of semantic patterns incorporates more than a single meaning, for example ‘‘hColori hParasitei hDevelops.Verbi on hPlantParti’’ this pattern suggests that there is a parasite develops on a specific plant part and in the same time this parasite has a color and it will match this phrase ‘‘white fungus growth on leaves’’. That is to say, compound patterns incorporate more than simple pattern. This type of semantic pattern has the major use in text. (3) Reference unit pattern The phrase matched with this type of semantic patterns does not contain any but make reference to the semantic unit in other resource. The resources may be in prior expression for example ‘‘similar spots are on the stem’’ or the resources may be out of context like ‘‘Symptoms are similar to the fungal diseases’’. There is no knowledge can be extracted from the natural text expressions matched with this type of semantic patterns but knowledge is found previous expression. (4) Context dependant pattern Sequence of expressions matched each contains part of semantic. This phrase is not clear Mohamed For example ‘‘yellow striping on leaves. Strips turn brown’’. The second sentence matched with ‘‘hShapei hBecomes.Verbi hColori’’, while the matched sentence has an incomplete meaning because it depends on the meaning in previous sentence. We do not know where the ‘‘Strips’’ appears in the second part of the pervious example, i.e., it is incomplete knowledge. The only difference between reference patterns and context dependant patterns is in context dependant pattern we find knowledge in the natural text expressions matched with this type of semantic patterns but this knowledge is incomplete but in reference pattern we cannot extract knowledge in the natural text expressions matched with this type of semantic patterns but the knowledge may be found in previous expression or may be found in other context. 4. TextOntoEx architecture Our objective is to extract non-taxonomic relations of specific domain model form free technical text utilizing pattern based approach. We as well as aimed at building library of semantic pattern by providing tools to ease this process. TextOntoEx does not discover new relation but discovers instances of known relation. We supply an example to show this idea from the agricultural domain and we have chosen the diagnosis model. Fig. 1 shows a portion of agriculture domain ontology, the abstract classes, and a number of non-taxonomic relations between these abstract classes. We use OWL language to represent the ontology. Our approach as shown in Fig. 2 contains the following phases: (1) Constructing semantic patterns using pattern editor. (2) Selecting domain natural text. (3) Extracting domain ontology from natural text. 4.1. Constructing semantic patterns using pattern editor First, construct a set of patterns that describe a particular domain relation between two or more concepts. This task is certainly time consuming but often very valuable task, so we built an editor to facilitate constructing library of semantic patterns. This task may be done once because the objective of this phase is to assemble the library. The editor uses abstract classes of the target domain ontology. In this task the ontology engineer select text, from a training text, that represents suggested semantic pattern using exited ontology, shallow ontology. The ontology engineer may pick free natural text that relates some how to the domain of interest. For example, the following natural text is related the plant diagnosis. Symptoms: yellow striping on second, third, or older leaves. Stripes turn brown, and infected leaves die as the disease progresses. Infected leaves may look frayed as they die. Infected plants are stunted, and flag leaves may be a light tan. The ontology engineer acknowledges and chooses a sentence that holds a piece of knowledge related to the domain of interest and then the ontology engineer enables the editor to generate the correspondence semantic pattern of the selected sentence. Table 2 shows the possible semantic patterns that can be originated from the previous natural text. The ontology engineer can categorize the suggested semantic pattern with the semantic role as shown below. The constructed semantic patterns are grouped by the semantic role. We found that the domain of diagnosis of plant diseases has six major semantic roles as listed below: (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) Discoloration Deformation Parasite Direction Location Time These roles used for describing a single meaning i.e., they represent simple semantic patterns. Also we found the domain has the following compound semantic pattern: Discoloration and deformation Discoloration and location Discoloration and direction Discoloration and time 11) (12) (13) (14) Fig. 3 shows an interface that used to construct the semantic patterns. From this interface the user can do the following tasks: Navigate existing free text of the domain or pick free text from any source. Know how many matched semantic pattern with the free text. Generate any semantic pattern whether from the free text or from his own. Know how many semantic patterns could be generated from the free text. Save the generated semantic pattern in a suitable semantic role. Test if the suggested semantic pattern already existed or not. 4.2. Selecting domain natural text After constructing semantic pattern library, as revealed in the previous section, we select domain natural text as a rich resource of domain ontology. This phase is the most frequently used. There are many resources of natural text but internet is the richest resource for natural text. The objective of this phase is to relate natural domain text to a specific domain and a specific topic. We have developed a home made program that follows the web links and save the extracted text, the body of the natural text of the topic, and put the extracted data on structured format i.e., domain, topic, and body of natural text. 4.3. Extracting domain ontology from natural text The objective of this phase is to extract and construct ontology from natural text. We should select natural text Parasite and discoloration Parasite and location Deformation and time Deformation and location used in a specific domain and under a specific topic. For example, we can choose the ‘‘diagnosis of chickpea diseases’’ as a domain and ‘‘Ascochyta blight disease’’ as a topic. The classification of natural text according to the domain and topic may be found on the internet, particularly if we follow the web links between pages. The determination of the domain and topic selected is very important, because they represent the basic classes that we want to enhance them with the extracted ontology. In this phase, we analyze any input paragraph of a domain titled with a related domain and topic. We ensure that the selected paragraph does not include any negated words. We accept any word or phrase that gives a possibility, for example ‘it is not common’, ‘rarely’, ‘in some cases’, ‘occasionally’ etc., because Ontology is stateless knowledge, so we are interested to find and extract the domain relations. We convert the input paragraph into one or more semantic-pattern-like format(s), intermediate format. We use exact match approach. We match the converted paragraph, natural text, with the pattern library to know the exact matched pattern(s). For example, we found this the following natural text that describes a symptom of ‘Barley yellow dwarf disease’ that affects barley crop: Early symptoms include blotches near the leaf tips small yellow-green We convert this text into the semantic-pattern-like format, intermediate format, and removing the phrase ‘Early symptoms include’ because it is out of concern in this stage. At last we save semantic pattern as follows: hHelp Abnormal Appearance.POSi hColori hAbnormalAppearanceinear the hPlantParti hPlantParti After we match this converted paragraph with the pattern library, we found that only one pattern matched which is: hHelp Abnormal Appearance.POSi hColori hAbnormalAppearancei near the hPlantParti hPlantParti Form the previous example we can note that: Not all text must be matched on the extraction process. The phrase ‘Early symptoms include’ is neglected. The ontological classes ‘location’ and ‘time’ need to be enhanced. That is clearly appears on the word ‘near’ for determining the location of symptom and the word ‘Early’ for determining the time that symptom appears on the plant. The matched process is very simple and easy after we include the ontological classes on the converted text. If there are more than a semantic pattern matched with the converted text, intermediate format, and one pattern is a part of the other, we do one from the following choices: – Neglect simple pattern and accept compound pattern. – Neglect smaller pattern and accept larger pattern. Otherwise, we accept all matched patterns. Most sentences in natural language may be matched with more than a semantic pattern and each pattern matched adds a meaning. In another words, a sentence in natural language hold more than an entry in extracted ontology. By applying this rule, we extract the more specific ontology and neglect the public ontology and also because any element in the semantic pattern plays a specific role or adds meaning. During this step, we generate a list of substitutions of the ontological classes and any other semantic elements for all patterns matched. 5. Evaluation of TextOntoEx We validate our approach of using semantic pattern at two levels. The first level concerns the relevance of the extracted semantic patterns: Do they represent the intended meaning with matched natural text? Do the natural texts have semantic expressions that do not represented as semantic patterns? Do the basic ontologies are completed enough? The second level concerns the method used for the extraction: Does it cover all domain meanings in the domain natural text? At the actual stage of development we provide for a small scale evaluation, which consists in establishing a small test corpus (65 sentences) selected randomly out of 13 different and complicated natural agricultural domain text resources. In this small test corpus, semantic relations have been extracted by hand (involving just one person), and the results of the TextOntoEx semantic relation extraction has been compared with the manual extraction. The Table 3 gives some indications on the performances of the system. We mean by ‘Semantic Unit’ in this table: a description or more for an ontological class in one sentence We can summarize our notes in the following points: There is no incorrect (Irrelevant) matching. This indication gives a great performance in the ambiguity problem when we work in large scale. The precision ratio is 100% because the irrelevant retrieved is nothing. The recall ratio is approximately 54%. Most of the unmatched patterns return to the lack of semantic patterns stored. 6. Conclusion and future work We have implemented our mechanism using C# and applied it into case study of agricultural domain. Some natural texts obtained from the Internet, using home made application, are used as a domain documents. These documents are classified into groups each group is titled with disorder, and crop name followed by free text describing symptoms. We have described a low-cost approach for automatic acquisition of non-taxonomy relations from unrestricted text. This framework can be used to analyze text under anchor tag ‘‘hai’’ in html files, as a web mining application. This framework can be used to extract knowledge as well. We have made a tool to build semantic patterns which is a bottleneck to extract ontology. The more semantic patterns we stored the more that recall ratio can be improved. The most suggested future work is to learn new semantic patterns from stored semantic patterns. 
Using Domain Ontology in a Semantic Blogging System for Construction Professionals. Abstract: Smooth communication is essential for the success of construction projects. As an easy-to-use, context-rich, and high-capacity communication tool, blogging is gaining popularity in construction industry. In this paper, the features of blogging technology and how it could benefit construction organizations are presented. To further improve the effectiveness of blogging technology in information and knowledge sharing, an ontology-based semantic blogging system is proposed. Semantic blogging is an extension of conventional blogging and ontology is the key enabling technology for it. Domain-ontology-based semantic blogging site is composed of a network of concepts, which are clearly defined and interlinked according to their context and bound to certain behaviors. This paper reports how the e-Cognos ontology was implemented into a blogging system and how the system functions to process its contents. The paper concludes that using ontology-based semantic blogging site can greatly enhance information sharing between construction professionals and it is a very promising tool for construction communities to publish and share their experience. Key words: semantic blogging; ontology; construction industry; information technology Introduction The construction industry is characterized by unique projects, mobile staff, and constantly changing teams. The industry needs a way of recording the “best practices” for use in future projects and a mechanism of sharing knowledge between project team members. Initially emerged as a personal web publisher, web logging (blogging) is gaining popularity for experience and knowledge sharing. However, although a blogging system built on the conventional Internet works well for posting and rendering all kinds of contents, it provides very limited support for processing them because most web contents are stored in natural language chunks, which makes them very dependent on human users to process them. A very typical example is the traditional keyword-based search, which usually results in high recalls and low precision. In response to this, Berners-Lee et al.[1] formed the vision of a semantic web, which enables automated information access based on machine-processable semantics of data. Semantic web was defined by him as “an extension of the current web in which information is given well defined meaning,” and it can “enable computers and people to work in co-operation better.” A number of recent papers have investigated semantic blogging from different angles. Karger and Quan[2] discuss a semantic blogging prototype built on top of a semantic web browser named Haystack. Cayzer[3] puts a strong emphasis on the use of semantic technologies to enhance the possibilities of blog usage, by allowing viewing, navigation, and querying with respect to semantics. Based on these, a semantic blogging site is proposed for construction professionals in this paper. First, the features of conventional blogging and how it could benefit the construction industry are presented in the paper. Then how the ontology based semantic blogging can further improve the effectiveness and its advantages are discussed. The proposed semantic blogging system is introduced to demonstrate how the domain ontology was implemented and how the functions can be improved. The paper also discusses how semantic blogging could be used by construction professionals to share information and knowledge. 1 Features of Conventional Blogging 1.1 Easy-to-use platform Blogging was initially developed as a low-threshold tool to publish personal journals on the web. The discussion about the possible use of blogging in sharing knowledge started several years ago. Nichani and Rajamanickam[4] found that a company-based employees’ web-blog can provide a unique opportunity to track trails of development and flows of ideas. This makes it a wonderful tool to use for construction projects, which usually involves a great number of individuals with different background and areas of technical expertise. As well known, the construction industry is also featured by a large number of small and medium enterprises (SMEs), which make up over 90% of construction organizations[5]. While larger companies in the sector may be well advanced in implementing formal and complex knowledge management (KM) practices, SMEs are unlikely to afford such a luxury. Blogging, as a low-threshold, light-weighted technology, is cheap to implement and easy to use. It enables users to publish information in small, discrete notes easily, as contrary to large, carefully organized systems. As a light weighted communication tool, blogging provides easy-to-use platform and has great potential in bringing the benefits of note-taking and reflection into learning environments, with the added value of collaboration and social networking. 1.2 A high capacity and context-rich communication medium Compared with conventional web-based groupware such as chat room and discussion forum, blogging can provide a much richer context. Construction projects are usually very complex and takes months to years to finish. Usually a number of diverse individuals and organizations are involved in one project, and it is also possible that project participants are working for more than one project at one time. As project participants often work in ever changing teams, the opportunities for team members to directly socialize with other members appeared to be limited[6]. Nowadays, the email technology has become the most intensively used communication tool in this industry. But email is not an ideal medium for store, annotate, and share information within a group of people due to its transient nature. Blogging in many situations can replace email and become a very handy and useful tool for construction professionals[7]. The strong communication capacity of blogging provides the opportunity of interaction. “Comment” and “trackback” mechanism in blogging enable it to set up an externalization space where the blog users’ ideas can be articulated together. By the processes of expressing, commenting, and discussion, the users can develop a shared understanding based on a piece of information. Blogging with a relatively simple userinterface has high communication capacity and may play an important role in information and knowledge sharing. 2 Semantic Blogging and Domain Ontology 2.1 Semantic blogging in construction Semantic blogging is a technology based on conventional blogging but equipped with more advanced features. In 2003, a research group from Aalborg University (Denmark) introduced the semantic web technology (the web which attaches machine-understandable metadata to the web content) in construction industry, and their work focuses on experience capture and quality assurance for early design collaboration[8]. A member from this research group pointed out that important advances in ICT such as semantic web will be adopted for the next generation knowledge management systems for the construction industry[9]. Anumba et al.[10] also noticed that the existing web applications have a number of limitations and suggested that semantic web and wireless communication system be used to facilitate context-aware collaboration for construction industry. Most recently, Beetz et al.[11] proposed using semantic-web-based multi-agent system for distributed collaboration in AEC/FM industry. 2.2 Construction domain ontology Semantic blogging must be implemented in a semantic web environment and ontology is a key enabling technology for the semantic web. It is composed of a network of concepts, which are clearly defined (creating a common vocabulary), interlinked based on their context (using a set of relationships), and bound to certain behaviors (through a set of rules). Domain ontology provides a common understanding of a domain (a particular area) in which people and the application system communicate with each other. Ontologies are developed for different areas. Some researchers have set up various ontologies for construction industry[12,13]. Among them, e-Cognos[13] is the most comprehensive and mature one, and it was developed by an IST (Information Society Technologies) founded project in Europe. e-Cognos is also compatible with IFC (Industry Foundation Class) and several other classification systems (such as BS6100 and UniClass). e-Cognos is adopted in this research as it provides adequate details for the proposed semantic blogging system and also satisfies the compatibility requirement for the industry’s software integration plan. About 15 000 concepts are identified and classified in e-Cognos ontology document. In e-Cognos, construction knowledge is encapsulated in several overlapping systems, and these relationships are illustrated in Fig. 1. In construction activities, “actors” use some “resources” to produce a certain kind of “products” by following some “processes” that form a “project” according to some boundary conditions. There are a set of “technical topics” which provide information to support the project processes to run smoothly within the work environment. 2.3 Semantic blog annotation “Annotation” is an important facility that semantic blogging can provide. Blog entries are organized chronologically by default, but they can also be grouped by topics. Categorizing is a method of organizing blog entries by assigning each entry to a predetermined topic. Another form of annotation is Tagging. Tags are collections of keywords that can be assigned to blog entries and they can be used to describe and categorize the entry. This feature can help to store and search for information items in a blogging system. In semantic blogging, blog entries are annotated with machine-understandable metadata according to the ontology. These metadata make the content published in the blog site to be found easily and precisely. According to Cayzer[3], because the machine “semantically” understands the blog entries by using conceptual inference, semantic blogging can provide richer processing features. An example of blogging annotation will be given in the later sections. 2.4 Semantic inference In semantic blogging, blog users can navigate the site by using the machine-understandable metadata to find blog entries effectively. Blog users can use dynamically constructed inference to browse related items and follow labeled annotations to find useful information and knowledge items. The semantic inference engine can also use the ontology and help the blog users to build up semantic queries and set up semantic subscription. Blog users can subscribe to a blog category such as “how to use a particular building material in a special construction process.” The query and subscription processes are based on blog aggregation. “Aggregation” is a very useful knowledge management feature provided by blogging. By generating RSS (really simple syndication) feeds (a machine-readable file which summarize the web site contents), blog users can subscribe to particular blog entries. Blog aggregator can automatically check a list of RSS feeds regularly on behalf of a user and supply relevant blog entries (blog articles) to the user. This is an advanced information and knowledge management facility as the information or knowledge items can be actively “pushed” to the users rather than passively wait for users to find and access. Examples of showing how semantic inference work are given in the later sections. 3 Proposed Semantic Blogging System 3.1 Structure of the proposed system As described above, semantic blogging is an advanced collaboration tool. Its informal and user-friendly interface makes it a wonderful communication tool for construction professionals. A domain-ontology-based semantic blogging site was proposed, and it was built as a sub-system of a collaborative knowledge management system. The system was called LiveNet and it was initially developed in the Faculty of IT at the University of Technology Sydney[14]. LiveNet provides facilities to support group work, including document management facilities (e.g., central repository, keyword search engine, etc.), various communication channels (e.g., discussion forum, chat room, email, internal messages, etc.), and work-plan tools (e.g., calendar, workflow, meeting schedule, etc.). In the joint research carried out by the staff in construction management and IT, LiveNet was used as a foundation to build a collaboration system for construction industry[15]. The proposed blogging site is integrated with LiveNet and this integration allows the blog users to take advantages of using the groupware built in LiveNet. The structure of the proposed blogging site is illustrated in Fig. 2. Blog entry creation, publication and aggregation are common features provided by conventional blogging. In the proposed semantic blogging site, e-Cognos was adopted as the construction ontology, blog entries are annotated according to the ontology, and semantic inference helps to locate a particular blog entry after its publication. The annotation process and semantic inference are further explained below. 3.2 Annotation in the proposed system The proposed blogging site in LiveNet uses a semiautomatic annotation approach and the annotation process is illustrated in Fig. 3. Once a blog entry is created, it will first be analyzed by text analysis tool. The text analysis tool extracts key sentences from the document based on the frequency of words and phrases occur in the document. The extracted keywords and the relationships between the keywords will be compared with the concepts defined in the ontology, the concepts with highest similarity will be presented to the blog author or the blog annotator. The blog author or the annotator may accept the suggestion to finish the annotation, or ignore the suggestion and choose his/her own tags to annotate the blog entry. The later action may trigger the feedback mechanism to update the concepts and keywords defined in the ontology. Figure 4 shows that a user Alice’s blog entry has been categorized and annotated as “Natural_material.” 3.3 Inference in the proposed system In the proposed blogging system, blog users can use dynamical inference to browse related items and follow labeled annotations to find useful information. Navigation, query, and aggregation are built as three modules in the blogging system, and Fig. 5 shows the system structure of these three modules with the semantic inference engine. The inference engine provides two important functions in these three modules: classifying concepts and identifying semantic links. • Classify concepts The inference engine is able to compute the relationships between concepts (in IT terms, the relationships between sub-classes and superclasses). This also helps to define “properties” of a certain concepts. For example, “Clay” is a kind of “Natural_material,” i.e., “Clay” is a subclass of “Natural_material.” Therefore, “Clay” has all the properties that “Natural_material” has (in IT terms, this is called “property inheritance”). Therefore, if a query is to find all the blog entries on “Natural_material,” then the blog entries on “Clay” will be included. Figure 6 illustrates that blog user can navigate blog entries by their ontology classification. • Identify semantic links All concepts in the ontology are related to each other via certain relationships; for example, a “process” utilizes “resources” in a “project” to produce a “product” (see Fig. 1). As a result, blog entries, after being annotated by these concepts, are also linked to each other semantically. Semantic inference can find and track these links and process complex queries. For example, semantic inference can identify the blog entries by using materials M, which is supplied by supplier S, used in process P, and used by contractor C in project T. Using Semantic Blogging in Construction Industry As mentioned previously, blogging has many advantages over other communication tools for construction professionals. As construction projects involve many practice-based activities, trainings for construction professionals are usually very practical orientated. As pointed out by Efimova et al.[16], blogging can be used as a technology for facilitating and extending existing apprenticeship and coaching programs or capturing stories of retiring experts. Blogging site set up for a construction organization could help employees to learn from past projects to respond more effectively in future situations. Another important feature of blogging is that it respects its author. Marshall and Sapsed[17] highlighted that knowledge resides in the heads of senior engineers who have gained it through experience; many of those engineers see “knowledge is power” and do not readily share. Blog entries are primarily kept together based on common authorship, not common subject. In other words, the blog author is the one who controls it and the feeling of “ownership” is part of the blogging experience. Blogging naturally has a knowledge-sharing culture built on a mutually beneficial relationship: audiences receive knowledge and contributors gain social capital. Roll and Efimova et al.[18,19] also pointed out that the personal publishing experience is very valuable for the bloggers. Blogging encourages people to contribute more of their knowledge to the community. Semantic blogging system will help users to categorize and retrieve blog entries easier. In the proposed semantic blogging site, if a few different terms are annotated by the same concept defined in the ontology, the blog users can form the same mental images of the meaning of these terms. These can greatly help users to communicate as confusion caused by using different terminologies is minimized. In addition, observing how other people annotate a document is also a process of learning and knowledge sharing. Annotating process shows how an annotator summarizes his/her stories and categorizes his/her collections into predefined topics. The blog author’s tacit knowledge such as intuition, feeling, and insights can be somehow revealed to the reader through the annotation process. The blogging site can also help to locate expertise, too. By introducing some statistic analysis, a tree-like taxonomy interface may clearly show who contribute more knowledge in what subject. This information can guide the knowledge seeker to find an expert in the organization who might help in solving a particular problem. When the blogging site is used in a construction organization and the organization’s policy has made all the employees to fully utilize it, the ontology could help to present a knowledge distribution map. From this map, the strength and the weakness of the organizational knowledge domain can be identified. These can greatly help the organization to do work planning and recruitment. 5 Concluding Remarks Semantic blogging can greatly help construction professionals to share their experience and knowledge. The proposed ontology-based semantic blogging annotates blog entries with machine-understandable metadata, which can facilitate information categorization and retrieval. Semantic inference can help users to build up queries and set up subscription, and therefore, information and knowledge can be “pushed” to the users rather than passively wait to be found and accessed. This domain ontology-based blogging system is a very promising tool to be used in the construction industry. Research and development of the proposed blogging system is ongoing. The research team is currently implementing the system into LiveNet and improving the user interface. The prototype system will be evaluated by construction professionals. Feedback from the evaluation process will be used to further improve the system and these will be reported in later papers. Acknowledgements This research was funded by the CIOB Australasia Research Grant Scheme. 
Using recursive ART network to construction domain ontology based on term frequency and inverse document frequency. Abstract Ontology describes data about data and offers a group of glossaries with a definition that encompasses them in their entire. It not only transfers syntax of words but also accurately transfers semantic data between human users and the network. Hence, the usefulness of the semantic web depends on whether the domain ontology can be constructed effectively and correctly. In this paper we propose an automated method to construct the domain ontology. First, we collected domain-related web pages from the Internet. Secondly, we use the HTML tag labels to choose meaningful terms from the web pages. Next, we use these terms to construct the domain ontology by calculating a TF–IDF to find the weight of terms, using a recursive ART network (Adaptive Resonance Theory Network) to cluster terms. Each group of terms will find a candidate keyword for ontology construction. Boolean operations locate individual keywords in a hierarchy. Finally, the system outputs an ontology in a Jena package using an RDF format. The primary experiment indicates that our method is useful for domain ontology creation. Ó 2006 Elsevier Ltd. All rights reserved. 1. Introduction Collecting useful information from web sites is an important operation on the Internet (Risvik & Michelsen, 2002). However, the content of web sites evolves and expands rapidly. Understanding the needs of the querist in returning related web pages from the Internet is a serious challenge (Risvik & Michelsen, 2002; Yates, 2003). Yahoo1 utilizes the manual classification method (Tirri, 2003) to erect categories for classification, then classifies web pages. The advantage is that the real content of the web pages can be understood because it is manual classification data, but the method is relatively costly and time-consuming. By contrast, the Google2 search engine utilizes a full-text retrieval method. The returned web pages contain at least one query keyword. However, this method retrieves a large number of pages. Users usually need to spend time to browsing the content of the web pages (Diligenti & Gori, 2004). Because the method focuses on keywords, the search results often have a variety of topics the user has no interest in. Sadly, search engines have great difficulty understanding the mind of users (Hendler, 2001). This problem is partly due to the inherent ambiguities of keyword searching. For example, the word ‘‘porter’’ is a kind of worker, but it is also a kind of beer. ‘‘Wind’’ is a meteorological phenomenon and a class of musical instrument. To attack the problem of ambiguity, the idea of using semantic webs in searches has been proposed (Tan, Han, & Elmasri, 2000). However, the expansion of a semantic web and its success or failure depends on whether the ontology can be constructed accurately. Rapid and successful construction of an ontology faces the problem of data on the web that multiplies geometrically each year (Bernaras, Laresgoiti, & Corera, 1996). As a result, ontology construction is an important field of study. The applications of ontology have a wide range, including enterprise integration, translation of natural language, mechanical engineering, E-Commerce, geographical information systems, legal information systems and biological information systems (Guarino, 1998). At present, most of the research on ontology construction depends on data classification, as determined by domain experts. It is difficult to modify the concepts of an extant domain ontology for a semantic web (Noy & McGuinness, 2001). However, since information increases and changes so rapidly, using manual methods to modify the structure of the ontology lacks flexibility. Many kinds of ontology representation methods exist, including directory, network, hierarchy, rule and frame. In this paper, we use the hierarchy structure to represent the domain ontology because it is the most general one (Liebig & Noppens, 2005). Ontologies may be constructed either by full manual construction and or semi-automatic construction, each with its own limitations and operating methods (Gillam, Tariq, & Ahmad, 2005; Maedche & Staab, 2001; Perez & Corcho, 2002). The full manual construction method is relatively partial to the constructor’s self-consciousness. On the other side, there are three major semi-automatical methods used to construct ontologies (Huhns & Singh, 1997; Khan & Luo, 2002; Motta, Shum, & Domingue, 2000; Philpot, Fleischman, & Hovy, 2003; Wang & Xu, 2000). They are the dictionary method, the correlation rules method, and the knowledge base method (Lammari & Metais, 2004). If the ontology is generated via the dictionary method, unrelated specific domains will result. Therefore, this method must be combined with other methods or experts to construct a meaningful domain ontology. However, this often limits by the dictionary size and quality. Nor is it suitable for data change. The correlation rule has a stronger degree of support and confidence, as well as extensive concept relations. In addition to using rules of correlations to calculate the concept layer, it also uses a rule of correlation to determine the relations between the concepts. When two concepts have a higher degree of support and confidence, the method returns to the original text to look for the term of the verb between the two concepts. This term can represent the relation between the two concepts. Constructing an ontology based on correlation rules is limited by the explication of rule bases. When we use correlation rules, some concepts may inadvertently be missed (Khan & Luo, 2002). Likewise, constructing an ontology based on a knowledge base is limited by the size of rules. Further, the knowledge base must be constructed in advance. A small set of knowledge base rules permits the formation of only a small ontology (Alani et al., 2003). In this paper, we will propose a novel automated method to construct an ontology. We collect useful terms from web pages and the term will be selected to construct an ontology is named ‘keyword’. The keyword has constructed on an ontology named ‘‘concept’’. The system selects the related web pages from the same domain, and then the system removes stop words. Stop words appear often in a document, cluttering word frequency analysis. After removal of the stop words, a term frequence and inverse document frequence (TF–IDF) calculation will be used to find the major keywords in this domain. Those keywords are transferred into duality data by a singular vector decomposition (SVD) operation, and then the data are clustered using a recursive adaptive resonance training (ART) network. Next, the system looks for the words of each group using web page content analysis. Finally, it assigns the levels of the keywords using Boolean operations and Is_A relation. The output of the ontology is in RDF format through a [package of Jena] (Michel, 2001). The remainder of the paper is organized as follows: Section 2 introduces the relevant technology, including the ART neural network, SVD, and Boolean operation. In Section 3, we describe how it generates an ontology. Section 4 specifies the system evaluation methods and primary experimental results. The paper makes conclusions and suggests directions for future research in Section 5. 2. Relevant technology Three major technologies, an SVD operation, an ART network, and a Boolean operation, are used in this paper. The three technologies are described below. 2.1. Single value decomposition (SVD) Traditional information retrieval matrix uses rows to represent documents and columns to represent keywords. The element of the matrix is the number of keywords in a document. If the two documents have many keywords in common, it indicates that the two documents are related. If two keywords appear in the same documents, the two keywords are related. Assume matrix X has t · d elements. The t dimensional space represents keywords and d dimensional space represents documents. Therefore, the similarity of two documents or keywords can be calculated by the cosine measure method. However, this method is unable to distinguish synonyms such as ‘‘car’’ and ‘‘automobile’’. Latent semantic analysis can solve this problem. In previously, we have proposed a latent semantic analysis to obtain the semantic feature (Chen & Hsieh, 2006). It projects documents and keywords onto a low dimension space in which a solution can be found. The X of SVD is defined as X = USVT. S = diag(r1 . . . rn), where the elements of S are all singular values of X. The singular value will then be represented by r1 P r2 P Á Á Á P rn P 0. U and V are d · d, t · t matrices, respectively. After processing by the SVD, X = USVT simplifies to X k 1⁄4 U k S k V T , as shown in Fig. 1. The dimensions k of Uk, Sk, V T reduce to d · k, k · k, and k · t. The common k element k is less than the original vector space. Sk retains k, a large singular value in the term-document. Uk denotes a document vector, while Vkis a term vector. For the termdocument matrix Xi of each category, we use the SVD to decompose Xi, obtaining three matrices U, S, V. Because we want to find the common semantic relation between different documents, we only process the document vector. For the singular value matrix, the top k singular value is selected. The top k singular value is most important for this data set, as it contains the latent semantic relationship. We add these latent semantic relations into each document vector for the same semantic document. 2.2. Adaptive resonance theory network (ART) The theory of ART grew from the theory of cognition (Kang, Huh, Lee, & Kim, 2000). First, the network obtains the training samples from the research domain, then the network learns how to form the inside clustering rules. After the training phrase, the network can classify new examples. If the example is not included within existing clusters, the network will generate a new cluster. The operation of ART is similar to a human neural system. Not only does it learn new examples, but also preserves old memories. An ART has the features of both stability and plasticity. In order to resolve the antinomy of stability and plasticity, the ART network adjusts the vigilance value. The structure of the ART network and the method of controlling vigilance test values are described below: (1) Input layer: The input data is training samples. The number of input vectors is determined by the question domain. (2) Output layer: This presents the results of the trained network. The number of output units increases as the number of training epochs increases. The training terminates when the number of output units arrives at a stable figure. (3) Weight connections: This connects the input layer and the output layer. Every connection between the input unit and the output unit has links running in two directions. The input and output units are complete connections. The two link directions, input units to output units and output units to input units, have different meanings. The link of input-to-output is represented by Wb, which can be used to calculate the value from input vector to one of unit of output layer. The output unit has a max corresponding value that will be used for the vigilance test. The values of Wb are real numbers between 0 and 1. The framework of the network is shown in Fig. 2. The corresponding value of the output unit ‘‘j’’ is described as follows: X Corresponding value : net1⁄2j 1⁄4 W b 1⁄2i1⁄2j Â X 1⁄2i i The weighted value of the output-to-input is Wt. The values of Wt are used to calculate the similar value of the input vector of trained examples connected to one unit of the output layer. The value of Wt is a real number between 0 and 1. The formula for a similar value of an output unit j is described as follows: 
 
 
 t 
 
W j Â X 
 P W t 1⁄2i1⁄2j Â X 1⁄2i Similar value : V j 1⁄4 1⁄4 i P X 1⁄2i kX k The relation of Wt and Wb is listed: Wb 1⁄4 ij W tij P 0:5 þ W tij The ART network operates the vigilance test based on the corresponding values. It performs a vigilance test for each output unit. In an ART network, if the output layer unit has a higher corresponding value, it will have a higher similar value, too. Hence, in an ART network, the number of output layer units will be affected by the vigilance test values. The higher vigilance value is due to incrementing of the number of output units. The vigilance value also controls the stability and plasticity of the ART network. The higher the vigilance value, the higher the plasticity is. Furthermore, as the vigilance value rises, the ART network will increase the number of clusters. Conversely, lower vigilance values will lead to fewer clusters. An ART network can provide plasticity and stability. Hence, we use this network to perform web pages clustering. 2.3. Boolean model The Boolean mode is also called duality mode. It is the simplest categorized way of calculating a document, and is based on set theory and Boolean algebra (Lammari & Metais, 2004). Generally, the Boolean mode can be divided into three relations: inheritance, intersection and independence, as shown in Fig. 3. If a concept B can be completely included by another concept A in its documents, we can say that concept B was totally included by concept A. So concept A can be considered a broader concept than concept B. Therefore, concept A will be located at a higher level than concept B on the concept relations map. If two concepts have a partial overlap on the documents they both belong to, they intersect. There is a correlation between these concepts. In that case concept A and concept B do not have any hierarchical relation, but instead concept A and concept B exist as sibling relations in the concept map. How many spaces are needed to overlay the two was solved with the fuzzy method. Finally, if a concept A and a concept B have no intersection whatsoever, they do not have any relations at all, and are considered independent. Hence, no relation is depicted between concept A and concept B on the concept map. The relations between different concepts are based on analysis of the documents the concepts belong to. The SVD operation is used to reduce the number of keywords and find the latent information. The ART network is used to do the keyword clustering. The Boolean operation is used to find the hierarchy between the keywords. 3. Constructing ontology The operations use to construct an ontology comprise six steps: (1) analyzing web pages, (2) finding the TF– IDF values of terms, (3) reducing the matrix and transfer elements to duality data, (4) using a recursive ART network to cluster the web pages, (5) applying a Boolean model to construct an ontology and (6) representing the ontology using a Jena package. The process is described in detail below. The framework is shown in Fig. 4. (1) Analyzing web pages First, the system removes stop words. At the moment, most web pages are written in HTML. HTML uses open/ closed tags to indicate web page commands, represented by ‘‘<’’ and ‘‘/>’’, respectively. Since content is not marked by these tags, the system then removes the HTML tags to reduce the analytical burden. The collected web pages of domain A are represented by SA={diji is an index of web pages}. di = {tij(A)jj is an index of domain terms}. Let tij(A) stand for one of the terms in domain A, i is an index of web pages and j is an index of the domain terms. Each term tij(A) will be found at least one time in one of the collected domain web pages. Likewise, each web page di contains at least one of the terms tij(A). However, merely computing the number of terms appearing in the web pages is not sufficient to represent the importance of the terms. We also consider the position of terms in the web pages when estimating the weight of the terms. Web pages are always organized by code segments between tags. For example, the tag <title></title> signals the presence of important words in a given web page. Finally, for different tags, we assign various weighting factors. Let Tij be the weight of term tij(A) in the web page di shown as follows: T ij 1⁄4 m X ntij ÂWt t1⁄41 !, m X ntij t1⁄41 ntij is the number of terms tij(A) in tag t. Wt is a weighting value of tag ‘‘t’’ where term tij(A) has appeared; the index i is the index of the domain web pages. After finding the weight of terms in the web pages, we will use TF–IDF to search for potential keywords and implicit keywords. (2) Finding term frequency–inverse document frequency (TF–IDF) In general, the importance of a keyword considers two factors in the collected documents, one is the relative frequency of the appearance of the keyword in the document, called term frequency (TF). Another is the quantity of documents containing the word, from all collected documents, named document frequency (DF). TF shows the relative frequency of a certain word appearing in a document. If a word appears at a higher frequency in a document, the system considers the word more important than other words in this document. Hence, the word is always used as the keyword of the document. DF shows the proportion of documents presenting this word among all documents. If the frequency of a keyword’s appearance is lower on other all documents than the current document, it means that this keyword is able to represent the current document better than other documents. We can thus use the product of TF and Inverse Document Frequency (TF_IDF) to represent the importance of a keyword in the document (Bhat, Oates, Shanbhag, & Nicholas, 2004; Hui & Yu, 2005; Shamsfard & Barforoush, 2004). The formula is as follows: X TF0ij 1⁄4 TFij Â T ij t TF IDFi;j 1⁄4 TF0i:j Â IDFi TF_IDFi,j represents weight, which is the importance of a keyword i in a document j. TFi.j represents relative frequency of keyword i in a document j, obtained by dividing the frequency of term i by the number of words in the document j. The value is between 0 and 1. TF0i:j is the term relative frequency of keyword i in a document j after weight operation: N IDFi 1⁄4 log ni N is the number of all documents, ni is the number of appearances of term i in the number of documents N. We use the values of TF–IDF to represent keywords and to form a keyword–document matrix. (3) Reducing the matrix and transferring elements to duality data We list out the keywords and web page documents to make a duality matrix. If the keyword appears in the documents, the keyword is set to 1; if not, it is set to 0. The SVD operation is used to reduce the large matrix to a small one, after which it can be sent to the recursive ART network for clustering. (4) Using the recursive ART network to cluster the web pages A recursive function is ‘‘a function call by itself.’’ Recursive methods can make the problem simple and easy to handle. We propose using a recursive ART network algorithm to produce a tree structure using an ART network to perform recursive clustering. For example, if there are total 100 terms, 50 terms have high similarity document indices, they will be clustered into one group. Meanwhile, another 20 terms have high similarity document indices will be clustered into another group. Finally, the remaining 30 terms it is clustered into still a group. We divide the clusters recursively using the same method, based on a threshold value on ART Net. If it is lower than this threshold value, the algorithm will not perform another iteration of the clustering process. The clustered results will form many groups. Each group contains a number of indices of web pages of terms. The system reverses the process to find a keyword to stand for each group, dubbed the candidate keyword of the group. After clustering, we calculate the TF–IDF values for each document group, and then use the keyword with the highest TF–IDF value to represent the candidate keyword of the group. Those candidate keywords are used to form the ontology. However, the system removes any duplicate keywords before they are used to generate the ontology. The system will merge the groups whose candidate keyword is the same into one group. The algorithm is described as follows: Algorithm: The Recursive ART Input: Duality data of an input matrix X Output: The clustering results of concepts Step 1. r = 1; Step 2. Initialization: set the number of output cluster Nout = 1; stack S is empty; W t 1⁄2i1⁄2N out 1⁄4 1; W b 1⁄2i1⁄2N out 1⁄4 1 ; 1þN z 1⁄4 1; Step 3. Read the input matrix X[i]; P Step 4. Calculate match value: net1⁄2j 1⁄4 i W b 1⁄2i1⁄2jÂ X 1⁄2i; I count 1⁄4 1; Step 5. Finding jÃ of net1⁄2jÃ 1⁄4 maxj net1⁄2j; kW t ÂX k Step 6. Calculate similar value: V jÃ 1⁄4 kjX k 1⁄4 P t Ã W 1⁄2i1⁄2j ÂX 1⁄2i i P ; X 1⁄2i Step 7. Test vigilance value (q) If V jÃ 6 q //check whether the other nodes have to be estimated; If Icount < Nout {Icount = Icount + 1; net[j*] = 0; Goto Step 5;} Else {Create a new cluster: Nout = Nout + 1; Update weight value: P W t 1⁄2i1⁄2N out 1⁄4 X 1⁄2i; W b 1⁄2i1⁄2N out 1⁄4 0:5þX 1⁄2i X 1⁄2i ; i Goto Step 3;} Ã > q // This means the input pattern matches If V j to the output node j*. Therefore, the j* node is the cluster for this pattern X. {Update weight: W t 1⁄2i1⁄2jÃ 1⁄4 W t 1⁄2i1⁄2jÃ Â X 1⁄2i; 1⁄4 W b 1⁄2i1⁄2jÃ W t 1⁄2i1⁄2jÃ Â X 1⁄2i P t ; 0:5 þ i W 1⁄2i1⁄2jÃ Â X 1⁄2i1⁄2jÃ Push the descent of Y[j] into stack S[r]; r ++;} Step 8. Pop S[r] If the elements of S[r] < 5 // at least five elements {A[z] = S[r]; z = z + 1; If S = empty Print A[z] // output the concept clustering map of tree structure Else go to Step 8;} Else Return Step 2; Step 9. End; (5) Applying Boolean operation The Boolean model is used to modulate and construct the relation between different concepts. In this paper, we assume that the ontology is constructed solely by an ‘‘Is_A’’ relation link. The ‘‘Is_A’’ link can judge the correlation between the upper and lower levels of the hierarchies of concepts, which is the relation between the hierarchy framework of the super-class and the sub-class. Upper-level concepts are more general, while lower-level concepts are sub-classes of the upper level. If X is the subset of G, and Y is the subset set of M, the system obtains X  G and Y  M. When a concept ‘y1’, found on all the documents of a given set, can encompass another concept ‘y2’, we say concept ‘y2’ is a sub-concept of concept ‘y1’ or ‘y1’ is an upper-level concept of concept ‘y2’. For example, imagine 10 documents involving four types of concepts (transports, flying, boats, and airplanes): Documents containing ‘‘transports’’: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Documents containing ‘‘fly’’: 2, 3, 6, 7, 9, 10. Documents containing ‘‘boat’’: 1, 4, 5, 8. Documents containing ‘‘airplane’’: 6, 7, 10. In these documents, ‘‘transports’’ is the upper-level concept of ‘‘fly’’ and ‘‘boat,’’ the parent concept. ‘‘airplane,’’ by the same token, is the child concept of ‘‘fly,’’ or its sub-concept. The system finds the complete concept stratum by calculating the stratum relations among different concepts. (6) Generating ontology through the Jena package A resource description framework (RDF) is a framework developed by W3C3 and metadata groups. It is able to carry several metadata while roaming on the Internet. Metadata is specific data about data describing web resources in the context of the RDF. In other words, an RDF may be used to describe the resources of a given web page. A given problem may be represented by a meaning graph of the RDF. Furthermore, the RDF accentuates the exchange and automation processing of web resources. A resource is the Unified Resource Identifier (URI), a string of web resources, or an element of XML. For example, the sentence: ‘‘John is the creator of the web pages at http://www.cyut.edu.tw/ ~s9214639/’’, is expressed by the RDF as a direction graph in Fig. 5 where the URL is a web resource and author is a property with the value ‘‘John’’. Finally, we use the W3C standard for ontology web languages to record our ontology. The system will output an ontology in a Jena package using an RDF format. 4. Experiment results The system was implemented on a PC Pentium 4 2.4 G with 512 M RAM, Java programming language and the RDF ontology language. The web pages are collected from the catalogues have already been classified by Google and DMOZ,4 separately. We use four types of tag weighting factors (Table 1) to modify the weighting of the terms. Two experiments were designed. The first experiment is semi-automatic. We select an expert music instrument ontology that an expert has constructed. We use the keywords of the existing domain ontology to produce a new ontology provided by our method. After the new ontology has been created, we compare the new ontology with the expert ontology to demonstrate the precision of the method. The second experiment involves the system producing a new domain ontology for beer, using our method, to evaluate the effectiveness of the method. 4.1. First experiment The number of web pages collected from the musical instrument domain was given in Table 2. The musical instrument domain has 36 catalogues consisting of 518 web pages by Google and DMOZ after filtering unrelated web pages. If the web page does not contain any concepts in its catalogue, it will be removed from the catalogue. For example, the web pages in the ‘‘Guitar’’ catalogue must contain at least one keyword ‘‘guitar’’. After removing the web pages without concepts, we collected 518 web pages. After web pages analysis stage, the column denotes terms, the row represents web documents. If the term can be found in the web document, it will be set to ‘1’, otherwise it will be set to ‘0’. After the SVD operation, the matrix values are inputted to the recursive ART network for clustering. The recursive ART network will check whether the output values are greater than the vigilance. We test the vigilance using a step 0.1 increment from 0.1 to 0.9 shown in Fig. 6. The vigilance of recursive ART network is used to balance the tension between the twin necessities of stability and plasticity, in order to locate superior representative tree models. After testing, the vigilance value 0.4 is used to form the recursive ART group, because a value of 0.4 yields the most groups containing clues that can facilitate keyword searching in the group. In Table 3, the clustering of the ART network results in 78 groups. Group 1 contains 364 web pages. The recursive ART algorithm leads to the appearance of individual web pages in more than one group (details of such scattering are listed in Appendix A). After clustering these web pages, we calculated the keywords TF–IDF values for each group, using the highest value to represent the keyword of the group. Each group generated a representative keyword, deleting identical representative keywords and then consolidating the groups leaving only 40 keywords listed in Table 4. In Fig. 7, we obtain a 5-level ontology from the 40 candidate nodes by Boolean logic level operations. This experiment is semi-automated. We use the concepts of the ontology that the expert has defined to search web pages from the Internet. Then we produce the ontology using our method from those web pages. After producing the ontology, we will compare this new ontology with expert-defined ontology. Precision and recall are then used to evaluate our ontology. The classification results of each category were listed by Table 5. In order to estimate the precision of the system, we defined two kinds of precision evaluation methods, as follows. Concept precision demonstrates the precision of the keywords the system selects; concept_location precision not only demonstrates the precision of the selected keywords but also shows the precision of the location in the hierarchy relations. The formula of concept precision, concept location precision, and recall are listed below: A AþB C PrecisionðC L P Þ 1⁄4 CþD A RecallðRÞ 1⁄4 AþE PrecisionðC P Þ 1⁄4 Fig. 8 shows the ontology that the expert has defined. We compare our generation ontology with it. The value of A is 40, reflecting the number keywords that the system has generated and the expert has defined. The value of B, the keywords that the system generates but the expert did not define, is 0. The concepts the system has produced, when compared with the expert-defined position, yield a value C = 29 (Fig. 7), indicating that the concepts and locations are correct. D, by contrast, presents concepts produced by the system, but which are in error relative to the concepts defined by the expert (D = 11). D is circled in Fig. 7. E represents concepts produced by the expert ontology, not the system (E = 12), and is represented by the double circle in Fig. 8. Based on the foregoing, the Precision(C_P) is 100%, the Precision(C_L_P) is nearly 73%, and the recall is just under 77%. The experimental results indicate that our proposed method has 100% concept precision and 73% concept hierarchy precision, based on a recall value of 77%, when compared with an ontology defined by an expert. 4.2. Second stage experiment In this experiment, no expert-defined ontology structure was used, so we evaluated the precision by domain experts. In the experiment, we selected the beer domain and collected web pages from the Internet. Web page content analysis was used to build a domain ontology but there was no corresponding domain ontology to compare it with. Table 6 shows the beer domain, consisting of 18 catalogues. Web pages that do not contain any concepts in the catalogues will be removed. For example, web pages in the ‘‘beer’’ catalogue must contain at least that one keyword ‘‘beer’’. After removing the web pages that do not contain the concepts, we collected 212 web pages. After filtering out stop words, 6914 terms were collected from the 212 documents. The system selected 1688 noun terms from the 6914 input terms. The system then calculated higher TF–IDF to obtain useful keywords from the filtered 1688 terms. Table 7, shows the higher TF–IDF value keywords for the beer domain. We also constructed a matrix in which the column denotes ontology keywords while the row represents web documents. If the keyword can be found in the web document, it will be set to ‘1’; otherwise, it will be set ‘0’. After the SVD operation, the matrix values were inputted to the recursive ART network for clustering. The recursive ART network then checked whether the output values exceed the vigilance value. We test the vigilance as discussed in the first stage above. A vigilance value of 0.3 is used to form the recursive ART group (Fig. 9), as that value results in many groups that have obvious differences between them. In Table 8, the clustering performed by recursive ART network yields 29 groups (details in Appendix B). After clustering, we calculated the TF–IDF values in each document group, and then used the keyword with the highest value to represent the concept of the group. In this manner each group generated a representative concept, but identical keywords were deleted and their groups consolidated, leaving only 13 keywords. The 13 keywords are Yeast, Beer, Malt, Association, Porter, Brewery, Hop, Mead, Award, Ale, Stout, Festival and Fermentation. Boolean logic is used to calculate relationships between levels of concepts. In Fig. 10, we obtain a 4-level ontology from the 13 candidate keywords. After producing the ontology, its precision must be evaluated. However, there was no another ontology to compare it with, so we invited 30 domain experts to evaluate its precision. However, the value for recall is not calculated, since it is difficult to know whether the collected web pages contain the keywords not selected by experts (Table 9): A AþB C PrecisionðC L P Þ 1⁄4 CþD PrecisionðC P Þ 1⁄4 After generating the ontology, we invited domain experts to evaluate our ontology. The average Precision(C_P) of 30 domain experts evaluate is 0.794 (almost 79%), and the average Precision(C_L_P) of 30 domain experts evaluate is 0.742 (almost 74%). Finally, we used the W3C standard for ontology web languages to record the ontology, and outputted the results in a Jena package using an RDF format (Fig. 11). The RDF is able to describe the resources of World Wide Web. We used the uniform resource identifier (URI) to describe the attributes and the relations between concepts and other resources. 5. Conclusions and future work An ontology can help a user to learn and search for relevant information. Rapid, accurate construction of an ontology has thus become an important topic for researchers working on content-based internet searching. This paper proposes a novel method that automates construction of the ontology. The method integrates the web page analysis, a TF–IDF calculation, an SVD operation, a recursive ART net, Boolean analysis and RDF technologies. Our ontology construction method does not require expenditure of time to select keywords and to define the relations by human edit or expert assistance. It can be used on the domain of semantic web. The method facilitates user understanding of the content of data and its relevancy, and is able to suggest content that is highly relevant. We designed two experiments to evaluate our system precision. The first compared a system-generated ontology to an extant domain ontology, while the other one was evaluated by the users. Performance was represented by precision, in two ways: first, the precision of a keyword as a concept in the domain ontology, and second, the precision of its location. The experimental results indicated that the Precision(C_P) keyword as a concept is 100%, the Precision(C_L_P) precision of its location is nearly 73%, and the recall is just under 77% in first experiments. In second experiments, the average Precision(C_P) of the method was 79% and the average precision of its location is 74%. However, this method still is based on the terms of web pages and it only constructs the ‘‘Is_A’’ relation between terms. In the future, we will propose a superior method for finding multi-relations between terms, and extend the system’s abilities to encompass a multi-field ontology as the foundation for a robust and accurate ontology construction methodology. 
A Survey of Ontology Learning Procedures. Abstract. Ontologies constitute an approach for knowledge representation that can be shared establishing a shared vocabulary for different applications and are also the backbone of the Semantic Web. Thus a fast and efficient ontology development is a requirement for the success of many knowledge based systems and for the Semantic Web itself. However, ontology development is a difficult and time consuming task. Ontology learning is an approach for the problem of knowledge acquisition bottleneck that aims at reducing the cost of ontology construction through the development of automatic methods for the extraction of knowledge about a specific domain and its representation in an ontology like structure. This paper provides a discussion on existing ontology learning techniques and the state of the art of the field. Key words: Ontologies; Ontology Learning; Machine Learning; Knowledge Acquisition 1 Introduction Ontologies constitute an approach for knowledge representation that is capable of expressing a set of entities and their relationships, constraints, axioms and the vocabulary of a given domain. An ontology should be machine readable in such a way that information systems are able to use them to represent and share the knowledge about the application domain. Ontologies hold a great importance to modern knowledge based systems. For instance, they constitute a powerful tool for supporting natural language processing [1][2][3], information filtering [4][5], information retrieval [6] and data access [7]. Besides, they also provide a formalism for specifying similarity measures which have presented good effectiveness [8][5][9]. One of the greatest application of ontologies is the Semantic Web [10][11], a new generation of the Web in which the semantic of documents, in most cases currently expressed only in natural language, would be expressed using ontologies. This way, the Semantic Web is an approach for enhancing the effectiveness of Web information access. However, the manual construction of ontologies is an expensive and timeconsuming task because the professionals required for this task (i.e. a domain 2 specialist and a knowledge engineer) usually are highly specialized. This difficulty in capturing the knowledge required by knowledge based systems is called ”knowledge acquisition bottleneck”. The fast and cheap ontology development is crucial for the success of knowledge based applications and the Semantic Web. An approach for this problem is to provide an automatic or semi-automatic support for ontology construction. This field of research is usually referred to as ontology learning [12][13][14]. This paper presents a survey of ontology learning techniques. Section 2 introduces the ontology concept as it is considered in this work. Section 3 discusses the overall process of ontology learning and some commonly cited approaches. The work on the evaluation of ontology learning procedures is discussed in Section 4. Section 5 concludes this paper with a final discussion on the approached topic. 2 Ontologies Before discussing the problem of ontology learning, we must first clarify what we mean by the term ”ontology”. There are a number of definitions and uses for this term and some of them can be found in [15]. For the sake of simplicity this paper uses the term ”ontology” according to a formal definition proposed in [16] and defined in this section. According to Russel and Norvig [17], the term ”ontology” is concerned with a theory about the existence. The Artificial Intelligence discipline considers ontologies as a formal specification of the concepts of an interest domain, where their relationships, constraints and axioms are expressed, thus defining a common vocabulary for sharing knowledge [18]. Indeed, what must be represented in a knowledge based system is what exists. This way, the definitions are complementary. An ontology is composed, on one hand, by concepts, taxonomic relationships (that define a concept hierarchy) and non taxonomic relationships between them and, on the other hand, by concept instances and assertions about them. An ontology must be formal and, therefore, machine readable. This way ontologies can provide a common vocabulary between various applications. This knowledge representation structure usually consists of a set of frame-based classes organized hierarchically describing a domain. It provides the skeleton of the knowledge base. The knowledge base uses the represented concepts to describe a dynamic reality and it changes according to the changes in the reality. However, the ontology only changes if there are changes in the described domain. More formally, an ontology can be defined, according to [16], as a tuple: θ := (C, R, H C , rel, Aθ ). (1) where: – C is the set of ontology concepts. The concepts represent the entities of the domain being modeled. They are designated by one or more natural language terms and are normally referenced inside the ontology by a unique identifier. 3 – H C ⊆ C × C is a set of taxonomic relationships between the concepts. Such relationships define the concept hierarchy. – R is the set of non-taxonomic relationships. The function rel : R → C × C maps the relation identifiers to the actual relationships. – Aθ is a set of axioms, usually formalized into some logic language. These axioms specify additional constraints on the ontology and can be used in ontology consistency checking and for inferring new knowledge from the ontology through some inference mechanism. Besides these elements, there are also the instances of the concepts and relationships, e.g. the instances of the elements of C, H C and R. A knowledge base is composed by an ontology θ and its instances. 3 Ontology Learning The term ontology learning refers to the automatic or semi-automatic support for the construction of an ontology θ, while the automatic or semi-automatic support for the instantiation of a given ontology is referred to as ontology population [19]. Ontology learning is concerned with knowledge discovery in different data sources and with its representation through an ontologic structure and, together with ontology population, constitutes an approach for automating the knowledge acquisition process. According to [20] there are two fundamental aspects on ontology learning. The first one is the availability of prior knowledge. In other words, wether the learning process is performed from scratch or some prior knowledge is available. Such prior knowledge is used in the construction of a first version of the ontology. Thus, a source of prior knowledge must demand little effort to be transformed into the first version of the ontology. This version is then extended automatically through learning procedures and manually by a knowledge engineer [21]. The other aspect is the type of input used by the learning process. Benz [20] defines three different kinds of input: – structured data: database schemes. – semi-structured data: dictionaries like WordNet [22]; – unstructured data: natural language text documents, like the majority of the HTML based webpages; Some approaches for ontology learning from structured data sources [23] and semi-structured ones [20] data have been proposed and presented good results. However, although they do provide some support to manual ontology development, the proposed methods for learning from unstructured data have not shown good results on the creation of semantic resources in a completely automated fashion [15]. Such methods are crucial to some areas. For instance, for the establishment of the Semantic Web, once most of the knowledge available on the Web is represented in natural language texts [24]. 4 Before establishing the tasks in the ontology learning process, one has to define the steps for ontology development, be it manual or automatic. Although there is no standard regarding this development process, Buitelaar et. al in [19] organizes the aspects and tasks involved in ontology development into a set of layers. Such layers are shown in Fig. 1. Fig. 1. Layers of the Ontology Development Process (adapted from [19]) According to the definition 1, an ontology consists of concepts, relationships between them and axioms. In order to identify the concepts of a domain, in first place, it is necessary to identify the natural language terms that refer to them. This task is specially important for ontology learning from free text. Synonym identification helps to avoid redundant concepts, since two or more natural language terms can represent the same concept. Another reason for identifying the terms is that, in the future, some of them can be used to uniquely identify their respective concepts. The terms are the source for identifying the concepts that will be part of the ontology, i.e. the set C of definition 1. The next step is to identify the taxonomic relationships (generalization and specialization) between the concepts. The product of this task is a set H C . It is also necessary to extract the non taxonomic relations, thus defining the set R and the function rel. Finally the extraction of the instances of the learned concepts and relationships takes place. Some authors [19] also consider rule acquisition for deriving facts that are not explicitly expressed in the ontology, that would constitute the set Aθ . Although, all of the ontology learning tasks are the same no matter what kind of input is used, there are specific issues associated with learning ontologies from each type of input data. Such issues regarding the ontology learning from structured, semi-structured and unstructured data are discussed, respectively, 5 in subsections 3.1, 3.2 and 3.3. Some examples of ontology learning applications are shown in subsection 3.4. 3.1 Ontology Learning from Structured Data These ontology learning procedures extract parts of the ontology using the available structural information. Examples of structured information sources are database schemas, existing ontologies and knowledge bases. The central problem in learning from structured data is to determine which pieces of structural information can provide relevant knowledge. For instance, a database schema may be used to identify ontology concepts and their relationships [25]. 3.2 Ontology Learning from Semi-structured Data Usually the quality of the results of ontology learning procedures using structural information is better than the ones using completely unstructured input data [21]. Unfortunately, most of the available knowledge is in the form of unstructured text. For this reason techniques for learning ontologies from semistructured data [26] (e.g. data composed by some structural information plus free text) have been developed. Examples of semi structured data are dictionaries, like WordNet [22] or the Wiktionary [27], HTML and XML documents, document type definitions (DTD), Wikis and User Tags. With the advent of the Web 2.0 and the Semantic Web, grows the interest for the development of procedures for extracting ontologies from semi-structured web documents [28] and from user tags [20]. 3.3 Ontology Learning from Unstructured Data Methods for extracting ontology parts from unstructured data are those that do not rely on any structural information for improving the quality of its results. They are important because unstructured data is the most available format for ontology learning input [21]. Unstructured documents consists of natural language texts such as Word, PDF documents or Web pages. For extracting knowledge from these sources, statistical and linguistic approaches are often used. One commonly accepted assumption in statistical approaches is the Harris distributional hypothesis [29]. This hypothesis states that similar words tend to occur in similar contexts. While statistical approaches often rely on word frequencies and word cooccurrence, linguistic approaches make use of natural language processing techniques, like syntactic, morpho-syntactic, lexico-syntactic and syntactic-semantic analysis, for extracting information from text [2]. There are also the pattern based approaches, which search the texts for certain patterns that indicate some kind of relation. One commonly cited approach is the Hearst patterns [30]. Hearst defined some lexico-syntactic patterns that indicate hyponymy/hypernymy relationships in the text. 6 Some ontology learning from text approaches uses clustering algorithms. In hierarchical clustering, term sets can be organized into a hierarchy that can be transformed directly into an ontology prototype. For this intent, the distance measure between terms has to be defined in such a way that it can be used as a criteria for merging terms [2]. An evaluation of the clustering methods for ontology learning from unstructured data can be found in [31]. In the first place, every clustering methods can be applied to various representation models, such as the vector space approach, associative networks or set theory based, as shown in [32]. Methods using distributional similarity can be divided into syntactic and window based approaches. Syntactic approaches use similarity considering the relationships between predicate and argument. Hindle [33] aims at finding similar nouns by comparing their behavior regarding the predicate argument structure. For each verb-subject and verb-object pair in their set of six million analyzed words, he computes the co-occurrence weights as being the mutual information existing on the pairs. An entire class of syntactic approaches is included in the Mo’k workbench [34], that provides a framework for defining term hierarchical clustering based on similarity and on limited contexts. There is another set of methods which produce paradigmatic relations (relations between the meaning of terms) as a mechanism for identifying the candidate terms without the need for syntactic pre-processing. A source for paradigmatic relationships is the second order co-occurrence, which does not rely on grammatical analysis. While the first order co-occurrence scores high pairs of words that appear together frequently in a certain text window, the second order cooccurrence scores high the ones that have similar first order co-occurrence [15]. 3.4 Ontology Learning Applications This subsection presents some examples of ontology learning applications. Text2Onto A tool for ontology learning from textual sources is the Text2Onto framework [35]. The Text2Onto is a framework which represents the learned knowledge into a meta level through modeling primitives from a model called probabilistic ontology model (POM). POM is a collection of modeling primitives independent from any ontology representation language. Such primitives are defined in the Modeling Primitives Library (MPL). The main modeling primitives are: – – – – – – concepts (Concepts C) concept inheritance (Taxonomic relationships H C ) concept instantiation (Instances) properties/relations (Non taxonomic relationships R) domain and range restrictions (Axioms Aθ ) mereological relations (Part-of relations R) 7 – equivalence Text2Onto uses data driven change discovery for algorithms for supporting automatic and semi-automatic adaptation of a given ontology according to changes in a data set and provides several algorithms for instantiating each modeling primitive from POM (which can be associated with the tasks of ontology learning shown in Fig. 1). WebKB The WebKB [28] is a research project that aims at creating a knowledge base with the knowledge contained in the World Wide Web. This represents an approach for ontology population from semi-structured and unstructured input data. This is achieved through a trainable information extraction system that instantiates a given knowledge base consisting of an ontology defining the classes and relations of interest, and optionally, instances of some of these classes and relations. WebKB uses logical and statistical learning algorithms for these tasks. DLLearner An approach for learning ontologies from structured data is proposed in [23]. This approach is based on inductive logic programming (ILP) [36]. An ILP algorithm aims at learning a logic program from examples and prior knowledge. Traditional ILP methods work usually with predicate logics for representing prior knowledge and the learned hypothesis. Description logics is the formalism in which the OWL (the W3C ontology recommendation for the Semantic Web) is based. A refinement operator for description logics is proposed in [23]. This refinement operator is used in a description logics based ILP procedure. The proposed algorithm learns a definition of a concept in description logics and takes as input a knowledge base and a training set with positive and negative examples of instances of the target concept. HASTI HASTI [37] is an automatic ontology building system, which uses as input unstructured data in the form of natural language texts in Persian. HASTI uses no prior knowledge, i.e. it builds ontologies from scratch. Its lexicon is nearly empty initially and will grow gradually by learning new words. The ontology in HASTI is a small kernel at the beginning. HASTI learns concepts, taxonomic and non-taxonomic conceptual relations, and axioms, to build ontologies upon the existing kernel. The learning approach in HASTI is a hybrid symbolic approach, a combination of linguistic, logical, template driven and semantic analysis methods. It performs online and offline clustering to organize its ontology. 4 Methods for the Evaluation of Ontology Learning Procedures Comparing techniques for learning ontologies is not a trivial task. For a given domain, there is not a unique possibility of conceptualization [38] and these 8 possibilities may differ in their usefullness but not in their soundness and justification [15]. Besides that, there is “no clear set of knowledge to be acquired” [39]. Although the evaluation of ontology learning procedures is still an open problem, there is already some work in this direction. In [40] two basic approaches for evaluating these systems are presented: the evaluation of the underlying learning methods and the evaluation of the learned ontology. However, because of the difficulty concerning the measurement of the correctness of the learning procedure, the former approach is less addressed. According to [41], the resulted ontologies can be compared by evaluating them in a running application, a posteriori evaluation by experts, or evaluation by comparison of learned results against a pre-defined gold standard. The automatically learned ontologies are useful to the extent they improve the effectiveness of the systems in which they are employed. Thus, the comparison of ontologies in a running application aims at evaluating the effectiveness of a system using the evaluated ontologies. For instance, a comparison between concept hierarchies in the context of a word sense disambiguation task is shown in [42] and in the context of text clustering is shown in [43]. Manual evaluation has advantages, since experts are supposed to know the concepts and their relationships in their domain of expertise and, therefore, they are supposedly able to tell wether a given ontology represents the domain or not. Although a posteriori evaluation by experts and their evaluation in a running application have their advantages, they also have their drawbacks. For instance manual ontology evaluation is subjective and time consuming. Besides that, these two methods are not feasible for large-scale evaluations [41]. Thus the comparison against a gold standard [44] is a plausible alternative. For a primary work on the comparison between two ontologies please refer to [45]. In [41], a framework for gold-standard based evaluation of ontology learning is introduced. However how can one say that an ontology is good enough for being a gold standard? The gold standard is a hand crafted ontology, developed by an error prone process. If the gold standard ontology presents modeling problems, the evaluation method rewards ontologies with similar problems and penalizes ontologies with concepts or relationships not appearing in the gold standard. 5 Conclusions This work presented a survey and a definition of the set of tasks known as ontology learning. For this intent, the ontology learning procedure was defined not as a single task, but as set of subtasks organized in the layers of Fig. 1. A definition of an ontology was provided, the tasks involved in the ontology learning process were identified and the most commonly cited approaches for this problem were introduced. Such approaches were classified according to the format of input data they use. Such data can be structured, semi-structured or unstructured. 9 Another fundamental difference between the cited approaches is the extent to which they automate the ontology development process. This can be observed when we compare which tasks are automated by each approach. This comparison is found in table 1. Table 1. Comparative table between some of the cited approaches DLLearner OL from Folksonomies Text2Onto HASTI Cluster Based Terms Synonyms C H C R Aθ X X X X X X X X X X X X X X X X X The field of ontology learning builds upon well founded methods from knowledge acquisition, machine learning and natural language processing. Although progress has been made over the last years, this field of research has not yet reached the goal of fully automating the ontology development process. It is possible to see that the most of the already cited approaches focus on concept identification and hierarchy extraction. Work on automatic learning of non taxonomic relationships have already been conducted but have not reached yet an advanced development stage. But, of the ontology learning subtasks, the one that is currently in the earlier stage of development is the axiom learning. Another great open question related to ontology learning is the evaluation of the proposed methods [41]. As shown in this paper there is already some work conducted in this direction but the establishment of formal, standard methods to evaluate ontology learning systems by proving their learning methods or proving the accuracy, efficiency and completeness of the built ontology is still an open problem [40]. Many of the ontology learning, methods discussed here address each one (or some) of the subtasks but not all of them in a coherent way. At last, another issue in this area is the development of methodologies which integrates methods for such a task. Because of the complexity of ontology development and the importance of the ontologies for the Semantic Web (given that the ontologies constitute its backbone), we believe that ontology learning will remain an active and central field of research, at least, over the next years. 
Ontology Learning from Text: A Survey of Methods 1 Introduction After the vision of the Semantic Web was broadcasted at the turn of the millennium, ontology became a synonym for the solution to many problems concerning the fact that computers do not understand human language: if there were an ontology and every document were marked up with it and we had agents that would understand the markup, then computers would finally be able to process our queries in a really sophisticated way. Some years later, the success of Google shows us that the vision has not come true, being hampered by the incredible amount of extra work required for the intellectual encoding of semantic mark-up – as compared to simply uploading an HTML page. To alleviate this acquisition bottleneck, the field of ontology learning has since emerged as an important sub-field of ontology engineering. It is widely accepted that ontologies can facilitate text understanding and automatic processing of textual resources. Moving from words to concepts not only mitigates data sparseness issues, but also promises appealing solutions to polysemy and homonymy by finding non-ambiguous concepts that may map to various realizations in – possibly ambiguous – words. Numerous applications using lexical-semantic databases like WordNet (Miller, 1990) and its non-English counterparts, e.g. EuroWordNet (Vossen, 1997) or CoreNet (Choi and Bae, 2004) demonstrate the utility of semantic resources for natural language processing. Learning semantic resources from text instead of manually creating them might be dangerous in terms of correctness, but has undeniable advantages: Creating resources for text processing from the texts to be processed will fit the semantic component neatly and directly to them, which will never be possible with general-purpose resources. Further, the cost per entry is greatly reduced, giving rise to much larger resources than an advocate of a manual approach could ever afford. On the other hand, none of the methods used today are good enough for creating semantic resources of any kind in a completely unsupervised fashion, albeit automatic methods can facilitate manual construction to a large extent. The term ontology is understood in a variety of ways and has been used in philosophy for many centuries. In contrast, the notion of ontology in the field of computer science is younger – but almost used as inconsistently, when it comes to the details of the definition. The intention of this essay is to give an overview of different methods that learn ontologies or ontology-like structures from unstructured text. Ontology learning from other sources, issues in description languages, ontology editors, ontology merging and ontology evolving transcend the scope of this article. Surveys on ontology learning from text and other sources can be found in Ding and Foo (2002) and Gómez-Pérez and Manzano-Macho (2003), for a survey of ontology learning from the Semantic Web perspective the reader is referred to Omelayenko (2001). Another goal of this essay is to clarify the notion of the term ontology not by defining it once and for all, but to illustrate the correspondences and differences of its usage. In the remainder of this section, the usage of ontology is illustrated very briefly in the field of philosophy as contrasted to computer science, where different types of ontologies can be identified. In section 2, a variety of methods for learning ontologies from unstructured text sources are classified and explained on a conceptual level. Section 3 deals with the evaluation of automatically generated ontologies and section 4 concludes. 1.1 Ontology in philosophy In philosophy, the term ontology refers to the study of existence. In this sense, the subject is already a central topic of Aristotle’s Categories and in all metaphysics. The term was introduced in the later Renaissance period, see Ritter and Gründer (1995), as “lat. philosophia de ente”. In the course of centuries, ontology was specified in different ways and covered various aspects of metaphysics. It was sometimes even used as a synonym for this field. Further, the distinction between ontology and theology was not at all times clear and began to emerge in the 17th century. For Leibniz, the subject of ontology is everything that can be recognized (germ. erkannt). Recognition (germ. Erkenntnis) as a basis of metaphysics is criticised by Kant, who restricts ontology to a propaedeutical element of metaphysics, containing the conditions and the most fundamental elements of all our recognition (germ. Erkenntniß) a priori. The relation of ontology to logic was introduced by Hegel and later strengthened by Husserl, who defends the objectivity of logical entities against subjectivation and replaces the notion of logical terms as psychical constructions with “ideal units” that exist a priori. Ontology in this context can be divided into two kinds: formal ontology that constitutes itself as a theory of all possible forms of theories, serving as science of sciences, and regional or material ontologies that are the a priori foundations of empirical sciences (Husserl, 1975). The latter notion paved the way to domain-specific ontologies, see section 1.2. For computer science, the most influential definition has been given by Quine (cf. Quine, 1969), who binds scientific theories to ontologies. As long as a theory holds (because it is fruitful), theoreticians perform an ontological commitment by accepting the a priori existence of objects necessary to prove it. A consequence of his famous quote “to be is to be the value of a bound variable” is: As long as scope and domain of quantified variables (objects) are not defined explicitly by an ontology, the meaning of a theory is fuzzy. Ontologies in the sense of Quine are the outcome of empirical theories, and hence they also need to be justified empirically. To subsume, ontology abstracts from the observable objects in the world and deals with underlying principles of existence as such. 1.2 Ontologies in Computer Science Ontology in computer science is understood not as general as in philosophy, because the perception of ontologies is influenced by application-based thinking. But still ontologies in computer science aim at explaining the world(s), however, instead of embracing the whole picture, they only focus on what is called a domain. A domain is, so to speak, the world as perceived by an application. Example: The application of a fridge is to keep its interior cold and that is reached by a cooling mechanism which is triggered by a thermostat. So the domain of the fridge consists only of the mechanism and the thermostat, not of the food in the fridge, and can be expressed formally in a fridge ontology. Whenever the application of the fridge is extended, e.g. to illuminate the interior when the door is opened, the fridge ontology has to be changed to meet the new requirements. So much about the fridge world. In real applications, domains are much more complicated and cannot be overseen at a glance. Ontologies in computer science are specifications of shared conceptualizations of a domain of interest that are shared by a group of people. Mostly, they build upon a hierarchical backbone and can be separated into two levels: upper ontologies and domain ontologies. Upper ontologies (or foundation ontologies), which describe the most general entities, contain very generic specifications and serve as a foundation for specializations. Two well-known upper ontologies are SUMO (Pease and Niles, 2002) and CyC (Lenat, 1995). Typical entries in upper ontologies are e.g. “entity”, “object” and “situation”, which subsume a large number of more specific concepts. Learning these upper levels of ontologies from text seems a very tedious, if not impossible task: The connections as expressed by upper ontologies consist of general world knowledge that is rather not acquired by language and is not explicitly lexicalized in texts. Domain ontologies, on the other hand, aim at describing a subject domain. Entities and relations of a specific domain are sometimes expressed directly in the texts belonging to it and can eventually be extracted. In this case, two facts are advantageous for learning the ontological structures from text: The more specialized the domain, the less is the influence of word sense ambiguity according to the “one sense per domain”-assumption in analogy to the “one sense per discourse”-assumption (Gale et al., 1993). Additionally, the less common-knowledge a fact is, the more likely it is to be mentioned in textual form. In the following section, distinctions between different kinds of ontologies and other ways of categorizing the world are drawn. 1.3 Types of Ontologies John Sowa (Sowa, 2003) classifies ontologies into three kinds. A formal ontology is a conceptualization whose categories are distinguished by axioms and definitions. They are stated in logic that can support complex inferences and computations. The knowledge representation community defines ontology in accordance as follows: “[An ontology is] a formal, explicit specification of a shared conceptualization. ‘Conceptualization’ refers to an abstract model of phenomena in the world by having identified the relevant concepts of those phenomena. ‘Explicit’ means that the type of concepts used, and the constraints on their use are explicitly defined. ‘Formal’ refers to the fact that the ontology should be machine readable. ‘Shared’ reflects that ontology should capture consensual knowledge accepted by the communities.” (Gruber, 1993; Ding and Foo, 2002) As opposed to this, categories in prototype-based ontologies are distinguished by typical instances or prototypes rather than by axioms and definitions in logic. Categories are formed by collecting instances extensionally rather than describing the set of all possible instances in an intensional way, and selecting the most typical members for description. For their selection, a similarity metric on instance terms has to be defined. The third kind of ontology are terminological ontologies that are partially specified by subtype-supertype relations and describe concepts by concept labels or synonyms rather than prototypical instances, but lack an axiomatic grounding. A well known example for a terminological ontology is WordNet (Miller, 1990). Figure (1) illustrates different ontology paradigms for a toy example food domain divided into vegetarian and non-vegetarian meals. All of these paradigms have their strengths and weaknesses. Formal ontologies directly induce an inference mechanism. Thus, properties of entities can be derived when needed. A drawback is the high effort of encoding and the danger of running into inconsistencies. Further, exact interference may become intractable in large formal ontologies. Terminological and prototype-based ontologies cannot be used in a straightforward way for inference, but are easier to construct and to maintain. A disadvantage of the prototype-based version is the absence of concept labels, which makes it impossible to answer queries like “Tell me kinds of cheese!”. Due to the absent labeling during construction, they are directly induced by term clustering and therefore easier to construct but less utilizable than their terminological counterparts. A distinction that causes confusion are the notions of taxonomy versus ontology, which are occasionally used in an interchangeable way. Taxonomies are collections of entities ordered by a classification scheme and usually arranged hierarchically. There is only one type of relation between entries, mostly the is-a or part-of relation. This corresponds to the notion of terminological ontologies. For formal ontologies, the concepts together with is-a relations form the taxonomic backbone of the ontology. Another kind of resource which is a stepping stone towards ontologies are thesauri like Roget’s Thesaurus (Roget, 1852) for English or Dornseiff (Dornseiff, 2004) for German. A thesaurus contains sets of related terms and thus resembles a prototype-based ontology. However, different relations are mixed: a thesaurus contains hierarchy relations amongst others, but they are not marked as such. 2 Learning Ontologies from unstructured Text Ontologies can be learnt from various sources, be it databases, structured and unstructured documents or even existing preliminaries like dictionaries, taxonomies and directories. Here, the focus is on acquisition of ontologies from unstructured text, a format that scores highest on availability but lowest on accessibility. Most approaches use only nouns as the bricks for ontology building and disregard any ontological relations between other word classes. To a large extent, the methods aim at constructing is-a-related concept hierarchies rather than full-fledged formal ontologies. Other subtype-supertype relations like partof are examined much less. One underlying assumption for learning semantic properties of words from unstructured text data is Harris’ distributional hypothesis (Harris, 1968), stating that similar words tend to occur in similar contexts. It gives rise to the calculation of paradigmatic relations (cf. Heyer et al., 2005), called ‘associations’ in de Saussure (1916). We shall see that the notion of context as well as the similarity metrics differs considerably amongst the approaches presented here. Another important clue is the use of patterns that explicitly grasp a certain relation between words. After the author who introduced patterns such as “X, Ys and other Zs” or “Ws such as X, Y and Z”, they are often referred to as Hearst-patterns (Hearst, 1992), originally used to extract is-a relations from an encyclopedia for the purpose of extending WordNet. Berland and Charniak (1999) use similar kinds of patterns to find instances of the part-of relation. As learning from text usually involves statistics and a corpus, using the world wide web either as additional resource or as the main source of information is often a possibility to avoid data sparseness as discussed in Keller et al. (2002) and carried out e.g. by Agirre et al. (2000) and Cimiano and Staab (2004). Ontology learning techniques can be divided in constructing ontologies from scratch and extending existent ontologies. The former comprises mostly clustering methods that will be described in section 2.1, the latter is a classification task and will be treated in section 2.2. Approximately, this is the distinction between unsupervised versus supervised methods, although we shall see that some clustering approaches involve supervision in intermediate steps. Section 2.3 summarizes research undertaken in semantic lexicon construction, which is a related task to ontology learning, although the representation of results might differ. In section 2.4, the view of ontology learning as an Information Extraction exercise is discussed. 2.1 Clustering for Ontology Learning In hierarchical clustering, sets of terms are organized in a hierarchy that can be transformed directly into a prototype-based ontology. For clustering, a distance measure on terms has to be defined that serves as the criterion for merging terms or clusters of terms. The same measure can be used – if desired – to compute the most typical instances of a concept as the ones closest to the centroid (the hypothetical ‘average’ instance of a set). Crucial to the success of this methodology is the selection of an appropriate measure of semantic distance and a suitable clustering algorithm. An overview of clustering methods for obtaining ontologies from different sources including free text can be found in Maedche and Staab (2004). In principle, all kinds of clustering methods – be it agglomerative or divisive – can be applied to all kinds of representations, be it vector space (Salton et al., 1975), associative networks (Heyer and Witschel, 2005) or set-theoretic approaches as presented in Cimiano et al. (2004). Here, the focus will be on just a few, illustrative methods. Methods using distributional similarity can be divided into syntactic and window-based approaches. Syntactic approaches make use of similarity regarding predicate-argument relations (i.e. verb-subject and verb-object relations), the usage of adjective modifiers or subjective predicates is rare. An early paper on semantic clustering is Hindle (1990), which aims at finding semantically similar nouns by comparing their behavior with respect to predicate-argument structures. For each verb-subject and verb-object pair in his parsed 6 million word corpus, he calculates co-occurrence weights as the mutual information within the pairs. Verb-wise similarity of two nouns is the minimum shared weight, and the similarity of two nouns is the sum of all verb-wise similarities. The exemplified analysis of this similarity measure exhibits mostly homogeneous clusters of nouns that act or are used in a common way. For obtaining noun hierarchies from text, Pereira et al. (1993) chose an encyclopedia as a well-suited textual resource for a divisive clustering approach based on verb-object relations, allowing the nouns to be members in multiple clusters. A whole class of syntactic approaches is subsumed in the Mo’K workbench (Bisson et al., 2000), which provides a framework to define hierarchical term clustering methods based on similarity in contexts limited to specific syntactic constructions. In the same work, comparative studies between different variants of this class are presented, including ASIUM (Faure and Nédellec, 1998; Dagan et al., 1994). Another paper on using selectional preferences is e.g. Wagner (2000). A different direction is using methods that produce paradigmatic relations as candidate extraction mechanism without syntactic pre-processing. A well-known source of paradigmatic relations is the calculation of second-order co-occurrences, which does not rely on parsing. While (first-order) co-occurrences rate pairs of word high that occur together often in a certain text window, second order co-occurrences are words that have similar distributions of first-order co-occurrences (see e.g. Ruge (1992), Schütze (1998), Rapp (2002), Biemann et al. (2004) – this corresponds roughly to Rieger’s δ-abstraction (Rieger, 1981; Leopold, 2005)). The context definition of these methods is mostly not restricted to any syntactic construction, which introduces more noise but keeps the method language-independent. It can be argued that given a sufficient corpus size, equal results to syntactically aided methods might be achieved, see e.g. Pantel et al. (2004). However, as the underlying bag-of-words simplification of window-based methods abstracts from the order of the words, no clues for the relation between candidate pairs can be drawn directly from this data, making these approaches on their own not viable for the construction of ontologies from scratch. While there does not seem to be an alternative to use patterns in order to alleviate the labeling problem, the action of naming super-concepts is not necessary when aiming at a prototypical ontology, such as in Paaß et al. (2004): here, a hierarchical extension to Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) is introduced. PLSA (like LSA – (cf. Deerwester et al., 1990)) assumes latent concepts, which are playing the role of an intermediate concept layer: the probability of seeing a word w in a document d is the sum of the product probabilities of d belonging to concepts c and w being generated when c is present. To introduce hierarchical dependencies, the probability mass is split between sub- and super-concepts. In an experiment, a fixed 4-level hierarchy with 1, 14, 28 and 56 nodes on the levels was defined. The words with the highest probability per concept constitute the entries of the prototypical ontology. While results look impressive, a clear drawback is the predefined structure of the hierarchy. The other possibility is to use explicit clues, like Hearst-patterns. Caraballo (1999) constructs a terminological ontology from text in the following way: noun candidates from a newspaper corpus are obtained by considering conjunction and appositive data. For all nouns, a co-occurrence matrix is set up. Similarity between two nouns is calculated by computing the cosine between their respective vectors and used for hierarchical bottom-up clustering. For labelling this hierarchy in a post-processing step, Hearst-patterns are used for finding hypernym candidates, which are placed as common parent nodes for clusters, if appropriate. Evaluated by human judgement, the method performs at about 35-55% precision. A similar approach is presented by Cimiano and Staab (2005) who also cluster nouns based on distributional similarity and use Hearst-patterns, WordNet and patterns on the web as a hypernym oracle for constructing a hierarchy. Unlike as in Caraballo (1999), the hypernym sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy. The resulting taxonomy outperforms Caraballo’s when evaluating the outcome against a reference ontology (see section 3). Methods based on extraction patterns 2.2 OL as a classification task Given an existing ontology, its extension can be viewed as a classification task: features of the existing data are used as a training set for Machine Learning, which produces a classifier for previously unknown instances. One possibility is to utilize the hierarchical structure in a decision tree, as proposed in Alfonseca and Manandhar (2002). When inserting new concepts, it is tested whether they fit best to the actual node or one of the daughter nodes. The tree is traversed top-down from the root until an appropriate position is found. The largest problem here is the general nature of top-level concepts that leads to taking the wrong path in the beginning of the process, which can be alleviated by propagating the signatures of lower-level concepts one step upwards. For around 1200 concepts, an accuracy of about 28% is reported. A related approach is Witschel (2005), which substitutes the syntactic dependencies for similarity by comparing words only on sentence-based co-occurrences. A small sub-tree of an existing WordNet-like hierarchy is used as training and test data. Propagating the semantic descriptions iteratively upwards to the root, the approach is biased towards putting new words into larger sub-trees. While Witschel’s results are better, this might be due to the smaller number of concept classes. In Fleischman and Hovy (2002), only eight categories for named entities denoting persons are considered. They examine five machine learning approaches on features based on preceding and following word N-grams which are combined into concepts using WordNet, reporting 70% accuracy. Placing words into WordNet where the concentration of words with similar distributional characteristics is highest is conducted by Widdows (2003). He arrives at about 80% precision for common nouns, 34% for proper nouns and 65% for verbs. How to enlarge WordNet by assigning appropriate named entities to the leaves using the Google index is discussed in Pa ̧ ca (2005). s 2.3 Ontology Learning as Semantic Lexicon Construction The similarities between the construction of semantic lexicons and lexical ontologies – be it terminological or prototype-based – are striking. Both encode semantic similarities between terms and they abstract terms to concepts. Whereas semantic lexicons often attach semantic categories to words and do not structure the set of words internally any further (although semantic lexicons like e.g. HaGenLex (Helbig, 2001) are organized in a flat hierarchy of categories), ontologies aim at explaining all possible relations between concepts, being more fine-grained. Nevertheless, words with the same semantic label should be found in the same region of the ontology, which makes part of the methodology for automatic construction applicable to both tasks. Let us assume that we have a small semantic lexicon, given by a set of categories, which are formed each by a set of words. Using a text corpus, we want to extend this lexicon. Mostly, bootstrapping approaches have been used to tackle this problem. On the one hand, because bootstrapping can iteratively use previously learnt examples, which reduces the minimal size of the seed lexicon. On the other hand it does not necessarily need negative examples for learning, making the procedure viable for learning single semantic categories. The largest problem that bootstrapping methods have to face is errorpropagation: misclassified items will lead to the acquisition of even more misclassified items. Various attempts have been made to minimize this thread. In general, bootstrapping starts with a small set of seeds as current category. For every candidate item, the similarity to the current category is computed and the most similar candidates are added to the current category. These steps are conducted iteratively until a stopping criterion holds; sometimes the process is deliberately stopped after about 50-200 iterations. Riloff and Shepherd (1997) were the first to apply bootstrapping for building semantic lexicons, extending one category at a time. Their context definition is one noun to the left and one noun to the right for head nouns in sentences. Collecting the contexts of the current category set, they calculate a score for each word by checking the relative frequency of the word appearing in the category’s contexts. Amongst the first 100 words retrieved by the algorithm for categories of a seed size around 50, about 25% were judged correct by human decision. In Riloff and Jones (1999) not only classes are assigned to words, but also the confidence of contexts supporting a class is estimated. Contexts in this work are patterns such as “headquartered in <x>” or “to occupy <x>”. Moreover, only the top 5 candidates are added to the knowledge base per step, alleviating error-propagation to a precision of about 46%-76% after 50 iterations. Further improvement was gained in Thelen and Riloff (2002), where multiple categories are learned at the same time to avoid too large single categories consisting of a mixture with several other categories. In that way, about 80% accuracy for the first couple of hundred new words can be reached. This complies well with the structuralist notion of semantics being defined in a negative way (de Saussure, 1916; Eco, 1977): A category “grows” in the space of meaning as long as it meets the border of another category. Building multiple categories simultaneously is also used in Biemann and Osswald (2005), who extend a semantic lexicon for the use of semantic parsing. As contexts, only modifying adjectives of nouns are taken into account. Semantic classes of known nouns are inherited via the modifying adjectives to previously unclassified nouns. In experiments using a co-occurrence significance measure to consider merely typical modifiers, the authors report to succeed in doubling their resource of 6000 nouns in 50 categories with an accuracy of about 80%. As opposed to these shallow approaches, Roark and Charniak (1998) look for words occurring together in syntactical formations that involve full parsing of the corpus. A radical break with syntactical pre-processing is conducted in Biemann et al. (2004), where a lexical-semantic resource is extended without using any tagging or parsing, merely by using sentence-based co-occurrence statistics. A word is added to a category if many words of the category occur with it within a sentence window. While scores are differing strongly for selected categories, the approach serves as a language-independent baseline. 2.4 Information Extraction for Ontology Population In Information Extraction (IE, see Grishman (1997) for a survey), templates containing roles, relations, temporal and time information to describe possible situations are encoded. The task is to fill the templates’ slots by extracting relevant data from documents. IE proceeds in a situative way: instantiated templates are attached to the texts from which they have been extracted. Ontologies, on the other hand, encode conceptualizations that are not bound to specific text snippets but apply in general. Nevertheless, templates can be defined in IE systems like GATE (Bontcheva et al., 2004) and the standard IE extraction mechanisms can be employed to fill these templates, producing eventually more powerful and flexible extraction rules than the patterns mentioned before. IE systems are historically suited for the extraction of named entities. This is why they are mainly used to find instances of concepts (like chocolate companies) and relations (like employer – employee) rather than the concepts themselves: they can be better used for populating than for constructing ontologies. After collecting all the situative template instantiations, pruning has to be applied to keep only relations that occur frequently and with high confidence. In Brin (1998), the DIPRE system is laid out that bootstraps the author-of relation between writers and book titles by automatically creating extraction patterns that heavily rely on HTML-tags, but also use clues from unformatted text. Using a DIPRE-like architecture, the SNOWBALL system (Agichtein and Gravano, 2000) learns patterns for free text that has been tagged by a named entity recognizer and uses them to extract instances similar to a few user-provided example tuples, never attempting to extract all the information from each document. For example, the SNOWBALL-pattern “<LOCATION>-based <ORGANISATION>” extracts headquarters of companies with high precision. Sufficient recall is ensured by using a large corpus. 3 Evaluation As ontology learning just emerged recently as a field of its own, there are not many gold standards that could be used for evaluation. Further, the desired result of ontology learning is not a simple list with binary classifications, but a far more complicated structure. To make it even worse, there is “no clear set of knowledge-to-be-acquired” (Brewster et al., 2004), not even for very specialized domains. As Smith (2004) claims, there are several possibilities of conceptualizations for one domain that might differ in their usefulness for different groups of people, but not in their soundness and justification. So even if the outcome of an algorithm does not compare well with a manually built ontology, how can its quality be judged? Of course, there is always the option of manual evaluation, with its well-known drawbacks of being subjective and time-consuming. For complicated tasks like ontology learning, a comparably low inter-annotator agreement can be expected, which in turn means that several annotators have to judge the results to arrive at consistent figures. But maybe it is not the ontology itself that is in the focus of interest, but its application. Learning ontologies is a goal of its own, but ontologies are usually just a resource that should improve performance on NLP tasks. Measuring improvements of ontology-supported approaches depicts best the gain for the application in focus, but it unfortunately does not provide direct scores for the ontology itself. In the remainder of this section, several possibilities to conduct an automatic evaluation on ontologies are discussed. 3.1 Evaluation against a Gold Standard The question on how to compare two taxonomies or ontologies is first dealt with in Maedche and Staab (2002), who show ways to compare them on lexical and on conceptual level. For the lexical level, they measure the lexical overlap between the concept names in a variant-robust way. For comparing the taxonomic backbones of two ontologies, the notion of semantic cotopy is introduced. Semantic cotopy of a concept is the union of all its sub- and super-concepts, approximating its intensional semantics. The averaged taxonomical similarity is determined by the maximal overlap of semantic cotopies. Further, the authors provide ways to compare the relations of two ontologies and evaluate their measures by an empirical study, using the tourism ontology developed within the GETESS project (Staab et al., 1999). When aiming at taxonomy relations, it is possible to compare results of an algorithm with lexical-semantic nets like WordNet, as employed by e.g. Wagner (2000) and Witschel (2005). Yet, whenever a relation is not found in the gold standard, the algorithm might be wrong or the gold standard might be incomplete. This even holds for large resources – Roark and Charniak (1998) report that 60% of the terms generated by their semantic class learner could not be found in WordNet. In Brewster et al. (2004) a comparison of ontologies with automatically extracted keywords from text corpora is proposed. The method measures lexical overlap as a score of how much the ontology fits the texts, but merely in a bag-of-words fashion, disregarding internal structure. 3.2 Application-based Evaluation Recent years saw an increasing amount of research using WordNet to improve any kind of NLP application. The bulk of these applications can in turn be used for evaluating automatically created semantic resources. In the following paragraphs, setups for an application-based evaluation of ontologies are discussed. Document similarity is usually measured by comparison of document vectors in a vector space (Salton et al., 1975), where each dimension in the space represents one term. Ambiguity and variability of natural language might cause several concepts to be mapped onto one dimension and several dimensions to be used for one concept, resulting in spurious similarity scores. This is the main motivation to use LSA (Deerwester et al., 1990), which reduces the number of dimensions by just considering main components as determined by singular value decomposition. But LSA has a number of drawbacks, including bad scalability and black-box-like latent concepts. With a domain-specific ontology, terms that are found in or around the same concept can be mapped into one dimension. On the other hand, terms that are present in many concepts due to their semantic ambiguity can be excluded or disambiguated, see next paragraph. The clustering induced by the similarity measure can be compared to pre-categorized collections such as the Reuters corpus (Reuters Corpus, 2000). It is also possible to train a classifier and compare its performance between presence and absence of ontology information. Evaluation using document similarity will favor ontologies that keep similar terms in similar places, possibly in a flat and not very fine-grained hierarchy. In Heinrich et al. (2005), two latent concept methods for constructing a prototype-based ontology are compared by measuring their effects on document clustering. As latent methods include the notion of a document into their models and can be applied to cluster words as well as documents, the choice seems natural. The ontology is used for dimensionality reduction in document clustering, which is compared to a gold standard. The task of word sense disambiguation (WSD) is to choose the appropriate sense for ambiguous words from a predefined inventory of senses. For English, WSD methods are usually evaluated on the SENSEVAL corpora (Kilgarriff, 1998), using WordNet as sense dictionary. Senses are assigned according to the ambiguous words’ contexts: Either contexts are compared to glosses and terms close to the different concepts in WordNet (unsupervised WSD) or to context profiles per sense acquired from a training corpus (supervised WSD). Using WSD for the evaluation will favour ontologies that distinguish well between the different senses of words. WSD was successfully supported by semantic resources obtained from large corpora by Gliozzo et al. (2005), where terms are mapped to domains using LSA with a large number of dimensions. Word sense disambiguation Information Retrieval and Question Answering After various attempts to use query expansion methods in order to provide better coverage for information retrieval (see e.g. Ruge (1992); Stamou and Christodoulakis (2005)), this direction to improve information retrieval has been largely abandoned as it usually decreases precision too much without considerably improving recall. A possible reason is that users actually look for what they have been typing in and not for hypernyms or even synonyms. Another problem is the lack of disambiguation clues in the query which causes the query expansion mechanism to over-generate even worse. But ontologies can be used in other parts of the retrieval process. Taxonomies that are build automatically from web data are used by Sánchez and Moreno (2005) to group query results returned by a search engine. In this case, the user’s behavior of accepting or rejecting the interface is the instance of judgement. Improving question answering by overcoming the shortfalls of the bag-of-words model is the objective of e.g. Leveling and Hartrumpf (2005). Here, a semantic lexicon forms the background knowledge for semantic parsing, which yields a semantic representation much more precise than simply considering presence or absence of terms. Extending the lexicon as described in section 2.3 should result in higher performance. Using Information Retrieval and Question Answering tasks for evaluation will promote ontologies with high coverage, as these applications are usually tested in a generic rather than in a domain-specific setting. The goal of co-reference resolution is to detect words that form a referent chain in a text. These chains mostly consist of pronouns, but also synonyms, hypernyms and part-whole related terms can refer to a previously mentioned entity. Co-reference resolution can be viewed as the classification task of finding the right antecedent for a referent using e.g. grammatical, contextual and morphological features. The evaluation framework for English co-reference resolution, which is not an application itself but rather a pre-processing step for methods like summarization, abstracting and information extraction, are the MUC-6 and MUC-7 corpora (Chinchor, 1998). The use of semantic resources, however, is scarcely encountered for co-reference or anaphora resolution. An exception is Hoste (2005), where WordNet and the Dutch part of EuroWordNet are used for additional features, which bring about only a small gain because of lacking coverage. At first glance, it seems that ontologies can only support co-reference resolution in the rare cases of nouns referring to other nouns that are semantically related, but not in the default case of pronouns referring back to noun phrases. But there is the possibility of using the semantic role of the pronoun to find antecedents that are compatible, e.g. as subject or object of the pronoun’s sentence’s predicate, as pointed out by Johansson et al. (2005). As there is plenty of room for improvement in co-reference and anaphora resolution, this might be a suitable task to evaluate ontologies that encode semantic roles additionally to hierarchical relations. 4 Conclusion After clarifying the usage of the term ontology, a variety of methods have been described how to construct and extend ontologies using unstructured text sources. We have then been looking at approaches that are directly labeled with ontology learning, complemented by a consideration of earlier work that has similar goals despite differing terminology. Further, various scenarios for ontology evaluation have been conveyed. Currently, ontology learning cannot fulfill the promises that its name suggests. As far as prototype-based ontologies are concerned, clustering might yield more or less semantically coherent sets of words, but will not be of great help for carrying out the crucial step from terms to concepts. Taxonomical ontologies can be learnt as far as the relations are explicitly mentioned in the text and extractable by patterns that are scarcely met in real life texts. For circumventing the problem of possible pattern mismatches (i.e. “life is a highway”) even more text has to be considered, resulting in very small taxonomies as opposed to the size of the corpus, as pointed out by Brewster et al. (2005). Especially when comparing the requirements for formal ontologies as formulated by the Semantic Web community and the structures learnable from text as described, one has to state that the ‘self-annotating web’ will remain a vision for a long time. But maybe the task is ill-defined. It is beyond doubt that modeling semantics will carry natural language processing further, as it has reached a state where further improvement of systems would in fact need rather more language understanding than more rules or more training examples. It is an open question, however, whether formal specifications are the only way to reach the goal, or whether the manual approach of hand-coding semantics will be outperformed by inconsistent, statistical black-box methods again. 5 Acknowledgements The author would like to thank Gerhard Heyer and Christer Johansson for useful comments. This work was partially carried out at MULTILINGUA, University of Bergen, supported by the European Commission under the Marie Curie actions. 
A Description of the State-of-the-Art,Applications,Challenges and Trends for Ontology Learning from Text. Ontologies are nowadays used for many applications requiring data, services and resources in general to be interoperable and machine understandable. Such applications are for example web service discovery and composition, information integration across databases, intelligent search, etc. The general idea is that data and services are semantically described with respect to ontologies, which are formal specifications of a domain of interest, and can thus be shared and reused in a way such that the shared meaning specified by the ontology remains formally the same across different parties and applications. As the cost of creating ontologies is relatively high, different proposals have emerged for learning ontologies from structured and unstructured resources. In this article we examine the maturity of techniques for ontology learning from textual resources, addressing the question whether the state-of-the-art is mature enough to produce ontologies ‘on demand’. Lernen von Ontologien aus Texten: Stand der Technik, Anwendungen, Herausforderungen und Trends Ontologien spielen heutzutage eine wichtige Rolle in Anwendungen, die auf die Interoperabilität und Maschinenverständlichkeit von Daten, Diensten und Ressourcen im allgemeinen bauen. Solche Anwendungen findet man z.B. in den Bereichen der Suche und Komposition von Diensten, Integration von Informationen aus verschiedenen Datenbanken und intelligente Suche von Ressourcen. Die generelle Idee ist dabei, dass Daten und Dienste, die semantisch in Bezug zu einer Ontologie beschrieben werden, über verschiedenen Anwendungen und Parteien hinweg mit einer wohldefinierten und auf Konsens beruhenden Bedeutung verwendet werden können, die von der Ontologie formal spezifiziert wird. Da die Erstellung von Ontologien typischerweise relativ kostspielig ist, sind in der Vergangenheit Verfahren zum automatischen Lernen von Ontologien aus strukturierten und unstrukturierten Ressourcen vorgeschlagen worden. In diesem Artikel wird der Reifegrad von Techniken zum Lernen von Ontologien aus textuellen Quellen analysiert, besonders im Hinblick auf die Frage, ob mit solchen Verfahren Ontologien ‚auf Knopfdruck’ gelernt werden können. 1 Introduction The aim of the Semantic Web as originally envisioned by Tim BernersLee and others [Berners-Lee et al. 2001] is to add a layer of meaning on top of data, services and resources in general to enforce their interoperability and enable machine interpretability. This is especially important for applications and scenarios in which data needs to be shared in a way such that their meaning is preserved. The data, services and resources are then described semantically via metadata describing their meaning or capabilities. These metadata are captured with respect to ontologies, which are logical theories and thus have a formal logical interpretation which is independent of specific applications. Ontologies can then be formalized in different ontology languages such as the W3C Standard OWL1. If data is annotated with metadata specified with respect to an ontology, it can not only be shared between different parties in a meaning-preserving way, but it can be searched and retrieved in a more effective way. If services are annotated semantically, the search for the appropriate service can be made more effective, and different services can be orchestrated and composed on the basis of their semantic descriptions to achieve a more complex goal. A crucial question in the vision of a semantic web is how to support and ease the process of creating and maintaining ontologies. By definition, ontologies represent a shared conceptualization of a domain of interest and should thus be jointly engineered by a number of parties (see the ontology definition in [Gruber 1995]). As any engineering process, this involves a high effort and some clearly defined methodology consisting at least of a feasibility study, a requirement analysis phase, a conceptualization and formalization phase as well as an application, evaluation and refinement phase (compare Figure 1). It is well known that the process of engineering an ontology is costly. Recently, models for estimating the actual costs have been proposed (compare [Paslaru Bontas et al. 2006]). In order to alleviate the costs involved in the activity of engineering ontologies, several proposals for automatically learning ontologies from data have emerged. In particular, in recent years there has been a great surge of interest in methods for learning ontologies from textual resources, which are still the main means of knowledge externalization used by people. In this article we will look at the status of maturity of different techniques for learning ontologies from text, examining the maturity of different methods for learning the most important ontological primitives. More importantly, we also discuss potential applications for ontology learning techniques and conclude the article with a reflection on the main challenges and trends within ontology learning. The structure of the article is as follows: in the following section 2 we introduce ontologies as well as ontology engineering, highlighting the role of ontology learning for this purpose. In section 3, we review the state-of-the-art methods applied for learning ontologies from text data and discuss applications for ontology learning in section 4. Before concluding, we summarize the main challenges and trends in the field of ontology learning. 2 Ontologies and Ontology Engineering Ontologies are typically defined as an abstract model of a domain of interest with a formal semantics in the sense that they constitute a logical theory. These models are supposed to represent a shared conceptualization of a domain as they are assumed to reflect the agreement of a certain community or group of people (see [Gruber 1995]). In the simplest case, ontologies consist of a set of concepts or classes2 which are relevant for the domain of interest as well as a set of relations defined on these concepts. Typically, one distinguishes between taxonomic and nontaxonomic relations. Taxonomic or subclass relations establish a hierarchical order between concepts, which is defined semantically by set inclusion of the members of a class or concept. Non-taxonomic relations are other relations which are defined on concepts but do not serve the purpose of ordering the concepts hierarchically. The concepts and relations can be axiomatically defined by specifying additional properties such as transitivity or cardinality like in database systems. Ideally, ontologies should be abstract enough in order to be reused across different applications. The crucial question certainly is how such ontologies can be built. The process of creating an ontology is actually an engineering activity the outcome of which is an ontology which represents a shared conceptualization of the people involved in the process. This process is typically referred to as ontology engineering. Though ontology engineering has been more an art than a science in the past, the Semantic Web community has spent great effort to turn it into the latter by proposing different methodologies and carefully analyzing them. Most of the ontology engineering methodologies distinguish at least the following phases: feasibility study, requirements analysis, conzeptualization and finally deployment, which typically consist in a loop of application, evaluation and maintenance of the ontology (see Figure 1). These phases are sometimes called differently and sometimes partitioned into subphases. Conzeptualization can be for example separated in at least three subtasks: i) development of the domain model, ii) formalization of the model and iii) its implementation in a certain ontology language (see [Pinto & Martins 2004]). Ontology learning techniques can for example be applied in the conceptualization phase in order to learn a first ‘kick-off’ model which people can use as a basis for discussion. Once this model is formalized and implemented in a formal ontology language, ontology learning techniques can be applied in the maintenance phase to extend, refine or modify the model. 3 State-of-the-Art in Ontology Learning In this section we discuss the state-of-theart in ontology learning by analyzing the most prominent methods applied to learn certain ontology primitives. In particular, we discuss the different methods with respect to the following tasks: I Extracting the relevant domain terminology and synonyms from a text collection. I Discovering concepts which can be regarded as abstractions of human thought. I Deriving a concept hierarchy organizing these concepts. I Extending an existing concept hierarchy with new concepts. I Learning non-taxonomic relations between concepts. I Populating the ontology with instances of relations and concepts. I Discovering other axiomatic relationships or rules involving concepts and relations. Table 1 shows different organizations and their ontology learning tools together with the different tasks addressed. We now proceed to discuss the state-of-theart techniques with respect to the above mentioned tasks, grouping certain tasks wherever appropriate. 3.1 Extracting the Relevant Terminology and Discovery of Synonyms Classes are the building block of an ontology. In ontology learning, typically the assumption is made that some terms unambiguously refer to a domain-specific concept. Thus, extracting the relevant domain terminology from a text collection is a crucial step within ontology learning. Methods for term extraction can be as simple as counting raw frequency of terms, applying information retrieval methods such as TFIDF (see [Baeza-Yates & Ribeiro-Neto 1999]) or applying sophisticated methods such as the C-value / NC-value method (see [Frantzi & Ananiadou 1999]). In any case, the resulting list of relevant terms will for sure need to be filtered by a domain expert. In order to detect synonyms, the most common approaches either apply clustering techniques to group similar words together or use some association measure to detect pairs of statistically correlated terms (compare [Manning & Schütze 1999]). The detection of synonyms can help to cluster terms to groups of terms sharing (almost) the same meaning, thus representing ontological classes. In general, methods for extracting terms and synonyms have gained a reasonable maturity. Synonym extraction methods, for example, have been shown to achieve nearly human-like results on the TOEFL synonym task (compare [Turney 2001]). 3.2 Learning Concepts and Concept Hierarchies The backbone of any ontology is constituted by a set of taxonomic relationships between classes. Each of the classes can be defined intentionally, e.g. by a descriptive label or its relationships to other classes, as well as extensionally by specifying a set of instances belonging to this class. Since the core taxonomy of an ontology, independently of the underlying ontology representation language, is of crucial importance for the use of ontologies as a means of abstraction, most ontology learning approaches so far have focussed on the formation of concepts and concept hierarchies. According to the different ways to define the meaning of classes, various types of systems have recently been developed in the area of ontology learning. The main methods which have been applied to the task of learning subclass relations are unsupervised hierarchical clustering techniques known from machine learning research e.g. [Cimiano et al. 2005, Faure & Nedellec 1999, Caraballo 1999]. These techniques typically learn concepts at the same time as they also group terms to meaning-bearing units which can be regarded as abstractions over words and thus, to some extent, as concepts. Typically, the hierarchies produced by such clustering approaches are very noisy as they highly depend on the frequency and behaviour of the terms in the text collection under consideration. Thus, some researchers have aimed at introducing a supervision into the clustering process by either directly involving the user to validate or reject certain clusters (compare the ASIUM system as described in [Faure & Nedellec 1999]) or including external information to guide the clustering process (see [Cimiano & Staab 2005]). The other paradigm is due to Marti Hearst and based on the idea that certain patterns reliably indicate a relation of interest between terms. A pattern like “X such as Y” for example indicates that Y is a subclass of X. Though such approaches are more or less reliable, they suffer from a low recall in the sense that such patterns do not occur frequently enough in text data. The proposed solution to this problem is to match these patterns on the Web (see the PANKOW and KnowItAll systems [Cimiano et al. 2004, Etzioni et al. 2004]). Also, other linguistically-inspired heuristics have been applied to increase the coverage of these methods (see [Cederberg & Widdows 2003]). Approaches based on matching such patterns can be implemented relatively easily by using regular expressions and are typically quite efficient as they basically just have to run once through the text collection. The conceptual drawback of such methods is that they essentially discover lexical relations between words but not between concepts, which are supposed to be abstractions and not merely plain words. Recently, new techniques have been proposed to derive new patterns indicating a relation of interest by bootstrapping procedures which learn new patterns and examples in each iteration (e.g. DIPRE [Brin 1998], Snowball [Agichtein & Gravano 2000], Espresso [Pantel & Pennacchiotti 2006], etc. ) 3.3 Extending an Existing Concept Hierarchy with new Concepts This task consists in extending a concept hierarchy with new concepts by adding a new concept at an appropriate position in the existing taxonomy. Supervised as well as unsupervised methods can be applied for this purpose. In the case of a supervised approach, classifiers need to be trained which predict membership for every concept in the existing concept hierarchy. As such methods need a considerable amount of training data for each concept, such approaches do typically not scale to arbitrary large ontologies. Unsupervised approaches assume a similarity function which computes a measure of fit between the new concept and the concepts existing in the ontology. Such methods rely on an appropriate contextual representation of the different concepts on the basis of which similarity can be computed. In this case, the hierarchical structure of the ontology needs to be considered and somehow integrated into the similarity measure (compare [Cimiano & Völker 2005b] as well as [Pekar & Staab 2003]). Some of these approaches, namely those which build upon a given set of instances of initially unknown classes, inherently tackle both the problem of taxonomy construction and population. 3.4 Learning NonTaxonomic Relations Given a taxonomic hierarchy, many existing ontology learning tools try to learn the “flesh” of the ontology, i.e. a set of nontaxonomic relationships which are essential for expressing domain-specific properties of both classes and instances. In order to learn non-taxonomic relations, one possibility is to learn ‘anonymous’ associations between terms on the basis of textual material, and then labeling the relations appropriately at a second step. Mädche and Staab for example make use of the well-known association rule learning algorithm to derive such anonymous relations (compare [Mädche & Staab 2000]). Other researchers have mainly exploited verbs appearing in text as indicators of a relation between their arguments (compare [Cimiano et al. 2006], [Ciaramita et al. 2005], [Schutz and Buitelaar 2006]). In general, while the quality of such approaches is in general reasonable, the relations will need to be inspected and validated by an ontology engineer. An important problem for extracting relations is, however, to find the appropriate level of generalization for these relations (compare [Cimiano et al. 2006]). Depending on concrete applications, other types of relationships such as equivalence, part-of or causality may be of interest. To some extent, one can even consider the identification of meta-properties (properties of classes), as an ontology learning task – although this kind of properties do not form part of standard ontology representation formalisms. A system which automatically learns meta-properties for concepts is the AEON system by [Völker et al. 2005]. The learning of general axioms or rules is currently out of scope for most ontology learning systems, such that we will not discuss this aspect of ontology learning any further. 3.5 Ontology Population Ontology population essentially consists in adding instances of concepts and relations to the ontology. For the population of ontologies with concept instances, approaches based on matching certain lexico-syntactic patterns - as described above – on the Web using a standard search engine have been shown to perform quite successfully (compare PANKOW [Cimiano et al. 2004] or KnowItAll [Etztioni et al. 2004]). For the task of learning instances of relations, mainly bootstrapping approaches harvesting relation tuples on the Web have been explored (see DIPRE [Brin 1998], Snowball [Agichtein & Gravano 2000], Espresso [Pantel & Pennacchiotti 2006]). In general, it seems that approaches to ontology population have gained a certain maturity and perform reasonably well. We thus conclude that ontology population seems an easier task than the one of learning the actual schema of the ontology. 4 Applications Currently, there seems to be a trend in the Semantic Web community to focus on what Gruber (compare [Gruber 2004]) called “semi-formal” ontologies. Though there is no clear definition of what a semiformal ontology is supposed to be, the intuition is that we are talking about an ontology which is to a large extent not axiomatized in the sense of a logical theory. Such ontologies typically consist of a set of concepts and a loosely defined taxonomic organization of these concepts. Such semiformal ontologies have the potential of providing a benefit for applications which need some abstraction over plain words but do not mainly rely on logical reasoning. Such applications can be mainly found in the fields of information retrieval, text mining and machine learning, where the applied methods are inherently fuzzy and error-prone. In the remainder of this section we examine the application of semi-formal ontologies to the tasks of structuring information as well as information retrieval, but also text mining in general. Further, we discuss a concrete application study to the British Telecom digital library. 4.1 Structuring Information for Advanced Search As the amount of information available in companies’ intranets steadily increases, the need for advanced search functionality grows at the same pace. However, it seems that advanced search functionality presupposes that information is accordingly structured with respect to certain categories. Search tools can then profit of information resources indexed with respect to these categories to provide advanced search functionality such as: search by category, retrieval of documents with similar categories, etc. Ontology learning can be applied for example to automatically derive categories from the underlying data which can then be used to index information resources and consequently to foster their search and reuse. The indexing of resources with respect to a given taxonomy of categories can either be done manually or by applying text mining techniques. As described below, it has recently even been shown that text mining techniques such as text classification or clustering can also be improved by integrating ontological information. 4.2 Information Retrieval and Text Mining One of the main problems that people have been struggling with in information retrieval is the so called vocabulary mismatch problem, which essentially consists in the fact that, in many cases, the words used in a query do not match the words in a document though both fit each other from a semantic point of view. In text mining, we face a similar situation. Unsupervised techniques used for clustering documents and relying on the bag-of-words model presuppose a considerable overlap in the words contained in documents in order to group them into one cluster. However, in many cases semantic overlap is not reflected in a corresponding overlap of the words in a document. Thus, many proposals in information retrieval and text mining have come up with the idea of integrating hierarchical organizations of words to either expand the query of a user with relevant words or extend bag-ofwords approaches by generalizing words along a taxonomic hierarchy. Recent results in the field of classification and clustering of documents have shown that automatically learned concept hierarchies can be successfully applied to partially overcome the vocabulary mismatch problem (compare [Bloehdorn et al. 2005]). 4.3 BT Digital Library Case Study The ‘Digital Library Case Study’ at British Telecom is one of three case studies within the EU IST integrated project Semantically Enabled Knowledge Technologies (SEKT3) which aims at the development and exploitation of technologies to support the ‘Next Generation Knowledge Management’ (see [Davies et al. 2006] for details). The library consists of an extensive on-line collection of technical articles, business journals, proceedings and books which are accessible to a variety of different users. The contents are structured by means of ‘Information Spaces’, i.e. keyword-based queries and associated documents belonging to one or more topics of interest that are defined in a global topic hierarchy. One of the main objectives of the SEKT case study is to enhance knowledge access to BT’s digital library. Up to now, searching and browsing has been limited to simple keyword-based queries and rather broad topics. Four major use cases for ontology learning techniques emerged from the requirement of improved searching and browsing: First, the extraction of ontologies from particular information spaces is supposed to enable a more fine-grained representation of the contents that could be used to improve visualization and browsing of information spaces. Second, topics and relationships learned from the documents may be used for refining the global topic hierarchy. Furthermore, structured knowledge which has been extracted from a particular information space might serve as a basis for semanticsbased query expansion. And finally, ontology learning techniques will enable sophisticated knowledge access by means of ontologybased question answering. Research within the SEKT project has led to the development of a prototype (depicted by Figure 2) which allows for querying the whole variety of information sources provided by the digital library - full text documents, structured metadata and topic hierarchies - by means of a single natural language query. The query is transformed into a structured logical form which is then passed on to a Description Logics reasoner, i.e. KAON2 [Motik & Sattler 2006], that tries to provide an answer by inferencing over an integrated ontology built from the different information sources. Whereas the integration of structured metadata and topic hierarchies into this ontology can be done in a relatively straightforward way, the creation of ontological data from the full text documents requires the application of ontology learning techniques such as those provided by Text2Onto (see [Cimiano & Völker 2005b]). Ontology learning tasks being performed in this scenario include the extraction of concepts, instances as well as taxonomic and non-taxonomic relationships such as part-of and subtopicof relations. 5 Challenges and Trends The main trends which can be identified in the field of ontology learning are on the one hand the creation of flexible ontology learning frameworks and the treatment of the uncertainty of the predictions of ontology learning algorithms. Current ontology learning frameworks such as Text2Onto4 [Cimiano & Völker 2005b] or JATKE5 have been designed with a central management component allowing various algorithms for ontology learning to be plugged in, thus being very flexible and modular. To some extent this is an engineering problem but it also requires a clear understanding of the way ontology learning algorithms work. On the other hand, most researchers have realized that the output of ontology learning algorithms is far from being perfect. As a consequence, to make the process controllable, we need an assessment of how certain an algorithm is in its predictions. Numeric confidence values of an algorithm in the certainty of a prediction could then be used as a basis to combine different algorithms compensating for the drawbacks and false predictions of each other. The representation of uncertainty and the combination of algorithms given their certainty are thus inherently coupled and represent one of the main open problems in the field of ontology learning. Though several proposals have recently emerged, the problem is far from being solved. In particular, coming up with a well-defined interpretation of certainty to give a sound basis for the combination of algorithms seems a non-trivial problem. One of the most important challenges for ontology learning is the tight integration of automatic approaches with methodologies for ontology engineering and evaluation. Depending on the intended usage of an ontology, a sophisticated process of requirements specification, learning, evaluation and refinement may be necessary in order to create ontologies of sufficient conceptual preciseness. The user should be given the possibility to specify the objectives of the ontology learning process with respect to formal and subjective aspects (e.g. complexity or domain coverage), and to choose among a variety of modelling primitives suitable for his application. Whereas in some cases a bare taxonomy will be sufficient, other modelling tasks may require the use of more specialized ontological constructs or relationships such as causality or equivalence. At each stage of the ontology learning process or, at least, after each iteration of the whole engineering cycle, an automatic or manual evaluation of the learned ontology should take place in order to avoid the propagation of errors and to allow for a re-configuration of the algorithms by the user or the system itself. This evaluation must take into account the evidences or certainties generated by the individual ontology learning algorithms, the specification of the user requirements as well as formal constraints of the target ontology modelling language. Conclusions and Outlook In this article we have briefly discussed the state-of-the-art in ontology learning with respect to different ontology learning subtasks. Further, we have also discussed potential applications for ontology learning to support structuring information as well as in the field of text mining and information retrieval. We have also described how ontology learning algorithms have been applied within the SEKT project at British Telecom. Though the methods for ontology learning are still in their infancy, they have already today the potential to improve certain classical applications such as information retrieval and helping in structuring huge collections of resources by applying text mining techniques. We have also discussed challenges and trends within ontology learning research and highlighted two main trends: the trend to build flexible frameworks into which diverse algorithms can be plugged-in in a simple way, as well as the trend to extend algorithms towards predicting how confident they are in their predictions. The most obvious challenges in ontology learning research are on the one hand to come up with a suitable interpretation of what the confidences indicated by algorithms mean from a formal point of view, thus providing a basis on which to combine the results from different algorithms in a sound way. A further important question is how axiomatized the ontologies we learn can actually be and which methods we need to obtain ontologies for the whole bandwidth of potential applications. Finally, coming back to the title of the article, it seems appropriate to provide an answer to the question whether the field of ontology learning is advanced enough to provide ontologies on demand. The answer to this question is actually twofold. We have seen on the one hand that for certain applications such as information retrieval and text mining, automatically learned ontologies already have the potential to provide an added value. These ontologies essentially provide an abstraction over plain words which Gruber refers to as `semi-formal ́ ontologies. On the other hand, ontology learning techniques still seem to be in their infancy, since in many cases ontologies generated by off-the-shelf ontology learning methods do not meet the demands of the envisioned applications. Highly configurable methods and frameworks as well as a tight integration with manual or automatic ontology evaluation approaches (see for example [Guarino & Welty 2000]) will be required to ensure the applicability of ontology learning across different application areas. It seems crucial to invest in the development of new ontology engineering methodologies which are able to integrate the results of ontology learning systems into the ontology engineering process, keeping user input at a minimum while maximizing the quality of the ontologies with respect to a particular domain or application. Acknowledgements We acknowledge financial support from the projects SEKT and X-Media, funded by the European Union under grants EU-ISTIP-2003-506826 and EU-IST-FP6-026978, respectively, as well as the BMBF project SmartWeb, funded by the Germany Ministry of Education and Research. 
The State of the Art in Ontology Learning: A Framework for Comparison. In recent years there have been some efforts to automate the ontology acquisition and construction process. The proposed systems differ from each other in some distinguishing factors and have many features in common. This paper presents the state of the art in ontology learning (OL) and introduces a framework for classifying and comparing OL systems. The dimensions of the framework answer to questions about what to learn, from where to learn and how to learn. They include features of the input, the methods of learning and knowledge acquisition, the elements learned, the resulted ontology and also the evaluation process. To extract the framework over 50 OL systems or modules from the recent workshops, conferences and published journals are studied and seven prominent of them with most differences are selected to be compared according to our framework. In this paper after a brief description of the seven selected systems we will describe the framework dimensions. Then we will place the representative ontology learning systems into our framework. At last we will describe the differences, strengths and weaknesses of various values for our dimensions in order to present a guideline for researchers to choose the appropriate features (dimensions’ values) to create or use an OL system for their own domain or application. Keywords: Ontology Learning, Ontology, Knowledge Acquisition, machine learning, Information Extraction. 1. Introduction Ontologies are means of knowledge sharing and reuse. They are semantic containers. The term ‘Ontology’ has various definitions in various texts, domains and applications. In Philosophy and Linguistics ontology is ‘The study of existence’, ‘A theory of what there is in the world’, or ‘A taxonomy of the world Concepts ’. The most popular definition of ontology in information technology and AI community in theoretical view is "A formal explicit specification of a shared conceptualization" or “An abstract view of the world we are modeling, describing the concepts and their relationships” (Gruber, 1993). In practical view an ontology may be defined as O = (C , R, A, Top ) in which C is the non-empty set of concepts (including relation concepts and the Top), R is the set of all assertions in which two or more concepts are related to each other, A is the set of axioms and Top is the most top level concept in the hierarchy. R itself is partitioned to two subsets, H and N. H is the set of all assertions in which the relation is a taxonomic relation and N is the set of all assertions in which the relation is a non-taxonomic relation (Shamsfard & Barforoush, 2002b). There may be also bidirectional functions that relate the members of C and their motivating elements in the real world (for example words in a natural language). 1 Ontologies are widely used in information systems, and ontology construction has been addressed in several research activities. The major problems in building and using ontologies are the bottleneck of knowledge acquisition and time-consuming construction and integration of various ontologies for various domains/ applications. In recent years two approaches have been concerned to solve these problems: (1) Development of methods, methodologies, tools and algorithms to integrate existing ontologies. Many disparate source ontologies are made for various domains or applications. There are different approaches to bring these sources together and reuse them. The integration process finds commonalities between source ontologies and derives a new ontology that facilitates interoperability between computer systems that are based on the source ontologies (Sowa, 2000). The integration may be done in the following tasks (adopted from Noy and Musen, 1999): a. Merging the ontologies to create a single coherent ontology, b. Aligning the ontologies by establishing links between them and allowing them to reuse information from one another, c. Mapping the ontologies by finding correspondence elements in each one. As an example of a merged ontology we can mention the project of merging the top-most levels of two general commonsense knowledge ontologies -SENSUS1 (Knight & Luk, 1994) and Cyc (Lenat, 1995)- to create a single top-level ontology of world knowledge (Chapulsky, et al, 1997). There are also some research projects working on general methods to merge and align ontologies such as the work by Noy and Musen (2000) introducing PROMPT, an algorithm for semi-automatic merging and alignment of ontologies or (Ryutaro, et al., 2001) proposing a concept alignment method used to induce appropriate align rules for concept hierarchies. There are also some researches on ontology mapping such as the proposed approach by Lacher and Groh (2001) using supervised classification. (2) Development of methods, methodologies, tools and algorithms to acquire and learn ontologies (semi) automatically. In this paper we focus on the second approach and introduce a framework for classifying and comparing ontology learning systems. The framework dimensions answer to questions about what to learn, from where to learn and how to learn. They focus on features of the inputs, the elements learned, the built ontology, and the methods of learning and knowledge acquisition. In the following sections we will first have an overview of some existing ontology learning systems and then will describe the framework dimensions. The references of this study are papers presented in the last three workshops on ontology learning held in the last three years (Staab, et al., 2000; Staab, et al., 2001; OLT’2002) and other journal and conference papers, technical reports and books recently published in this topic. Among the over 50 studied works, seven prominent ones are selected to be described and compared explicitly in the paper and others are referred while discussing our framework. The conclusion section will make the differences, strengths and weaknesses of various values for the dimensions more clear and give a guideline for researchers to choose the appropriate features (dimensions’ values) to build ontologies for their interesting domain or application or choose an existing OL system. 1 The SENSUS ontology itself resulted from manual merging of the PENMAN Upper Model, WordNet, and several other ontologies. 2 2. Ontology Learning Systems Ontology Learning refers to extracting ontological elements (conceptual knowledge) from input and building an ontology from them. Manual building of ontologies is a cost and time consuming, tedious and error-prone task and the manually built ontologies are expensive, biased based on their developer, non-flexible to changes and specific just to their motivated purpose. Automation of ontology construction not only eliminates the costs, but also results in a better matching ontology with its application. Ontology learning uses methods from diverse spectrum of fields such as machine learning, knowledge acquisition, natural language processing, information retrieval, artificial intelligence, reasoning and database management. During the last decade several ontology learning approaches and systems are proposed. Some of them are autonomous ontology learning systems while some others are supporting tools to build ontologies. In this section we will discuss a selection of both. Our criteria for selecting systems for the study were to (1) select from well developed autonomous systems and supporting tools which are described from end-to-end in the documents, (2) select the most recent systems (from 1997) (3) select systems with most differences, each as a representative of its group, having some distinguishing features and (4) select welldocumented systems to be able to answer the questions we are asking about the dimensions. For example we chose ASIUM as the representative of the category of systems on learning verb sub categorizations, HASTI for learning axioms besides words, concepts and relations from scratch, TEXT-TO-ONTO for learning from structured, semi-structured and unstructured data using a multi strategy method and WEB→KB for combining statistical and symbolic methods to learn instances and rules from Web documents. Table (1) shows a summary of the selected systems in alphabetical order with their references. In this table we mention the most distinguishing features of the selected system for which it is selected. As the table shows, among the selected systems two are supporting tools to learn ontologies and five are autonomous OL systems. The supporting tools (SVETLAN’ and DODDLEII) extract essential structures from input to make an ontology learning system able to build an ontology. For the rest of this paper we will first have an overview of the seven selected systems. Then we will describe the extracted feature set (the framework dimensions) to classify and compare ontology learning systems in the next section. There we will bring some examples from over 50 studied works for each dimension, and at last we will compare the selected systems according to the introduced framework. ASIUM- ASIUM learns subcategorization frames of verbs and ontologies from syntactic parsing of technical texts in natural language (French). The inputs of ASIUM result from syntactic parsing of texts, they are subcategorization examples and basic clusters formed by head words that occur with the same verb after the same preposition (or with the same syntactical role). ASIUM successively aggregates the clusters to form new concepts in the form of a generality graph that represents the ontology of the domain. Subcategorization frames are learned in parallel, so that as concepts are formed, they fill restrictions of selection in the subcategorization frames. The ASIUM method is based on conceptual clustering. ASIUM proposes a cooperative ML method, which provides the user with a global view of the acquisition task and also with acquisition tools like automatic concepts splitting, example generation, and an ontology view with attachments to the verbs. Validation steps using these features are intertwined with learning steps so that the user validates the concepts as they are learned. DODDLE II- DODDLE II is a Domain Ontology Rapid Development Environment. It makes an environment to construct domain ontologies with both taxonomic and non-taxonomic conceptual relationships, exploiting a machine readable dictionary (MRD) and domainspecific texts. It supports a user in constructing domain ontologies. The taxonomic relationships come from WordNet in the interaction with a domain expert, using match result analysis and trimmed result analysis. The non-taxonomic relationships come from domain specific texts with the analysis of lexical cooccurrence statistics, based on WordSpace to represent lexical items according to how semantically close they are to one another. To evaluate the system, some case studies have been done in the field of law. HASTI- HASTI is an automatic ontology building system, which builds dynamic ontologies from scratch. HASTI learns the lexical and ontological knowledge from natural language (Persian) texts. Its lexicon is nearly empty initially and will grow gradually by learning new words. The ontology in HASTI is a small kernel at the beginning. HASTI learns concepts, taxonomic and non-taxonomic conceptual relations, and axioms, to build ontologies upon the existing kernel. The learning approach in HASTI is a hybrid symbolic approach, a combination of linguistic, logical, template driven and semantic analysis methods. It performs online and offline clustering to organize its ontology. SYNDIKATE - SYNDIKATE is a system for automatically acquiring knowledge from realworld texts (German), and for transferring their content to formal representation structures which constitute a corresponding text knowledge base. It integrates requirements from the analysis of single sentences, as well as those of referentially linked sentences forming cohesive texts. Besides centering-based discourse analysis mechanisms for pronominal, 4 nominal and bridging anaphora, SYNDIKATE is supplied with a learning module for automatically boot-strapping its domain knowledge as text analysis proceeds. The approach to learn new concepts as a result of text understanding builds on two different sources of evidence: the prior knowledge of the domain the texts are about, and grammatical constructions in which unknown lexical items occur in the texts. A given ontology is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered on the linguistic and conceptual “quality” of various forms of evidence underlying the generation and refinement of concept hypotheses. On the basis of the quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base. SVETLAN’- SVETLAN’ is a system to classify nouns in context. It is able to learn categories of nouns from texts, whatever their domain is. Words are learned considering the contextual use of them to avoid mixing their meanings. SVETLAN’ is a supporting tool. Its input data are semantic domains with the Thematic Units (TUs) automatically learned by SEGAPSITH (Ferret & Grau, 1998) and the output is the learned structured domain containing the noun classifications with their relations to verbs. It is based on a distributional approach: nouns playing the same syntactic role with a verb in sentences related to the same topic, i.e. the same domain, are aggregated in the same class. TEXT-TO-ONTO - TEXT-TO-ONTO is an ontology learning environment, based on a general architecture for discovering conceptual structures and engineering ontologies from text. It supports as well the acquisition of conceptual structures as mapping linguistic resources to the acquired structures. It makes an environment to discover conceptual relations to build ontologies. Its new version which support learning ontologies from web documents, allow the import of semi structured and structured data as input as well as texts. It also has a library of learning methods which use each on demand. Its learning method is a multi strategy one combining various methods for various inputs and tasks. WEB→KB - The goal of this research is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Its approach is to develop a trainable information extraction system that takes two inputs: (1) a knowledge base consisting of an ontology defining the classes (e.g., person) and relations (e.g., instructor-of) of interest, and optionally, instances of some of these classes and relations, (2) training examples from the Web that describe instances of these classes and relations. Given these inputs, the system determines general procedures capable of extracting additional instances of these classes and relations by browsing the rest of the Web. The outputs would be the classified instances and rules to extract new instances, rules to classify pages and rules to recognize relations among several pages. WEB→KB uses logical and statistical learning algorithms for these tasks. 3. The Dimensions of the Framework During the last decade there have been some efforts to automate ontology construction. Several ontology learning approaches and systems are proposed, which are different with each other in some dimensions. The major distinguishing factors between ontology learning systems are classified in six main categories (dimensions) and some subcategories. For each dimension some features are selected and for each feature there are some references to systems having them. 5 The framework dimensions are as follows: 1) Elements learned (concepts, relations, axioms, rules, instances, syntactic categories and thematic roles), 2) Starting point (prior knowledge and the type and language of input), 3) Preprocessing (linguistic processing such as deep understanding or shallow text processing), 4) Learning method consisting of a) Learning category (supervised vs. unsupervised, online vs. offline), b) Learning approach (statistical vs. symbolic, logical, linguistic based, pattern matching, template driven and hybrid methods), c) Learning task (classification, clustering, rule learning, concept formation, ontology population), d) Degree of automation (manual, semi-automatic, cooperative, full automatic), i) Type and amount of user intervention. 5) The result (ontology vs. intermediate structures and in the first case the features of the built ontology such as coverage degree, usage or purpose, content type, structure and topology and representation language), 6) Evaluation methods (evaluating the learning methods or evaluating the resulted ontology) Figure (1) shows the framework dimensions, their sub-dimensions and some of their possible values. Dimensions and sub-dimensions are shown in boxes and values in ellipses. For the rest of this section, the framework dimensions will be described in more detail. Each subsection will describe one of the dimensions. In each subsection, sub-dimensions will be numbered and the values for each will be bulleted. 6 3.1.The Elements Learned In this part we answer to the question: “what type of conceptual structure is acquired?” The learned elements can be merely ontological knowledge or both lexical and ontological knowledge. The main lexical elements, which proposed systems learn, are words and the main ontological elements are concepts, relations and axioms. There are also some systems which learn Meta knowledge about how to extract ontological knowledge from input. • Words Although most ontology learning systems (to mention few, TEXT-TO-ONTO; DODDLE II; Kietz, et al., 2000; Borgo, et al., 1997) use predefined lexicons, some of them (Thompson & Mooney, 1999; HASTI; SYNDIKATE) learn lexical knowledge about words too. The method of handling unknown words and the type of lexical knowledge to be learned about words are different in different OL systems. For instance SYNDIKATE predicts word class for unknown lexical items given a hierarchy which covers all relevant word classes for a particular natural language. So, once a word class is hypothesized, grammatical information associated with this word class (such as valency frames, word order constraints, or morphosyntactic features) comes for free due to the organization of the grammar as a lexical class hierarchy. HASTI learns morphological features and syntactic category of unknown words besides their meanings and WOLFIE (WOrd Learning From Interpreted Examples), (Thompson & Mooney, 1999) acquires a semantic lexicon from a corpus of sentences paired with semantic representations. The lexicon learned consists of words paired with meaning representations. • Concepts A concept can be anything about which something is said and can be abstract or concrete, elementary or composite, real or fictious, the description of a task, function, action, strategy, reasoning process, etc (Corcho & Gomez-Perez, 2000). Concepts are represented by nodes in the ontology graphs and may be learned by the ontology learning system (such as HASTI; SYNDIKATE; Roux, et al, 2000; Soderland, et al., 1995). They may be extracted from input or be created during the ontology refinement from other concepts. In other words they may have or not corresponding elements in the input. In terminological (or term based) acquisition of concepts, a concept node will be created corresponding to the extracted term which may be natural language words or phrases, while in conceptual (or semantic based) concept creation which is usually done in the refinement phase, the concept will be built according to its features (attributes/values), its functionality, etc and hence may have no corresponding input (no corresponding word or phrase in the input text). Instances Some ontology learning systems use an existing ontology and just populate it by instances of classes and relations such (WEB→KB; Suryanto & Compton, 2001). Most of these systems do not learn new concepts (classes) and just learn instances of existing classes. • Conceptual Relations Relations may be studied in two ways: 1) A relation is a node in the ontology, so it is a concept and may be learned like other concepts. 7 2) A relation relates two or more concepts and so it should be learned as a subset of a product of n concepts (for n>1). In other words relations may be learned intentionally independent to what concepts do they relate or extensionally considering the concepts which are being related to each other by it. The first case will be counted in the first type of learned elements (concepts) and the second case in the second type (conceptual relations). For example the binary relation part-of is a concept under the super-concept say ‘relation’ and has its own features (such as transitivity) and may be related to other concepts by other relations too. On the other hand it relates concepts ‘hand’ and ‘human’ or ‘door’ and ‘house’ which may be shown as (part-of hand human) or (part-of door house) which is its second aspect. In some systems both aspects of relations may be learned (HASTI), while in some others the relations themselves (the first aspect) are predefined and their occurrences (the second aspect) will be learned using some templates (Borgo, et al., 1997). In this subsection we mention learning of the extensional definition of relations. Relations may be intra-ontology or inter-ontologies. Intra-ontology relations are those relating an ontology’s elements (structures) to each other while the inter-ontology relations relate the structures in two or more ontologies together. Most works on learning relations are from the first kind. But there are some researches on learning the relations between ontologies too, such as the proposed work by Williams and Tsatsoulis (2000) on acquiring related concepts within diverse ontologies using an instance based approach. Conceptual relations may be taxonomic or non-taxonomic relations. Taxonomic Relations Taxonomies are widely used to organize ontological knowledge using generalization/specialization relationship through which simple/multiple inheritance could be applied. (Corcho & Gomez-Perez, 2000). Although some references (Sowa, 2000) refer to hyponymy and meronymy relations as taxonomic relations, most taxonomic relation learning systems (some of them are mentioned below) just learn the hyponymy relations (the ISA hierarchy of concepts). Thus, in this part by taxonomic relations we mean the hyponymy relations and leave the meronymy relations to the next part (non-taxonomic relations). Taxonomic knowledge is learned by some ontology learning systems. Some among others are SYNDIKATE; HASTI; DODDLE II; Todirascu, et al. (2000); Agirre, et al. (2000); Suryanto & Compton (2000); Heyer et al. (2001); Caraballo (1999); Delteil, et al. (2001); Sundblad (2002) and Sporleder, (2002). A survey on existing works on learning taxonomic relations from texts is given in (Maedche, et al., 2002). Non-Taxonomic Relations Non-taxonomic conceptual relations refer to any relation between concepts except the ISA relations, such as synonymy, meronymy, antonymy, attribute-of, possession, causality and other relations (learned by systems such as HASTI; TEXT-TO-ONTO; Agirre et al, 2000; Gamallo, et al., 2002), knowledge about specific words’ syntactic categories and thematic roles such as learning subjects and objects of verbs (Pereira et al., 1993), discovering verb frames (ASIUM; Wagner, 2000), inferring verb meanings (WiemerHastings, et al., 1998), classifying adjectives (Assadi, 1997) and nouns (SVETLAN’) and identifying names (Bikel, et al., 1999). 8 • Axioms Axioms are used to model sentences that are always true. They can be included in an ontology for several purposes, such as constraining the information contained in the ontology, verifying its correctness or deducting new information (Farquhar, et al., 1996). There is an open problem to learn axioms (semi) automatically. HASTI is a one that learns axioms in restricted situations. It translates explicit axioms in conditional and quantified natural language sentences to logically formatted axioms in KIF. There is also ongoing work to extend HASTI to learn implicit axioms from text too. • Meta knowledge Beside systems that learn ontological knowledge, there are systems which learn how to learn/extract ontological knowledge. They learn meta knowledge such as rules to extract instances, relations and specific fields from web (WEB→KB) or patterns to extract knowledge from text (Fienkelstein & Morin, 1999; Soderland, et al., 1995) or association rules in a corpus (Cherfi & Toussaint, 2002). 3.2. Starting Point This dimension is concerned with the answer of “From where to start ontology acquisition and from what to learn?” Ontology learning systems use their background knowledge (prior knowledge) and acquire new knowledge elements (or update the existing ones) from their input. The quality and quantity of the prior knowledge and the type, structure and language of the input -from which the system learns ontological knowledge- differ from one system to another. 3.2.1. Background or Prior Knowledge (Base Ontology) Essential background knowledge varies in both type and volume in different projects. The background knowledge may be presented in linguistic (lexical, grammatical, templates, etc) or ontological (base ontology) resources. In many projects there is a predefined lexicon used to process texts (such as Kietz et al, 2000). In some of these projects the lexicon is a semantic lexicon covering ontological knowledge too (such as using (Euro) WordNet in TEXT-TOONTO; SYNDIKATE; DODDLE II; Wagner, 2000; Agirre et al, 2000; Termier et al, 2001). The base ontology’s size and coverage is another distinguishing factor varying from almost empty (a small kernel of primitives) as in HASTI, a seed ontology sketched by the user as in (Brewester, et al., 2001), small number of seed words that represent the high-level concepts as in InfoSleuth (Hwang, 1999), to huge general commonsense ontologies such as Cyc (Lenat, & Guha, 1990). 3.2.2. Input Ontology learning systems extract their knowledge of interest from their input. Input sources differ by type and language. a) Type: The type of input from which system acquires ontological knowledge may be of followings: • Structured data: Ontology learning systems may extract ontological knowledge from structured data such as database schemata (Kashyap, 1999), existing ontologies (Williams & Tsatsoulis, 2000), knowledge bases (Suryanto & Compton, 2000) and lexical semantic nets such as WordNet. 9 • Semi structured data: Other sources for ontology learning systems (such as Pernelle, et al., 2001) are semi structured data such as dictionaries, HTML and XML docs and DTD’s (document type definitions). Growing interest in semantic web leads to increasing interest in building ontologies for the web. So learning ontologies from semi structured data in web documents is a hot topic today. TEXT-TO-ONTO, WEB→KB, and (Kavalek & Svatek, 2002) are instances of such systems. • Unstructured data: The most difficult type of input to extract knowledge from is the unstructured ones. Tools that learn ontologies from natural language exploit the interacting constraints on the various language levels (from morphology to pragmatics and background knowledge) in order to discover new concepts and stipulate relationships between concepts (OLT’2002). The unstructured input of OL systems may be natural language texts (HASTI; SVETLAN’; SYNDIKATE; Heyer et al, 2001) or Web texts (as in TEXT-TO-ONTO; Todirascu et al, 2000 and other systems studied in Omelayenko, 2001). b) Language: The input may be texts of natural languages such as English (DODDLE II; Wagner, 2000; Termier et al, 2001), German (SYNDIKATE; TEXT-TO-ONTO), French (Todirascu et al, 2000; ASIUM; SVETLAN’), Persian (HASTI), etc. or datum presented in artificial languages such as XML (TEXT-TO-ONTO) or RDF (Delteil et al., 2001). 3.3.Preprocessing This dimension answers to the question: “Is there any preprocessing to convert the input to a suitable structure to learn from?” The most popular preprocessing used in learning ontologies from texts is the linguistic preprocessing. Deep understanding would provide specific relations among concepts, whereas shallow techniques could provide generic knowledge about the concepts (Agirre et al, 2000). As deep understanding usually decreases the speed of the ontology construction process, most existing systems use shallow text processing techniques such as tokenizing, part-of-speech (POS) tagging, syntactic analysis and so on to extract their essential structures from input texts. For example TEXT-TO-ONTO uses shallow text processing methods developed at SMES (Saarbrucken Message Extraction System) (Neumann et al, 1997) to process German texts and identify linguistically related pairs of words, which are mapped to concepts using the domain lexicon. InfoSleuth (Hwang, 1999) uses a simple POS tagger to perform superficial syntactic analysis and ASIUM uses Sylex (Constant, 1996) to process French texts. SYNDIKATE uses deep understanding to extract ontological knowledge from text and HASTI exploits Petex (Shamsfard & Barforoush, 2002c) a Persian text processing system to extract sentence structures, which indicate thematic roles, from text. There are also other preprocessing modules to extract special structures from input to make the learning modules able to learn ontological elements such as SVETLAN’ extracting noun categories and (Moigno, et al., 2002) extracting terminology to help a domain expert build an ontology in surgical intensive care field. 3.4.Learning Method By this dimension we answer to the question: “what kinds of methods are used to extract knowledge?” Knowledge extraction methods range from knowledge poor approaches (statistical techniques) to knowledge intensive approaches (logical techniques). In this range 10 we may mention different approaches and techniques to extract ontological knowledge and learn ontologies. These methods may be supervised or unsupervised and online or offline. 3.4.1. Learning Approaches The ontology learning approach may be statistical or symbolic. From symbolic approaches we mention the logical, linguistic based and template driven approaches. Heuristic methods may be used to facilitate each approach. There are also Hybrid approaches, which combine two or more of the above approaches and employ their benefits and eliminate their limitations. • Statistical In this approach, statistical analysis will be performed on data gathered from the input. For instance WEB→KB uses a statistical bag-of-words approach to classify Web pages, (Wagner, 2000) exploits a modification of the algorithm by Li & Abe (1996) for acquisition of selectional preferences and locating concepts in the ontology at the appropriate generalization level, TEXT-TO-ONTO; Heyer (2001) and DODDLE II use statistical analysis of cooccurrence data to learn conceptual relations from texts, (Bikel, et al., 1999) uses Hidden Markov Model (HMM) to find and label names and other numerical entities and (Cherfi & Toussaint, 2002) uses statistical indices to rank the association rules that are more capable of reflecting the complex semantic relations between terms. Statistical methods may work on isolated words or batches of words together. They differ in size of batches, distribution function and statistical analysis done on input data. Models based on isolated words are often called unigram or bag-of-words models. They ignore the sequence in which the words occur. Since the unigram model naively assumes that the presence of each word in a document is conditionally independent of all other words in the document given its class, this approach, when used with Bayes Rule is often called naive Bayes (Craven, et al., 2000). WEB→KB is a system that classifies web documents using a modification of naive Bayes method. It builds a probabilistic model of each class using labeled training data, and then classifying newly seen pages by selecting the class that is most probable given the evidence of words describing the new page. Its method for classifying web pages is naive Bayes, with minor modifications based on Kullback–Leibler Divergence. Other statistical methods often consider batches of words. The main idea common to these approaches is that the semantic identity of a word is reflected in its distribution over different contexts, so that the meaning of a word is represented in terms of words cooccurring with it and the frequencies of the cooccurrences (Maedche, et al., 2002). The occurrence of two or more words within a well-defined unit of information (sentence, document) is called a collocation (Heyer, et al., 2001). Learning by Collocations and Cooccurrences is the most addressed methods in statistical learning of ontological knowledge. In learning by collocation and cooccurrence, first a collocation structure (e.g. matrix) will be created. Then using statistical analysis of this structure, the conceptual relations between concepts will be discovered. For instance in (Heyer, et al., 2001) a kind of average context for every word A is formed by all collocations for A with a significance above a certain threshold. This average context of A is transferred into a feature vector of A using all words as features as usual. The feature vector of word A is indeed a description of the meaning of A, because the most important words of the contexts of A are included. Clustering of feature vectors can be used to investigate the relations between groups of similar words and to figure out whether or not all the relations are of the same kind. Cohyponymy, top-level syntactic relations (such as semantic ‘actor-verb’), often used properties of a noun and instance-of are some discovered relations by this approach. 11 Another project using collocation analysis is DODDLE II. In this project semantic similarity is calculated by inner product (cosine of angle) of two word vectors in the wordspace, which contains the vectors representing all concepts. After finding similarities, taxonomic and non-taxonomic relations will be extracted using concepts specification templates. To build the wordspace, the collocation matrix should be built from 4-grams extracted from the corpus. Using the collocation matrix, the context-vectors and the word-vectors will be built. As another example, TEXT-TO-ONTO uses the frequency of word co-occurrences in discovering non taxonomic relations using background knowledge (a lexicon and a taxonomy) from linguistic processing of text. An algorithm for discovering generalized association rules analyzes statistical information and derives correlations at the conceptual level. Thereby, it uses the background knowledge from the taxonomy in order to propose relations at the appropriate level of abstraction. • Logical Logical methods such as inductive logic programming (ILP) (Zelle & Mooney, 1993), FOL (First Order Logic) based clustering (Bisson, 1992), FOL rule learning (WEB→KB) and propositional learning (Bowers, et, al., 2000) are also used to extract ontological knowledge from input. Logic-based learning methods may discover new knowledge by deduction or induction and represent knowledge by propositions, first order or higher order logics. Deduction based learning systems (such as HASTI) exploit logical deduction and inference rules such as resolution to deduce new knowledge from existing ones while induction-based learning systems (such as WEB→KB; Bowers, et, al., 2000) induce hypotheses from observations (examples) and synthesize new knowledge from experience. Inductive Logic Programming (ILP) lies in the intersection of inductive learning and logic programming in which the hypotheses and observations are represented in first order logic or variants of it (Muggleton & De Raedt, 1994). FOIL (Quinlen & Cameron-Jones, 1993) is one of the bestknown and most successful empirical ILP systems and some ontology learning systems such as WEB→KB use variants of it. FOIL is a greedy covering algorithm that induces concept definitions represented as function-free Horn clauses, optionally containing negated body literals. It induces each Horn clause by beginning with an empty tail and using a hillclimbing search to add literals to the tail until the clause covers only (mostly) positive instances. WEB→KB uses first-order learning algorithms to learn (1) rules for classifying pages, (2) rules to recognize relations among several pages and (3) rules to extract specific text fields within a web page. Its learning algorithm to classify pages is Quinlan’s FOIL algorithm. The learning algorithm to induce relation rules is also similar to FOIL in that it uses a greedy covering approach to learn a set of Horn clauses, but it differs from FOIL in that it merges the hill-climbing search in FOIL with a variant of the relational pathfinding method by Richards and Mooney (1992) to be able to learn rules for paths consisting of more than one hyperlink. It also uses a different evaluation function (using m-estimates of a clause’s error) to guide the search process. WEB→KB uses the SRV (Sequence Rules with Validation) algorithm to learn rules to extract specific text fields. The SRV algorithm is a first-order learner in the spirit of FOIL. It shares FOIL’s top-down approach and gain metric, but is designed with the information extraction problem in mind. Consequently, it is limited to a few pre-defined predicates, and it encompasses search heuristics specific to the information extraction problem. Input to SRV is a set of pages, labeled to identify instances of the field we want to extract, and a set of features defined over tokens. Output is a set of information extraction rules. The extraction process involves examining every possible text fragment of appropriate size to see whether it matches any of the rules. 12 Other logic-based work is done by Bowers and colleagues (Bowers, et, al., 2000) in which a decision tree learning algorithm is used to learn predicates represented in typed higher order logic. • Linguistic Linguistic approaches such as syntactic analysis (ASIUM), morpho-syntactic analysis (Assadi, 1997), lexico-syntactic pattern parsing (Finkelstein-Landau & Morin, 1999), semantic processing (HASTI) and text understanding (SYNDIKATE) are used to extract ontological knowledge from natural language texts. They are mostly language dependent and usually perform the preprocessing on the input text to extract essential knowledge to build ontologies from texts. For instance Assadi (1997) performs a partial morpho-syntactic analysis to extract “candidate terms” from technical texts. Then using these candidate terms the knowledge engineer, assisted by an automatic clustering tool, builds the conceptual fields of the domain. The result of the morpho-syntactic analysis would be a network of noun phrases which are likely to be terminological units. Any complex term is recursively broken up into two parts: head and expansion, which are both linked to the complex candidate term in a terminological network. The network will then be used by the conceptual analyzer to build a classification tree. ASIUM uses syntactic analysis to extract syntactic frames from texts. It only uses head nouns of complements and links with verbs. Adjectives and empty nouns are not used. The learning method relies on the observation of syntactic regularities in the context of words. It does conceptual clustering based on head nouns occurring with the same couple verb + preposition/ syntactic role. HASTI exploits both morpho-syntactic and semantic analysis on input texts to extract lexical and ontological knowledge. The morpho-syntactic analysis predicts the features of unknown words and creates sentence structures, which indicate the thematic roles in the sentence. The semantic analysis completes the empty or ambitious slots of the sentence structures and conducts the process of extracting conceptual knowledge from them using semantic templates. SYNDIKATE uses text understanding techniques to acquire knowledge from real-world texts. It integrates requirements from the analysis of single sentences, as well as those of referentially linked sentences forming cohesive texts. The result of its syntactic analysis is captured in a dependency graph in which nodes are words and edges are dependency relations such as specifier, subject, dir-object, etc. Then semantic interpretation would be performed to find conceptual relations in the knowledge base between conceptual correlates of words. SYNDIKATE learns by quality which means that linguistic and conceptual quality labels are assigned to generated hypotheses and then the higher ranked hypothesis will be chosen. Linguistic quality labels are APPOSITION, EXEMPLIFICATION, CASE-FRAMEASSIGNMENT, PP-ATTACHMENT, GENITIVE-ATTRIBUTION and have higher score than conceptual ones. Another linguistic method is Lexico-syntactic pattern parsing. In this method the text is scanned for predefined lexico-syntactic patterns that indicate a relation of interest, e.g. the taxonomic relation (Maedche, et, al., 2002). We will discuss these patterns in the next section in more detail. • Pattern Based / Template Driven Keyword/ pattern/ template matching approaches are widely used in the information extraction field and are also inherited to the ontology learning domain. In template driven methods the input (usually the text) will be searched for predefined keywords, templates or 13 patterns which indicate some relations (e.g. hyponymy). There are various types of templates, syntactic or semantic and general or special purpose to extract various ontology elements. As the primary work on pattern matching we can mention the one done by Hearst (1992). In his paper, he introduced some lexico-syntactic patterns in the form of regular expressions to extract hyponymy/ hyperonymy relations from texts. As examples of these patterns we may mention the following: NPsuchas { NP , }* ( and | or ) NP NP {, NP }* {, }( or | and ) otherNP NP {, }including { NP , }* {or | and } NP HASTI is another system that uses lexico-syntactic and also semantic patterns (templates) to extract taxonomic and non-taxonomic relations such as hyponymy, meronymy, thematic roles, attribute-values (‘has-prop’ relation) and other relations and also axioms from texts. An example of its lexico-syntactic pattern is the exception template to extract hyponymys {all | every} NP0 except NP1 {( and | ,) NPi}*... (i>1), implies (sub-class NPi NP0 ) (i ≥1) and an example of its semantic template is the one for modal (copular) sentences with an adjective phrase as the predicate to extract attribute-value (‘has-prop’) relations: (⇒ ( and (isa <subject.head> Property) (isa <adjective> <subject.head>)) (has-prop <subject.modifier.head> <subject.head> <adjective>)) or in case that the predicate is a noun phrase a templates to extract the equality relation is (equal <subject.head> <predicate.head>) Symbolic interpretation rules in (Gamallo, et al., 2002) are somehow close to semantic templates in HASTI. They use grammatical patterns to map syntactic dependencies onto the semantic relations such as hyperonymy, possession, location, modality, causality, agentivity, etc. A dependency is represented as the binary relation (r: w1↓, w2↑) where r can be instantiated by specific grammatical markers such as particular prepositions, subject relations, direct object relations, etc., arrows“↓” and “↑” represent the head and complement position, respectively; w1 is the word in the head position and w2 the word in the complement position. The grammatical patterns (markers) indicate syntactic relators (subject, direct object, and prepositions), morpho-syntactic categories of the two related words (verb and noun), and presence or absence of determinant in the Complement. For example an interpretation rule is following: x=possessed; y=possessor => [λx↓λy↑(de+; x↓,y↑)] or [λx↓λy↑(a+; x↓,y↑)] or [λx↓λy↑(para; x↓,y↑)] In which for instance the grammatical pattern de+ indicates (1) preposition is de; (2) the determiner is present before the complement; (3) the head and the complement are both nouns. By means of this rule, the heads (expressed by the variable x) of the patterns de+, a+ and para are mapped onto the semantic role “Possessed”, whereas the complements (expressed by y) are mapped onto the role “Possession”. Another work is done by Sundblad (2002) in which some linguistic patterns are used to extract hyponymy and meronymy relations from question corpora such as Who is/was X? 14 What is the location of X? What is/was the X of Y? How many X are in/on Y? Heyer, et al., (2001) proposed two patterns to extract first names and instance-of relations from sentences: a) Extraction of first names: A pattern like (profession) ? (last name) implies (with high probability) that the unknown category ? is in fact a first name (e.g. actress Julia Roberts) b) Extraction of instance-of-relations given the class name: The pattern (class name) like ? implies (with high probability) that the unknown category ? is in fact a instance name. (e.g. metals like nickel, arsenic and lead). The patterns may be general and application/domain neutral such as those proposed by Hearst, HASTI and Sundblad or specific to a domain or application such as those used by Assadi (1999) to extract knowledge from electric network planning texts. On the other hand patterns may be manually defined (HASTI; Sundblad, 2002; Gamallo, et al., 2002) or may be extracted (semi) automatically such as in PROMETHEE (FinkelsteinLandau & Morin, 1999), AutoSlog-TG (Riloff, 1996) and CRYSTAL (Soderland, et al., 1995). • Heuristic Driven (Ad Hoc Methods) Heuristics may be used besides any of other approaches. In other words heuristic driven methods are not independent and complete methods they rather should be used to support other approaches. To name few examples for this approach we may mention TEXT-TO-ONTO; HASTI; InfoSleuth (Hwang, 1999) and (Gamallo, et al., 2002). TEXT-TO-ONTO uses heuristic rules to increase the recall of the linguistic dependency relations (even for loss of linguistic precision) such as the NP-PP-heuristic which attaches all prepositional phrases to adjacent noun phrases, the sentence-heuristic which relates all concepts contained in one sentence if other criteria fail and the title-heuristic which links the concepts in the HTML title tags with all the concepts contained in the overall document. HASTI uses some simplifying heuristics to decrease the size of hypothesis space such as the priority-assignment heuristics to assign priorities to ambitious terms and candidate-choosing heuristics to choose a merge set among others in the ontology refinement task (offline clustering). InfoSleuth uses some heuristic rules to locate new concepts in an appropriate place in the ontology. The heuristic says that (for specific terms) the concept corresponding to a noun phrase should be located under the concept of its head. (Gamallo, et al., 2002) employs heuristics to select candidate dependencies. They use simple heuristics based on right association in order to attach basic chunks: a chunk tends to be attached to another chunk immediately to its right. They consider that the word heads of two attached chunks form a candidate syntactic dependency. • Multi Strategy learning Most systems, which learn more than one type of ontology elements, use combined approaches. They apply multi strategy learning to learn different components of the ontology using different learning algorithms such as TEXT-TO-ONTO using Association rules, formal concept analysis and clustering techniques, WEB→KB combining FOL rule learning with Bayesian learning, HASTI applying a combination of logical, linguistic based, template driven and heuristic methods and (Termier, et al., 2001) combining statistics and semantics for word and document clustering. 15 3.4.2. Learning Task Learning methods may be categorized based on the task they do. In this category, classification (Suryanto & Compton, 2000; Bowers, et al., 2000), clustering (HASTI; ASIUM), rule learning (WEB→KB; Soderland, et al., 1995), formal concept analysis (TEXT-TO-ONTO; Richards & Compton, 1997), and ontology population (instance assignment) (WEB→KB; Brewster, et al., 2001) are some learning tasks (for each we named two examples), which may be done in each of the above approaches (statistical, logical, linguistic based or template based). For example the system ASIUM does conceptual clustering using a syntactic parsing approach, while (Wagner, 2000) uses statistical approach to cluster and (Termier et al., 2001) combines statistics and semantics to cluster words and documents. The learning task may be used to extract knowledge from input or to refine an ontology. Below we will discuss clustering, one of most applied tasks in ontology learning in more detail. • Conceptual Clustering Various clustering methods (some are described in Bisson, et al., 2000) are distinguished by four factors (adopted from Maedche et, al., 2002): clustering mode, clustering direction, similarity measure, and computation strategy Clustering mode: - Online vs. offline: clustering may be done in online or offline modes. The online mode does incremental clustering while the offline mode does periodic clustering. - Hierarchical vs. non-hierarchical: In hierarchical clustering the clusters that are built have hierarchical relations with one another while in non-hierarchical clustering they have nonhierarchical relations with one another. - Single vs. Multi clustering: In Multi clustering each concept may be clustered into more than one cluster or in other words, in the directed graph constructed by clustering, each node may have many parents and/or many children. Clustering Direction: The hierarchical clustering may be done in either of the directions: - Top-down: In the top-down direction at first all concepts are collected in a single cluster. Then the cluster will be split and most similar concepts made into sub-clusters. In other words the hierarchy will be built by incremental specification from top to bottom. - Bottom-up: In the bottom-up direction at first there are N clusters for N concepts (there is one concept in each cluster). Then the most similar clusters will be merged and make super-clusters. In other words the hierarchy will be built by incremental generalization from bottom to top. - Middle-out: A combination of top-down and bottom-up methods. Similarity Measure: In clustering algorithms there is a similarity measure indicating the similarity between two classes. In the literature, two main different types of similarity have been addressed (Eagles, 1996): - “Semantic similarity” (so-called paradigmatic or substitutional similarity) - “Semantic relatedness” (so called syntagmatic similarity) Two words can be considered to be semantically similar if they may be substituted for one another in a particular context and can be considered to be semantically related if they typically or significantly co-occur within the same context. Semantic similarity may be taxonomy based (path-length similarity) or distributionally based. In taxonomy based semantic similarity shorter path (in the hierarchy) between two concepts means more similar concepts. In distributionally based semantic similarity, less distance between distributions of two concepts means more similar concepts. The formal characterizations of distributions (in a window, in neighborhood, in a sentence or in a 16 linguistic based relation such as verb-object) are different in different methods. There are also different distance metrics to compute the distance of two distributions. (For more details cf. Eagles, 1996) Semantic relatedness also uses cooccurrence patterns to find similarities. It is somehow similar to distributionally based methods but in semantic relatedness two concepts are related if they occur in the same context which means that they cooccur with each other frequently while in distributionally similarity two concepts are similar if their contexts (distributions) are close to each other which means that their cooccurrent words are the same. For example cut and knife are semantically related as they occur frequently together in a context while knife and scissor are semantically similar as they occur in the same contexts (e.g. both are instruments for cutting). Computing Strategy: To compute the similarity between two clusters we may use the single link strategy in which the similarity between two clusters is the similarity of the two closest objects in them, complete link strategy in which the similarity of two clusters is the similarity of their two most dissimilar members or group average similarity in which the similarity is the average similarity between members. 3.4.3. Degree of Automation The knowledge acquisition phase may be carried out from manual to fully automatic. As this paper concerns ontology learning systems, we ignore systems with manual knowledge acquisition. Others use automatic (HASTI; Wagner, 2000), semi-automatic (TEXT-TO-ONTO, Todirascu, et al., 2000) and cooperative (HASTI, ASIUM) acquisition tools/methods. In semi automatic and cooperative systems, the role of user varies in a wide range. He may propose initial ontology, and validate/change different versions proposed by the system (Brewster, et al., 2001) or select patterns in the class relations (Suryanto & Compton, 2000) or control the generality levels, handle noise and label new concepts (ASIUM), labeling concepts, determining weights, validating the artificial sentences built by the system and confirming the system’s decisions (HASTI). 3.5.The Result This dimension is concerned with the result of the learning process and answers to question of “what would be built and what would be its features?” The first step to answer this question is to distinguish ontology learning versus support for building ontologies. Most of the systems studied here, learn ontologies (ontological structures), but some of them just support users, experts or other systems to learn ontologies. In other words some systems are autonomous ontology learning systems while some others are modules that perform a task and result in a set of intermediate data that will be used to build the ontology. In these latter kind of systems (such as DODDLE II; SVETLAN’; Moigno, et al., 2002) initial structures to build ontologies are acquired. For these systems the resulted ontology would not be built unless by the user or other systems. For example (Moigno, et, al., 2002) proposes utilizing NLP, corpus analysis and distributional analysis tools to help an expert build an ontology in surgical intensive care. For autonomous systems, which build ontologies, we consider their ontology features in this section. Among several features one can encounter for an ontology, we chose the most distinguishing ones between different OL systems. 3.5.1. Ontology Type Ontology type is made of several features indicating the nature of the intended ontologies such as coverage degree, usage or purpose, content type, detail level, etc. Coverage degree 17 denotes that the ontology is general (such as Cyc by Lenat & Guha, 1990) or special purpose and domain specific (such as DODDLE II). The usage and purpose of the ontology shows the potential applications in which it may be used and also the specific domain which it is designed for. For example the ontology in (Moigno, et al., 2002) is for the surgical intensive care domain while the one in (Kietz, et al., 2000) is for the insurance domain and (Faure & Poibeau, 2000) describe applying ASIUM in the terrorism domain. The applications in which these ontologies are used are different too. The most addressed application of these ontologies is information extraction and retrieval as in (Todirascu, et al., 2000; Faure & Poibeau, 2000). Other applications can be question answering, information brokering, search engines and natural language applications. The content type may be representation ontologies (e.g. Frame Ontology), natural language ontologies (e.g. WordNet) or domain ontologies. 3.5.2. Structure and topology Projects on learning ontologies work on different ontological structure and topology. Strict hierarchy of concepts (just one parent for each node) (Emde & Wettschereck, 1996), pyramid hierarchy (no two links crossing) (ASIUM), directed graphs (TEXT-TO-ONTO) and conjunction of hierarchies and axioms (HASTI) are some of structures used. 3.5.3. Representation Language Ontological knowledge can be represented by various representation languages, some of them are mentioned below. • Logic based languages such as KIF (Knowledge Interchange Format) (HASTI), description logics (Todirascu, et al., 2000) and KL-ONE family (SYNDIKATE), • Frame based Languages such as OKBC (Hwang, 1999), • Conceptual graphs (Roux, et, al., 2000), • Web based Languages such as XML (TEXT-TO-ONTO) and • Hybrid languages such as F-Logic based extensions of RDF (TEXT-TO-ONTO). The representation language of the resulted ontology becomes more important when the output of the ontology learning system is to be used in an existing application or be merged with other ontologies. 3.6.Evaluation Finding formal, standard methods to evaluate ontology learning systems is an open problem. To evaluate such systems there are two approaches: a) Evaluating the learning methods b) Evaluating the resulted ontology As comparing the accuracy of techniques for learning ontologies is not a trivial task, the first approach concerned with measuring the correctness of the learning techniques is less addressed. Thus, the more popular method to evaluate ontology learning systems is to (partially) evaluate their resulted ontology with one of the following approaches: b-1) Comparing two or more ontologies modeled within one domain using cross-evaluation techniques such as (Maedche & Staab, 2001b) which presents a multi-level approach for cross-evaluating ontologies. b-2) Evaluating domain ontologies through the application in which they are employed. As different systems learn different ontological elements, by applying different methods on different inputs, and the performance of proposed methods depends heavily on the applied 18 methods and textual sources on which they have been applied, comparing their results by a formal method is difficult. So most of proposed learning systems have their own testing and evaluation environment based on their application and selected domain. Such as the evaluation in (Agirre, et al., 2000) which is a task-oriented evaluation, via word sense disambiguation using SemCor corpus, or evaluating DODDLE II with small-scale case studies in the law field. Although it is difficult to establish and prove the quality of results, good indications can be found by the authors themselves. Most OL systems are evaluated by measuring the recall and precision of the learning module. The recall shows the number of correct concepts divided by the total number of concepts in the test set and the precision shows the number of correct concepts divided by the total number of extracted concepts. But still these results are not comparable because these systems work in different domains, on different inputs using different backgrounds. 4. Comparing the Systems in our Framework In section 2 we selected seven prominent Ontology learning systems and described their distinguishing features. Then we introduced a framework for classifying and comparing ontology learning systems in section 3. There we discussed the framework dimensions, subdimensions and some possible values for them and cited some examples (OL systems) on each value. In description of each dimension, we mentioned the position of the selected systems besides many other systems cited and discussed their features according to our framework. In this section we summerize these features in a table. Table (2) shows the features of the systems studied here in the framework introduced above. In this table columns show the dimensions and sub-dimensions of our framework and rows indicate the systems studied here. As the table shows, some major columns indicating dimensions of the framework are divided into minor columns indicating sub-dimensions (such as dividing ‘starting point’ into ‘prior knowledge’ and ‘input’ in which ‘input’ itself is divided into ‘type’ and ‘language’). And some major rows indicating systems are divided into minor rows (in part) indicating subsystems or different aspects/capabilities of a system (such as having sub-rows in the second row, showing the different elements learned by DOODLE II each using different learning methods and creating different outputs). Each cell in the intersection of a column (a dimension) and a row (a system) indicates the value(s) of the dimension for the system. As the table shows the systems are so selected to cover a wide range of values and have different values for each dimension. 5. Conclusion In this paper we introduced a framework for classifying and comparison of ontology learning systems and had an overview of some prominent ontology learning systems according to this framework. Dimensions of this framework help system developers and knowledge engineers to choose or build their desired OL system with appropriate features according to their requirements. In this section we will first summarize the differences, strengths and weaknesses of various values for the framework dimensions to help developers to choose the appropriate features and then give a brief list of open problems to improve the ability and performance of ontology learning systems. 5.1. Choosing an Appropriate Learning System To build a suitable ontology for an application (if there is not an existing one) we shall first notice to what we have (the starting point) and what we desire to have (the resulted ontology). Then we shall find an appropriate path from the starting point to the desired result. The path may be using an existing ontology building system or creating our own. • The starting point Choosing the starting point depends on both availability and necessity of background knowledge and the input. In environments for which background knowledge is unavailable, e.g. domains for which no base ontology (domain ontology) is developed or languages for which no semantic lexicon is available, we shall build the ontology from scratch. Building ontologies from scratch (with minimal pre-knowledge) has the advantage of turning round the integrating problems and also eliminates the knowledge acquisition bottleneck for creating the background knowledge by automating this process too. In this approach the system can acquire what it needs according to its application and ignore the superfluities, irrelevant or intemperate knowledge. Moreover decreasing the amount of initial knowledge will result in decreasing the developer-biasness of the result. This way the result would be more flexible and more justified for the special purpose it is applied. The main disadvantage of this approach is its longer process to learn more knowledge and its difficulties to resolve ambiguities caused by the lack of knowledge. The necessity and type of the background knowledge are also determined by the type of intended ontology and methods to be applied. For example building general purpose and domain specific ontologies may need different background knowledge: general ontology building relies most of the time on general lexical resources, while in specialized domains the language is more constrained and generally methods are based on computing distributional classes. The type of input depends heavily on the application. If we are building ontologies for the semantic web, we should accept semi-structured and unstructured data, while if our system learns ontological knowledge from databases it should accept and process structured data. • The resulted ontology Different applications require different ontologies. Ontologies may differ in their contents and the activities they support such as logical reasoning. For instance scientific, financial or business problem solving applications usually use small, narrow, deep ontologies with specialized details coded in axioms to solve their specific problems, while for some information retrieval systems (such as web search engines) wide but superficial ontologies containing concept hierarchies with few inter-relations, without axioms (such as WordNet) may be enough to improve the performance of their keyword search (Sowa, 2000). NLP applications use various kinds of 21 ontologies according to the depth of their language processing and reasoning. Applications that need deep natural language understanding and reasoning such as automatic programming (translation of natural language specifications to executable programs), some consulting and decision making programs with natural language interfaces and some question answering systems, require depth axiomatized background rich ontologies, with reasoning capabilities, while some other NLP applications such as commercial machine translation systems which are used to create a quick draft of a text in the destination language just use semantic lexicons such as WordNet. • The Learning Process The degree of automation: At present, although there are some fully automatic systems, they are very restricted, work under limited circumstances and have lower performance (according to their acceptable results) compared to semi automatic or cooperative systems. In other words cooperative systems give much more acceptable results because some interpretation decisions are left to the user during the learning process. A practical comparison between automatic and cooperative learning is done by some systems (HASTI; Finkelstein-Landau & Morin, 1999). Although this fact is generally valid for any learning system, it is more certain for systems extracting information from texts. The problem comes from the limitations of linguistic tools (taggers, parsers), together with the particular nature of language. For example in clustering based on distributional similarities, distributional classes don't correspond exactly with semantic classes, so the results can only give list of words from which the user decides if this is a class or not and if it is, then chooses its name and selects the right members. Fully automating this process should exploit multiple (hybrid) approaches to disambiguate results and complement each other. The learning approach: In learning approaches one should choose statistical versus symbolic methods. Statistical methods are blind or knowledge poor while symbolic methods are knowledge rich. So for systems in which there is no semantic analysis or no reasoning (such as information retrieval systems), statistical methods are applicable. Statistical methods are more computable, more general, more scalable and easier to implement, while symbolic approaches are more precise, more robust and give more reasonable results. Statistical methods are usually general and can be used for different domains or languages while some symbolic methods such as linguistic based or pattern driven methods need more adaptations. Statistical methods need less initial resources than symbolic ones; they do not consider background knowledge. The main disadvantage of statistical methods is that the data is usually sparse especially for general concepts in technical texts or technical concepts in general or irrelevant texts. Another characteristic of statistical approaches is that they are often used in offline (non-incremental) ontology learning. In other words the OL systems, which use these methods, usually learn from the whole input (e.g. from fully parsed text) at once. In symbolic methods we may learn incrementally, may exploit reasoning techniques and may use semantic knowledge to extract new knowledge. In symbolic approaches, the depth of processing is usually more than statistical approaches while its width is usually less. In other words symbolic methods often have higher accuracy and lower coverage than statistical methods. This can be seen in works such as WEB→KB which use both of them. The most knowledge intensive methods in this category (symbolic approaches) are the logic based ones. For systems that aim at deep understanding and reasoning and also 22 extracting meta-rules (rules to extract knowledge) the logical methods are appropriate. Another category of symbolic methods, which need prior knowledge are pattern or template driven methods, which (most of them) are in nature linguistic based too. They are the most popular symbolic methods to extract conceptual relations from texts. Although these methods have good performance in particular domains for extracting particular relations, they are limited and inflexible. In other words they extract limited relations and the cost of adapting them to a new domain or extracting new patterns for the new domain may be high. 5.2. Open problems Although there have been several works on different aspects on ontology learning, still there are open problems that need more attention. Much work has been done on extracting taxonomic relations, less work has been done on discovering non-taxonomic relations and little has been done on axiom learning. On the other hand most of the work done is domain-specific and has built domain-specific ontologies. Automatic building of general ontologies still needs more work. Most proposed systems have been tested in small, limited domains and need enhancements to work in real applications. Some open problems to be considered to improve the field, are as follows: • Axiom learning: The only report we found on learning axioms is by HASTI, which learns some axioms in restricted circumstances. In this system the explicit axioms in conditional and quantified sentences in input texts are learned. There is ongoing work to extend it to learn implicit axioms from text too. • Evaluating ontology learning systems: Currently ontology learning systems are evaluated by evaluating their results in specific domains (for studying some ontology evaluation methodologies see Gomez-Perez, 1999; Gruninger & Fox; 1995 and for comparing ontologies see Maedche & Staab 2001b; 2002). Finding formal, standard methods to evaluate ontology learning systems by proving their learning methods or proving the accuracy, efficiency and completeness of the built ontology is an open problem. • Full automation of ontology learning process: Most systems use semi automatic, cooperative or supporting tools to learn ontologies. Moving toward automation and eliminating user intervention needs more research. • Integrating successful modules to build complete autonomous systems: There are some successful modules, methods or tools, which each carry out a learning task with high performance and leave others. Integrating them may eliminate their weaknesses and intensify their strengths. • Flexible, domain/language/application neutral ontology learning: Many proposed ontology learning methods and approaches highly depend on their specific environment consisting of language, domain, application and input. Moving toward flexible and general methods may eliminate the need for reconstruction of the learning system for new environments. 
Learning Web Service Ontologies: an Automatic Extraction Method and its Evaluation. Abstract. Successful employment of semantic Web services depends on the availability of high quality ontologies to describe the domains of these services. Building such ontologies is difficult and costly, thus hampering Web service deployment. As a solution, we describe an automatic extraction method that learns domain ontologies from textual documentations attached to Web services. We evaluate our method in two different domains, that of RDF ontology storage tools and that of bioinformatics services. The quality of the extracted ontologies is verified against high quality hand-built ontologies of the corresponding domains. We conclude that our method allows extracting a considerable amount of information for a domain ontology and that it is applicable across different domains. 1. Introduction The promise of the emerging Semantic Web Services field is that machine understandable semantics augmenting Web services will facilitate their discovery and integration. Several projects used semantic Web service descriptions in very different application domains (bioinformatics grid [38], Problem Solving Methods [25]). A common characteristic of these descriptions is that they rely on a generic description language, such as OWL-S [10], to specify the main elements of the service (e.g., inputs, outputs) and on an ontology containing knowledge about the domain of the service such as the types of offered functionalities (e.g., TicketBooking, CarRental) or the types of service parameters (e.g., Ticket, Car). We refer to these ontologies as domain ontologies. The quality of the used domain ontologies influences the complexity of reasoning tasks that can be performed with the semantic descriptions. For many tasks (e.g., matchmaking) it is preferable that Web services are described according to the same domain ontology. This implies that the domain ontology used should be generic enough to be used in many Web service descriptions. Domain ontologies also formally depict the complex relations that exist between the domain concepts. Such rich descriptions allow performing complex reasoning tasks such as flexible matchmaking. We conclude that building quality (i.e., generic and rich) domain ontologies is at least as important as designing a generic Web service description language such as OWL-S. The acquisition of semantic Web service descriptions is a time consuming and complex task whose automation is desirable, as signaled by many researchers in this field, for example [38]. Pioneer in this area is the work reported in [18] which aims to learn Web service descriptions from existing WSDL1 files using machine learning techniques. They classify these WSDL files in manually built task hierarchies. Complementary, we address the problem of building such hierarchies, i.e., Web service domain ontologies. Despite their importance, few domain ontologies for Web service descriptions exist and building them is a challenging task. A major impediment is the lack of guidelines on what knowledge such ontologies should contain and what design principles they should follow. In the bioinformatics domain, for example, different communities used different approaches to build very different ontologies for semantically describing Web services [19]. Further, in order to build a generic and rich domain ontology one would ideally need to inspect a large number of Web services in that domain. Several domains witnessed a rapid increase in the number of available Web services to several hundreds (600+ in bioinformatics). Therefore tools that support ontology curators to get a quick insight in these large and dynamic data sets become crucial. Our approach to the problem of building quality domain ontologies is motivated by the observations that textual sources attached to Web services (e.g., short descriptions of these services, the documentation of the code of the underlying software) (1) contain valuable information for building ontologies and that (2) they use natural language in a specific way. In fact, such texts belong to what is defined as a sublanguage in [15]. A sublanguage is a specialized form of natural language which is used within a particular domain or subject matter and characterized by a specialized vocabulary, semantic relations and syntax (e.g., weather reports, real estate advertisements). We implemented an ontology extraction method that leverages the sublanguage nature of these texts. In what follows, we start by describing the difficulties of domain ontology building in two domains that served as case studies for applying and evaluating our method (Section 2). Then we present the ontology learning method (Section 3), some considerations about its evaluation (Section 4) and the experimental results (Section 5). We list related work in Section 6, then summarize the paper and point out future work in Section 7. 2. Two Case Studies In this Section we describe the various difficulties in building domain ontologies in the domains of RDF(S) storage tools and bioinformatics services. 2.1. Case Study 1: RDF(S) Storage tools In the context of two projects semantic description of ontology storage tools was required: the KAON Application Server, a middleware system which facilitates the interoperability of semantic Web tools [35], and the AgentFactory project that performs configuration of semantically described Web services using agent-based design algorithms [33]. The major impediment in building a domain ontology for describing RDF(S) storage tools was that, while there are many tools offering ontology storage (a major survey [14] reported on the existence of 14 such tools), only very few are available as Web services (two, according to the same survey). Therefore, it is problematic to build a good domain ontology by analyzing only the available Web services. Our approach in this situation relied on the observation that, since Web services are simply exposures of existing software to Web-accessibility, there is a large overlap (often one-to-one correspondence) between the functionality offered by a Web service and that of the underlying implementation. Accordingly, we built a domain ontology by analyzing the APIs of three tools (Sesame [4], Jena [24], KAON RDF API [21]). The time consuming process of reading, understanding the documentation, identifying overlapping functionalities offered by the APIs of these tools and modelling them in an ontology required three weeks (for one person). The ontol- Figure 1.: RDF(S) Storage Doogy contains a hierarchy of concepts denoting functionalities main Ontology Snapshot. (e.g., AddData, AddOntology) as well as elements of the RDF Data Model (e.g., Statement, Predicate, ReifiedStatement) and their relations (see a snapshot in Fig. 1). 2.2. Case Study 2: my Grid Bioinformatics Services my Grid is a UK EPSRC e-Science pilot project building semantic grid middleware to support in silico experiments in biology. The experimental protocol is captured as a workflow, with many steps performed by Web services. Core to the infrastructure is an ontology for describing the functionality of these services and the semantics of the manipulated data. A key role of the ontology is to facilitate user driven discovery of services at the time of workflow construction. Several factors hampered the building of this ontology, transforming the process into a time consuming and difficult activity. First, ontology building itself is time consuming. The ontology was initially built with two months of effort by an ontology expert with four years experience in building description logic based biomedical ontologies. The ontology was built manually, initially by using the documentation for 100 EMBOSS services as a source of relevant terms2 . A second impediment was the dynamic nature of the field. The exponential rise in the number of bioinformatics Web services over the past year required a further two months effort to maintain and extend the ontology. However, its content currently lags behind that needed to describe the 600+ services available to the community. Thirdly, lack of tools hampered the process. At the time of development, tool support for handling separate ontology modules was minimal, hence the existence of one substantial ontology with distinct subsections covering the domains of molecular biology, bioinformatics, informatics and generic tasks, all under a common upper level structure. A fourth impediment that the ontology curator encountered was the lack of guidelines on how to build the domain specific ontology, or indeed how to relate it to upper level ontologies. Lacking guidance from the Web services field, the curator relied on design principles employed in other large open source biomedical ontologies such as openGALEN [31] and the TAMBIS ontology [2]. Currently only a part of this ontology (accounting for 23% of its concepts) provides concepts for annotating Web service descriptions in a forms-based annotation tool Pedro3 and is subsequently used at discovery time with or without reasoning to power the search [38]. Summarizing Section 2, we note that the ontology building activity in both domains was hampered by the large number of documents to be analyzed, the lack of guidelines about ontology building and the lack of tools to support the process. We conclude that, with the increasing number of available Web services, (semi-)automatic tool support for ontology curators will become crucial. The extraction method presented here addresses this need for ontology building support tools. We benefitted from work done in these two case studies to evaluate the extraction method. The corpora used for the creation of the manual ontologies were used as a basis for extraction while the manually built ontologies themselves served as Gold Standards to evaluate the extracted ontologies. 3. The extraction process Software documentation in general, and Web service descriptions in particular, employ natural language in a specific way. Our extraction method exploits the syntactic regularities which are inherent from the sublanguage nature of Web service documentations. Note that in many cases, for example as demonstrated by the presented case studies, the Web service descriptions also reflect the particularities of the domain sublanguage, for example bioinformatics in the second case study. Therefore, characteristics of the domain sublanguage can influence the extraction process. The ontology extraction consists of several steps, as depicted in Figure 2. During the first step of the extraction we annotate the corpus with Part of Speech (POS) linguistic information. In the second step, a set of surface patterns are applied on this linguistic Figure 2.: The Extracknowledge to identify potentially interesting information for ontol- tion process. ogy building. The next step, ontology building, transforms the extracted relevant information into ontological constructs. Finally, a pruning step excludes potentially uninteresting concepts from the ontology. In what follows we detail the last three extraction steps. 3.1. Surface Patterns. A set of patterns identify potentially interesting information for ontology building. They are surface patterns because, besides the POS tag linguistic information, they rely on surface knowledge such as the position of words in the sentence. We distinguish two major categories of patterns used to derive different types of information. 1. Identifying domain concepts. Domain concepts are depicted by the nouns in a corpus. We extract entire noun phrases where a noun phrase consists of a head noun preceded by an arbitrary (0 or more) number of nouns or adjectives known as its modifiers. To perform this extraction we used JAPE [12], a rich and flexible regular expression based rule mechanism. For example, this is the JAPE rule that identifies noun phrases: ( (DET)* (ADJ|NOUN|POS)* (NOUN) ):np -->:np.NP={} The pattern in the left hand side of the rule identifies all sequences of words starting with 0 or more determiners (e.g., the, a), 0 or more adjectives, nouns or possession indicators in any order and mandatorily finishing with a noun. DET, ADJ, NOUN and POS are placeholders for other rules identifying words that are part of these categories. The left hand side of the rule annotates the identified sequence as a noun phrase (NP). 2. Identifying functionalities. Besides domain concepts, a domain ontology used for Web service descriptions needs to specify types of functionalities that are frequently offered in that domain. We observed that, in the majority of cases, verbs identify the functionality performed by a service and nouns following these verbs refer to data structures that are involved in some way in that functionality. Therefore our extraction pattern identifies verbs and following noun phrases as potential information to be added to the domain ontology. Having identified and annotated noun phrases (NP) and verbs (VB) with two previous rules, the JAPE rule for identifying and annotating verbs and following noun phrases is straightforward. ( {VB}{NP} ):funct -->:funct.Functionality = {} 3.2. Ontology Building The ontology building step collects the results of the previous pattern based extraction. The extracted terms are used for building two different aspects of the domain ontology. Noun phrases are a basis for deriving a data structure hierarchy and the functionality information is used for building a functionality hierarchy. Lemmatization of the words returned by the patterns is performed before the terms are used for ontology building. Building the data structure hierarchy. We observed that many of the terms mentioned in the analyzed corpora (and especially in the bioinformatics corpus) expose a high level of compositionality, in the sense that they incorporate other meaningful terms as proper substrings. Our observation is backed up by a recent study of the Gene Ontology terms which proved that 63,5% of all terms in this domain are compositional in nature [28]. Another observation, also proven by this study, is that compositionality indicates the existence of a semantic relationship between terms. Namely, a term A contained as a proper substring of a term B is more generic than term B. This translates in the ontological subsumption relationship. The hierarchy building algorithm reflects our observations. If a concept A’s lexicalization is a proper substring of another concept B’s lexicalization (e.g., Site in AntigenicSite) then A is more generic than B and the corresponding subsumption relationship is added to the ontology. Also, if the lexicalization of two concepts B and C contain a common substring we speculate that this substring represents a valid domain concept (even if the string does not appear as a stand alone term in the corpus) and add it as a parent concept of B and C. As an illustration, Figure 3 depicts the data structure hierarchy for the Site concept. Such compositionality based hierarchy building has also been used in other ontology learning approaches ([6,37]). Building the functionality hierarchy. There are no clear guidelines in the field of semantic Web services about how functionality hierarchies should look like. Major semantic Web initiatives such as OWL-S [10], IRS [25] and WSMO4 model functionalities by including both the verb of the action and a directly involved data element in the functionality (e.g., BuyTicket). This modelling style was followed in case study 1. On the other hand, in the bioinformatics domain ontology from case study 2, functionality concepts denote action (e.g., Aligning) without mentioning the involved data structures. We provide ontology building modules that produce functionality hierarchies fulfilling either of these two modelling styles. 3.3. Ontology Pruning The pruning module filters out irrelevant concepts from the extracted ontologies. We employ a baseline pruning strategy which advocates that frequent terms in a corpus denote domain concepts while less frequent ones lead to concepts that can be safely eliminated from the ontology [20]. We consider the average frequency of the terms as a threshold value and prune all concepts that have a lower frequency than this value. Another heuristic for the pruning is based on the observation that noun phrases included within a Functionality annotation by our rules are more likely to denote domain concepts. Therefore, if a low frequency data structure concept’s lexicalization was identified within a Functionality annotation and the corresponding functionality concept was not pruned then the data structure concept will not be pruned either. 3.4. Implementation The extraction method5 is implemented using the GATE [11] framework. The linguistic preprocessing was entirely performed using processing resources offered by GATE: a tokenizer, a sentence splitter and the Hepple POS tagger [17]. We implemented our extraction patterns using the JAPE mechanism which is part of the framework. Other modules of our method (e.g., Ontology Building) were declared as GATE Processing Resources. The data used by our method (such as the linguistic knowledge or the structures identified by patterns) is represented as annotations on the analyzed texts. Both patterns and individual modules operate on these annotations and represent their output as annotations. We greatly benefitted from the support of GATE to (partially) perform the evaluation of our prototype by using its data storage and evaluation facilities. 4. Evaluation criteria Evaluation of ontology learning is an important but largely unsolved issue, as reported at the workshop [5] upon which this volume is based. Two evaluation stages are typically performed when evaluating an ontology learning method. First, term level evaluation assesses the performance of extracting domain relevant terms from the corpus. Second, an ontology quality evaluation stage assesses the quality of the extracted ontology. While term level evaluation can be performed by using the well-established recall/precision metrics, ontology quality evaluation is more subtle and there is no standard method for performing it. One approach is to compare an automatically extracted ontology with a Gold Standard ontology which is a manually built ontology of the same domain ([32] and in this volume), often reflecting the knowledge existing in the corpus used for the extraction ([9]). The goal of this approach is to evaluate the degree to which the ontology covers the analyzed domain. Another approach is to evaluate the appropriateness of an ontology for a certain task. Initial experiments with such a task-based ontology evaluation are reported in [30] (and in this volume). As a third approach, a perconcept evaluation by a domain expert is used in [27] (and in this volume). We consider that all these types of evaluation, depicted in Fig. 5, cover important and complementary aspects such as: 1. 2. 3. 4. What is the performance of the learning algorithm? - extraction performance Is the extracted ontology a good basis for ontology building? - expert evaluation Does the extracted ontology cover the analyzed domain? - domain coverage Does the extracted ontology support a certain task? - appropriateness for a task In our case studies we perform three out of the four types of evaluation. We first perform a term level evaluation (1). Then, we rely on the domain experts’ concept per concept based evaluation of the ontologies to conclude their usefulness for supporting the ontology building task (2). The domain experts in both cases are the curators of the corresponding Gold Standard ontologies. Finally, we compare the extracted ontologies to the corresponding Gold Standards to assess their domain coverage (3). In what follows, we present the methodology and metrics for performing each type of evaluation. 1. Extraction Performance. To measure the performance of the extraction modules we manually identified all the relevant terms to be extracted from the corpus. Misspelled terms were not considered for extraction. Then, using the Benchmark Evaluation Tool offered by GATE, we compared this set of terms with the ones that were identified through pattern based extraction. We use Term Recall (TRecall) to quantify the ratio of (manually classified) relevant terms that are extracted from the analyzed corpus (correctextracted ) over all terms to be extracted from the corpus (allcorpus ). Term Precision (TPrecision) denotes the ratio of correctly extracted terms over all extracted terms (allextracted ). T Recall = correctextracted allcorpus ; T P recision = correctextracted allextracted 2. Expert Evaluation. Assessing whether the extracted ontology offers a useful basis for building a domain ontology is important since the main goal of our research is supporting the ontology building task. During the concept per concept analysis of the extracted ontologies the domain experts rated concepts correct if they were useful for ontology building and were already included in the Gold Standard. Concepts that were relevant for the domain but not considered during manual ontology building were rated as new. Finally, irrelevant concepts, which could not be used, were marked as spurious. The higher the ratio between all relevant concepts and all concepts of the ontology, the better it supports ontology building. We express this ratio as ontology Precision (OP recision): OP recision = correct + new correct + new + spurious Very useful side effects of the expert evaluation were the opinion and suggestions of the experts. They provided valuable ideas for further improvements. 3. Domain Coverage. We evaluate domain coverage by comparing the extracted ontologies to the corresponding Gold Standard ontologies. According to [22], one of the few works on measuring the similarity between two ontologies, one can compare two ontologies at two different levels: lexical and conceptual. Lexical comparison assesses the similarity between the lexicons (set of terms denoting concepts) of the two ontologies. At the conceptual level the taxonomic structures and the relations in the two ontologies are compared. In this paper we only perform a lexical comparison of the two ontologies. Our first metric denotes the shared concepts between the manual and extracted ontology. This metric was originally defined in [22] as the relative number of hits (RelHit), then renamed in [9] to Lexical Overlap (LO). Let LO1 be the set of all domain relevant extracted concepts (which is a subset of all extracted concepts rated correct and new by the domain expert) and LO2 the set of concepts of the Gold Standard ontology. The lexical overlap is equal to the ratio of the number of concepts shared by both ontologies (i.e., the intersection of these two sets - also noted correct) and the number of all Gold Standard ontology concepts (noted all). LO(O1 , O2 ) = correct |LO1 ∩ LO2 | = |LO2 | all ; OI(O1 , O2 ) = |LO1 \ LO2 | new = |LO2 | all It is interesting to analyze the concepts that are not part of this overlap. Often, the extracted ontology can bring important additions to the manual ontology by highlighting concepts that were ignored during manual creation. We are not aware of any previously defined metric for measuring these additions. Therefore we define Ontological Improvement (OI) as the ratio between all domain relevant extracted concepts that are not in the Gold Standard ontology (noted new) and all the concepts of the Gold Standard ontology. 5. Experiments 5.1. Experimental setup We applied the extraction method in the context of both case studies6 . The ontology building algorithm was adjusted to follow the modelling principle employed by each Gold Standard ontology. We then evaluated the extraction results using the criteria discussed in the previous section. All evaluations were performed for both the DataStructure and Functionality part of the extracted ontologies as well as on the ontologies as a whole. In order to get an insight in the efficiency of our pruning heuristics we evaluated both a pruned and an un-pruned version of the extracted ontologies. By comparing the performance of the extraction method in the two case studies we wish to gain an insight in its applicability on data sets from different domains and of different level of complexity. 5.2. Corpora Case study 1: RDF(S) Storage Tools. The first corpus, Corpus 1, contains the documentation of the tools used to build the manual ontology (Jena, KAON RDF API, Sesame) and has 112 documents. Each document in the corpus contains the javadoc description of one method. Previous work showed that the short textual descriptions of these methods contain the most information and other javadoc elements such as the method syntax and the description of the parameters introduce a lot of noise severely diminishing the performance of the extraction [34]. Case Study 2: Bioninformatics services. Our experimental corpus for this domain (Corpus 2) consisted of 158 individual bioinformatics service descriptions as available at the EMBOSS Web site7 . We worked only on the short method descriptions since they are significant for Web service descriptions in general being very similar to descriptions found in online Web service repositories as SalCentral8 and XMethods9 . The detailed descriptions of the EMBOSS services present a specific layout which makes extraction much easier however using it would have biased our extraction methods towards this particular kind of documentation. 5.3. Results 1. Extraction Performance. The results of the first evaluation step, depicted in Table 1, indicate that we can extract the desired lexical information with a good confidence from both corpora. We observed that errors are mostly due to mistakes in the POS tagger’s output. Most often verbs at the beginning of the sentence are mistaken for nouns causing a lower recall for functionality pairs compared to the recall of data structures. A second source for errors are spelling and punctuation mistakes in the corpora. Corpus 1 has, from this perspective, a lower quality than Corpus 2 and, indeed, this affects the Term Precision. 2. Expert Evaluation. The results for the expert evaluation for the two case studies are depicted in Table 2 and Table 3 both for the ontology as originally extracted and its pruned version. Observe that the value of the term precision has a direct influence on the value of the ontology precision. A term precision value of 0.73 for the first corpus resulted in an ontology precision value of only 0.36, while for the second corpus a slightly higher term precision (0.81) resulted in an almost double value for ontology precision (0.63). Within both ontologies, the same influence can be observed for the data and functionality parts of the ontology: the data part has a lower extraction performance than the functionalities, in both cases, and this leads to lower ontology precisions. The pruning mechanism increased the ontology precision in both cases: in the first case study this value almost doubled, and in the second case it increased with 10 units leading to precisions over 0.6. This means that more than half of the concepts of both pruned ontologies are relevant for the analyzed domain. Besides our quantitative results, we derived several interesting observations from the comments of the domain experts. Recall vs. Precision. It seams that the cleanness of the ontology is not of major importance for the ontology engineer. Often even concepts that are not included in the final ontology are useful to give an insight in the domain itself and to guide further abstraction activities. We should therefore concentrate to increase the recall of our extraction process even at the expense of precision. Synonymy. During the evaluation, the expert recognized several potential synonym sets such as: {find, search, identify, extract, locate, report, scan}, {fetch, retrieve, return}, {pick, select}, {produce, calculate} or {reverse, invert}. Synonymy information is an important piece of knowledge for semantic Web services. Especially search and matchmaking algorithms would benefit from knowing which concepts are equivalent. Automatic acquisition of synonymy information remains an important are of future work. Abstractions. The experts often redistributed the extracted domain concepts according to their domain view. For example, two subclasses identified for Protein belong to different domains, molecular biology and bioinformatics, and have to be placed in the corresponding hierarchies accordingly. Such abstractions need to be still manually created according to the ontology engineers view on the domain. However, the abstraction step is considerably supported if the expert has an overview of relevant domain concepts. Support. The curators considered the extracted ontologies as a useful start for deriving a domain ontology. Several complex structures could be included in a final ontology without further changes (e.g., the Site hierarchy in Figure 3), or provided helpful hints about how certain concepts interrelate. The most appreciated contribution was that the learned ontologies even suggested new additions for the manually built ontologies. 3.Domain Coverage The unpruned RDF(S) ontology identified half of the concepts existing in the manually built ontology and many new concepts (see Table 2). Lexical overlap and ontological improvement registered higher values for the DataStructure part of the ontology probably as a direct result of the higher extraction recall for these elements from the corpus than for functionality terms. The pruning mechanism behaved optimally in this case. While it almost doubled the ontology precision (from 0.36 to 0.6) it only slightly affected the lexical overlap (from 0.5 to 0.44). As expected, ontological improvement was more affected (from 1.4 to 0.94) because many of the newly identified concepts have a low domain relevance. In this case our heuristic distinguishes between important domain relevant concepts and less important concepts. The comparison with the bioinformatics application ontology registered less success than in the previous case study (see Table 3). The unpruned ontology covered only 20% of the manual ontology, even if it suggested many new possible concepts. Pruning was less optimal in this case: it reduced both the overlap and the improvement to half while resulting in a low precision increase (from 0.63 to 0.74). One of the major reasons for this behavior is that, in bioinformatics, due to the compositionality of terms, deep DataStructure concept hierarchies are created where the frequency of the concepts decreases with their generality. These low frequency specialized concepts were pruned even if they are important. To avoid this, the pruning threshold should be decreased as advancing deeper into the hierarchy. Also, since the precision of the ontology was already high without pruning, we might have adopted a lower value for the pruning threshold as a whole. Our results suggest that the ontology curator worked rather independently from the given corpus during the building of the Gold Standard as he missed many concepts named in the corpus. Post-experimental interviewing of the curator revealed that the examination of the corpus was not meticulous. He used just in time ontology development: concepts were added if needed for describing a certain service. Note also that he worked on a subset of our analyzed corpus (100 service descriptions instead of 158 analyzed by us). Name changes could also account for the mismatch. The curator expanded abbreviations or created a preferred term for several synonyms (e.g., Retrieving for fetch, get, return). He acknowledged that the automatic approach leads to a more faithful reflection of the concepts the community uses to describe their services. The major cause for ontological losses was that the curator also included concepts about the fields of biology, bioinformatics and informatics that are not present in the corpus (see Section 2). 5.4. Conclusions The evaluation suggests that the extraction method is domain independent (performing similarly in two different domains) even if it is influenced by particularities of the analyzed corpora. In both case studies the method (1) efficiently extracts important terms from the corpus and (2) builds ontologies containing a majority of domain relevant concepts. Punctuation and spelling mistakes lead to a low extraction precision, and consequently, to a less precise ontology. The extracted ontologies (3) contain a fair part of the manually identified concepts and (4) suggest a lot of new additions complementing manual ontology building that tends to ignore much of this knowledge. The pruning mechanism (5) performs less optimally in the case of the second corpus suggesting that cleaner corpora need a lower pruning threshold. 6. Related Work The problem of automating the task of Web service semantics acquisition was addressed by the work of two research teams. Hess and Kushmerick [18] employ the Naive Bayes and SVM machine learning algorithms to classify WSDL files (or Web forms) in manually defined task hierarchies. Our work is complementary, since we address the acquisition of such hierarchies. Also, our method does not rely on any manually built training data as the machine learning techniques do. Patil et al. [29] employ graph similarity techniques to determine a relevant domain ontology for a WSDL file and to annotate its elements. Currently they determine the semantics of the service parameters and plan to concentrate on functionality semantics in the future. They use existing domain ontologies and acknowledge that their work was hampered by the lack of these ontologies. The ontology learning field offers a wide range of different approaches to ontology acquisition. While most work is targeted on specific domains we are not aware of any efforts that analyze software documentation style texts. Several generic ontology learning tools exist, namely Text-To-Onto [23], OntoLearn [26] or OntoLT [6]. We tried to extract a domain ontology from our corpora using Text-to-Onto, the only tool publicly available at the time of the experiments. The results were suboptimal due to the strong particular-ities of our corpora which hampered the efficiency of the generic methods implemented by the tool. In contrast, our method is tailored for dealing with Web service descriptions while, as our experiments prove in this paper, it is applicable across domains. Pattern based term extraction is a core part of our method. Pattern based techniques have been first used to derive semantic relations from large corpora. A pioneer in this direction of research was the work of Hearst which introduced the idea of learning hyponymy relations using lexico-syntactic patterns. Lexico-syntactic patterns are defined on both lexical and basic syntactic information (POS tags). As such, they allow extracting relations after shallow text processing only. For example, the hyponymy relationship suggested by Bruises, wounds, broken bones or other injuries could be extracted using the NP, NP*, or other NP pattern [16]. As a follow up of this work, Charniak developed a set of lexico-syntactic patterns that identify meronymy (partOf) relations [3]. Naturally, such patterns have a clear relevance for ontology learning. Indeed, Hearststyle patterns have been used in the work of Cimiano [8] or in the CAMELEON tool which incorporates over 150 generic patterns for the French language [36,1]. While such generic patterns work well in general corpora they often fail in small or domain specific corpora. In these cases domain-tailored patterns provide a much better performance [1]. Besides using domain tailored patterns one can enlarge the extraction corpora. For example, World Wide Web data can be used for pattern based learning [7]. In several ontology learning approaches, as in our work, pattern based extraction is just a first step in a more complex process [32,13,6]. In these cases patterns identify potentially interesting terms in the corpus and the next processing steps derive relevant semantic structures from them. 7. Summary and Future Work While domain ontologies are of major importance for semantic Web services, their acquisition is a time consuming task. We have developed an extraction method that relies on the sublanguage characteristics of textual sources attached to Web services to extract such domain ontologies. We have evaluated the ontology learning algorithm in two different domains and concluded that it is applicable across different domains. The experts indicate that the extracted ontologies represent more faithfully the knowledge in the corpus and that they provide a useful start for building a Web service domain ontology. One could imagine the use of our method in several application scenarios. We believe it is useful for providing a first insight into a domain and providing a sketch ontology that can be the basis for ontology construction. During our experiments it turned out that such a tool is also useful when an ontology already exists in a domain because it can suggest new additions that were ignored during manual ontology building. Also, as the domain changes such a method can help to discover newly available domain knowledge and to integrate it in the existing ontology. Finally, it can be the basis of harmonization efforts when two existing ontologies are merged by being applied on the union of two different corpora that underlaid the creation of each ontology. As future work we wish to extend the method with more fine-grained extraction patterns to complement the current high coverage patterns. We can also enhance the step of ontology building. Use of synonymy information during the conceptualisation step is an important development possibility. We would also like to concentrate on strategies that enrich the basic extracted ontology. For example defining different views on a set of domain concepts or providing a set of patterns that act on the extracted semantic information. Yet another development dimension is the use of external knowledge sources (such as WordNet for synonymy detection or WSDL files for input/ output information) to complement the small corpora. Further, the usability of the extraction tool could be enhanced by providing auxiliary information for each concept such as text segments in the corpus where it appears and the frequency of its appearance. Acknowledgments. We thank C. Goble and C. Wroe for providing us with the Grid experimental data and evaluating the extracted ontology. We thank the members of the GATE group for their support in the development of our prototype. 
Text2Onto A Framework for Ontology Learning and Data-driven Change Discovery. Abstract. In this paper we present Text2Onto, a framework for ontology learning from textual resources. Three main features distinguish Text2Onto from our earlier framework TextToOnto as well as other state-of-the-art ontology learning frameworks. First, by representing the learned knowledge at a meta-level in the form of instantiated modeling primitives within a so called Probabilistic Ontology Model (POM), we remain independent of a concrete target language while being able to translate the instantiated primitives into any (reasonably expressive) knowledge representation formalism. Second, user interaction is a core aspect of Text2Onto and the fact that the system calculates a confidence for each learned object allows to design sophisticated visualizations of the POM. Third, by incorporating strategies for data-driven change discovery, we avoid processing the whole corpus from scratch each time it changes, only selectively updating the POM according to the corpus changes instead. Besides increasing efficiency in this way, it also allows a user to trace the evolution of the ontology with respect to the changes in the underlying corpus. 1 Introduction Since ontologies provide a shared understanding of a domain of interest, they have become a key technology for semantics-driven modeling, especially for the ever-increasing need for knowledge interchange and integration. Semantic annotation of data with respect to a certain ontology makes it machine-processable and allows for exchanging this data between different applications. Therefore, ontologies are frequently used for the explicit representation of knowledge which is implicitly given by various kinds of data. Since building an ontology for a huge amount of data is a difficult and time consuming task a number of tools such as TextToOnto1 [17], the ASIUM system [8], the Mo’k Workbench [2], OntoLearn [21] or OntoLT [3] have been developed in order to support the user in constructing ontologies from a given set of (textual) data. However, all these tools suffer from several shortcomings. First of all, they all depend either on very specific or proprietary ontology models which can not always be translated to other formalisms in a straightforward way. This is certainly undesirable as ontology learning tools should be independent from a certain ontology model in order to be widely applicable and used. This is especially important in a context such as the Semantic Web in which different ontology models coexist next to each other. In Text2Onto2 we overcome this problem by representing the learned ontological structures at a meta-level in form of so called modeling primitives rather than in a concrete knowledge representation language. As in [11], a collection of instantiated modeling primitives can then be translated into any target language. In this way we are able to handle the most prevalent representation languages currently used within the Semantic Web: RDFS, OWL and F-Logic. Second, the interaction with end-users, in contrast to linguists or machinelearning specialists, has been largely neglected within such systems. As users are typically the ones who are most familiar with the domain, user interaction should be a central part of the system architecture. And third, most of these tools lack a certain robustness with respect to changes made to the data set. In fact, most state-of-the-art systems need to relearn the complete ontology once the underlying corpus has changed. Text2Onto is a complete re-design and re-engineering of our system TextToOnto, a tool suite for ontology learning from textual data [17]. Text2Onto targets all these problems by introducing two new paradigms for ontology learning: (i) Probabilistic Ontology Models (POMs) which represent the results of the system by attaching a probability to them and (ii) data-driven change discovery which is responsible for detecting changes in the corpus, calculating POM deltas with respect to the changes and accordingly modifying the POM without recalculating it for the whole document collection. The benefits of these key design choices are various. By assigning probabilities to the learned structures, the interaction with the user can be made more efficient by presenting him the learned structures ranked according to the certainty of the system or only presenting him the results above a certain confidence threshold. Moreover, in Text2Onto we store a pointer for each object in the POM to those parts of the document collection from which it was derived, allowing the user to understand why a certain concept, instance or relation was created and thus increasing the POM’s traceability. And finally, the POM allows to maintain even inconsistent alternatives in parallel thus relegating the task of creating a consistent ontology to the user. The benefits of data-driven change discovery are even more obvious. First, there is no need of processing the whole document collection when it changes thus leading to increased efficiency. Second, the user can explicitly track the changes to the ontology since the last change in the document collection thus being able to trace the evolution of the ontology with respect to changes in the underlying document collection. This paper describes the framework and architecture of Text2Onto. It does not focus on the evaluation of the single ontology-learning algorithms, which will be presented elsewhere. Nevertheless, we also briefly describe the algorithms implemented in the framework so far. 2 Architecture The architecture of Text2Onto (cf. figure 1) is centered around the Probabilistic Ontology Model (see Section 2.1) which stores the results of the different ontology learning algorithms (cf. section 2.4). The algorithms are initialized by a controller, the purpose of which is (i) to trigger the linguistic preprocessing of the data, (ii) to execute the ontology learning algorithms in the appropriate order and (iii) to apply the algorithms’ change requests to the POM. The fact that none of the algorithms has the permission of directly manipulating the POM guarantees maximum transparency and allows for the flexible composition of arbitrarily complex algorithms as described below. The execution of each algorithm consists of three phases: First, in the notification phase, the algorithm learns about recent changes to the corpus. Second, in the computation phase, these changes are mapped to changes with respect to the reference repository, which stores all kinds of knowledge about the relationship between the ontology and the data (e.g. pointers to all occurrences of a concept). And finally, in the result generation phase, requests for POM changes are generated from the updated content of the reference repository. The algorithms provided by the Text2Onto framework can be classified according to two different aspects: task, i.e. the kind of modeling primitives (see section 2.1) they produce, and type, that means the method which is employed in order to extract instances of the regarding primitives from the text. Each algorithm producing a certain kind of modeling primitive can be configured to apply several algorithms of different types and to combine their requests for POM changes in order to obtain a more reliable probability for each instantiated primitive (cf. [5]). Various types of pre-defined strategies allow for specifying the way the individual probabilities are combined. 2.1 The Probabilistic Ontology Model A Probabilistic Ontology Model (POM) as used by Text2Onto is a collection of instantiated modeling primitives which are independent of a concrete ontology representation language. In fact, Text2Onto includes a Modeling Primitive Library (MPL) which defines these primitives in a declarative fashion. The obvious benefits of defining primitives in such a declarative way are twofold. On the one hand, adding new primitives does not imply changing the underlying framework thus making it flexible and extensible. On the other hand, the instantiated primitives can be translated into any knowledge representation language given that the expressivity of the primitives does not exceed the expressivity of this target language. Thus, the POMs learned by Text2Onto can be translated into various ontology representation languages such as RDFS3 , OWL 4 and F-Logic [14]. In fact we follow a similar approach to knowledge representation as advocated in [11] and [19]. Gruber as well as Staab et al. adopt a translation approach to knowledge engineering in which knowledge is modeled at a meta-level rather than in a particular knowledge representation language and is then translated into different target languages. In Text2Onto we follow this translation-based approach to knowledge engineering and define the relevant modeling primitives in the MPL. So called ontology writers are then responsible for translating instantiated modeling primitives into a specific target knowledge representation language. The modeling primitives we use in Text2Onto are given below. The name of the corresponding primitive of Gruber’s Frame Ontology is shown in parenthesis where applicable: – – – – – – – concepts (CLASS) concept inheritance (SUBCLASS-OF) concept instantiation (INSTANCE-OF) properties/relations (RELATION) domain and range restrictions (DOMAIN/RANGE) mereological relations equivalence It is important to mention that the above list is in no way exhaustive and could be extended whenever it is necessary. The motivation for considering exactly these relations is the fact that the algorithms integrated in the framework are currently only able to learn is-a, instance-of, part-whole as well as equivalence relations and restrictions on the domain and range of relations. The POM is not probabilistic in a mathematical sense, but because every instantiated modeling primitive gets assigned a value indicating how certain the algorithm in question is about the existence of the corresponding instance. The purpose of these ’probabilities’ is to facilitate the user interaction by allowing her to filter the POM and thereby select only a number of relevant instances of modeling primitives to be translated into a target language of her choice. 2.2 Data-driven Change Discovery In order to define the task of data-driven change discovery we first distinguish between change capturing and change discovery. Change capturing can be defined as the generation of ontology changes from explicit and implicit requirements. Explicit requirements are generated, for example, by ontology engineers who want to adapt the ontology to new requirements or by the end-users who provide the explicit feedback about the usability of ontology entities. The changes resulting from this kind of requirements are called top-down changes. Implicit requirements leading to so-called bottom-up changes are reflected in the behavior of the system and can be induced by applying change discovery methods. Change discovery aims at generating implicit requirements by inducing ontology changes from existing data. [20] defines three types of change discovery: (i) structure-driven, (ii) usage-driven and (iii) data-driven. Whereas structuredriven changes can be deduced from the ontology structure itself, usage-driven changes result from the usage patterns created over a period of time. Datadriven changes are generated by modifications to the underlying data, such as text documents or a database, representing the knowledge modeled by an ontology. Therefore, data-driven change discovery provides methods for automatic or semi-automatic adaption of an ontology according to modifications being applied to the underlying data set. The benefits of data-driven change discovery are twofold. First, an elaborated change management system enables the user to explicitly track the changes to the ontology since the last change in the document collection thus being able to trace the evolution of the ontology with respect to changes in the underlying document collection. Second and even more important, there is no longer the need of processing the whole document collection when it changes thus leading to increased efficiency. Independently from a particular application scenario some requirements have to be met by any application which is designed to support data-driven change discovery. The most important one is, of course, the need to keep track of all changes to the data. Each change must be represented in a way which allows for associating with it various kinds of information, such as its type, the source it has been created from and its target object (e.g. a text document). In order to make the whole system as transparent as possible not only changes to the data set, but also changes to the ontology should be logged. If ontological changes are caused by changes to the underlying data, the former should be associated with information about the corresponding modification to the data. Moreover, the system should allow for defining various change strategies, which specify the degree of influence changes to the data have with respect to the ontology or the POM respectively. This permits to take into account the confidence the user has in different data sources or the fact that documents might become out-dated after a while. It is quite obvious that each algorithm in Text2Onto supporting automatic or semi-automatic data-driven change discovery requires a formal, explicit representation of two kinds of knowledge: First, knowledge about which concepts, instances and relations are affected by certain changes to the data and second, knowledge about how to react to these changes in an appropriate way, i.e. how to update the POM in response to these changes. Consequently, the concrete knowledge to be stored by an ontology extraction system depends on the way these algorithms are implemented. A concept extraction algorithm, for example, might need to store the text references and term frequencies associated with each concept, whereas a pattern-based concept classification algorithm might have to remember the occurrences of all patterns matched in the text. Therefore, in Text2Onto each type of algorithm is provided with a suitable reference store (see section 2). It is among the algorithm controller’s tasks to set up a suitable store each time a new algorithm is added. 2.3 Natural Language Processing Many existing ontology learning environments focus either on pure machine learning techniques [2] or rely on linguistic analysis [3, 21] in order to extract ontologies from natural language text. Text2Onto combines machine learning approaches with basic linguistic processing such as tokenization or lemmatizing and shallow parsing. Since it is based on the GATE framework [7] it is very flexible with respect to the set of linguistic algorithms used, i.e. the underlying GATE application can be freely configured by replacing existing algorithms or adding new ones such as a deep parser if required. Another benefit of using GATE is the seamless integration of JAPE which provides finite state transduction over annotations based on regular expressions. Linguistic preprocessing in Text2Onto starts by tokenization and sentence splitting. The resulting annotation set serves as an input for a POS tagger which in the following assigns appropriate syntactic categories to all tokens. Finally, lemmatizing or stemming (depending on the availability of the regarding processing components for the current language) is done by a morphological analyzer and a stemmer respectively. After the basic linguistic preprocessing is done, a JAPE transducer is run over the annotated corpus in order to match a set of particular patterns required by the ontology learning algorithms. Whereas the left hand side of each JAPE pattern defines a regular expression over existing annotations, the right hand side describes the new annotations to be created (see listing 1.1). For Text2Onto we developed JAPE patterns for both shallow parsing and the identification of modeling primitives, i.e. concepts, instances and different types of relations (c.f. [13]). Since obviously, both types of patterns are language specific, different sets of patterns for shallow parsing and ontology extraction have to be defined for each language. Because of this and due to the fact that particular processing components for GATE have to be available for the regarding language, Text2Onto currently only supports ontology learning from English texts. Fortunately, thanks to recent research efforts made in the SEKT project5 GATE components for the linguistic analysis of various languages such as German and Spanish have been made available recently. Since we want to provide full support for all of these languages in future releases of Text2Onto, we have already integrated some of these components, and we are currently working on the development of appropriate patterns for Spanish and German. 2.4 Algorithms This section briefly describes for each modeling primitive the algorithms used to learn corresponding instances thereof. In particular we describe the way the probability for an instantiated modeling primitive is calculated. Concepts In Text2Onto we have implemented several measures to assess the relevance of a certain term with respect to the corpus in question. In particular, we implemented different algorithms calculating the following measures: Relative Term Frequency (RTF), TFIDF (Term Frequency Inverted Document Frequency), Entropy and the C-value/NC-value method in [10]. For each term, the values of these measures are normalized into the interval [0..1] and used as corresponding probability in the POM. Subclass-of Relations In order to learn subclass-of relations, in Text2Onto we have implemented various algorithms using different kinds of sources and techniques following the approach in [5]. In particular we implemented algorithms exploiting the hypernym structure of WordNet [9], matching Hearst patterns [13] in the corpus as well as in the World Wide Web and applying linguistic heuristics mentioned in [21]. The results of the different algorithms are then combined through combination strategies as described in [5]. This approach has been evaluated with respect to a collection of tourism-related texts by comparing the results with a reference taxonomy for this domain. The best result obtained was an F-Measure of 21.81%, a precision of 17.38% and a recall of 29.95%. As the algorithm already indicates the confidence in its prediction with a value between 0 and 1, the probability given in the POM can be set accordingly. Mereological Relations For the purpose of discovering mereological (part-of) relations in the corpus, we developed JAPE expressions matching the patterns described in [4] and implemented an algorithm counting the occurrences of patterns indicating a part-of relation between two terms t1 and t2 , i.e. part-of(t1 ,t2 ). The probability is then calculated by dividing by the sum of occurrences of patterns in which t1 appears as a part. Further, as in the algorithm described above we also consult WordNet for mereological relations and combine the elementary probabilities with a certain combination strategy. General Relations In order to learn general relations, Text2Onto employs a shallow parsing strategy to extract subcategorization frames enriched with information about the frequency of the terms appearing as arguments. In particular, it extracts the following syntactic frames: – transitive, e.g. love(subj,obj) – intransitive + PP-complement, e.g. walk(subj,pp(to)) – transitive + PP-complement, e.g. hit(subj,obj,pp(with)) and maps this subcategorization frames to ontological relations. For example, given the following enriched subcategorization frame hit(subj:person,obj:thing,with:object) the system would update the POM with these relations: hit(domain:person,range:thing) hit with(domain:person,range:object) The probability of the relation is then estimated on the basis of the frequency of the subcategorization frame as well as of the frequency with which a certain term appears at the argument position in question. Instance-of Relations In order to assign instances or named entities appearing in the corpus to their correct concept in the ontology, Text2Onto relies on a similarity-based approach extracting context vectors for instances and concepts from the text collection and assigning instances to the concept corresponding to the vector with the highest similarity with respect to their own vector as in [1]. As similarity measure we use the Skewed divergence presented in [15] as it was found to perform best in our experiments. Using this similarity measure as well as further heuristics, we achieved an F-Measure of 32.6% when classifying instances with respect to an ontology comprising 682 concepts [6]. Alternatively, we also implemented a pattern-matching algorithm similar to the one used for discovering part-of relations (see above). Equivalence Following the assumption that terms or concepts are equivalent to the extent to which they share similar syntactic contexts, we implemented algorithms calculating the similarity between terms on the basis of contextual features extracted from the corpus, whereby the context of a terms varies from simple word windows to linguistic features extracted with a shallow parser. This corpus-based similarity is then taken as the probability for the equivalence of the concepts in question. 3 Graphical User Interface In addition to the core functionality of Text2Onto described above we developed a graphical user interface featuring a corpus management component, a workflow editor, configuration dialogues for the algorithms as well as tabular and graphbased POM visualizations. It will be available as an Eclipse6 plug-in which could facilitate a smooth integration into ontology editors at a later development stage. A typical usage scenario for Text2Onto is depicted by figure 2 (left). The user specifies a corpus, i.e. a collection of text, HTML or PDF documents, and starts the graphical workflow editor. The editor provides her with a list of algorithms which are available for the different ontology learning tasks, and assists her in setting up an appropriate workflow for the kind of ontology she wants to learn as well as to customize the individual ontology learning algorithms to be applied. Once the ontology learning process is started, the corpus gets preprocessed by the natural language processing component described in section 2.3, before it is passed to the algorithm controller. In the following, depending on the configuration of the previously specified workflow, a sequence of ontology learning algorithms is applied to the corpus. Each algorithm starts by detecting changes in the corpus and updating the reference store accordingly. Finally, it returns a set of requests for POM changes to its caller, which could be the algorithm controller, but also a more complex algorithm (cf. section 2). After the process of ontology extraction is finished, the POM is presented to the user. Since the POM unlike any concrete ontology is able to maintain thousands of conflicting modeling alternatives in parallel, an appropriate and concise visualization of the POM is of crucial importance for not overwhelming the user with too much information. Although several pre-defined filters such as a probability threshold will be available for pruning the POM, some user interaction might still be needed for transforming the POM into a high-quality ontology. Currently, two different visualization types are available: a tabular view showing a number of sorted lists for all kinds of modeling primitives and a graph-based representation which is depicted by figure 2 (right). After having finished her interaction with the POM, i.e. after adding or removing concepts, instances or relations, the user can select among various ontology writers, which are provided for translating the POM into different ontology representation languages. 4 Related Work Several ontology learning frameworks have been designed and implemented in the last decade. The Mo’K workbench [2], for instance, basically relies on unsupervised machine learning methods to induce concept hierarchies from text collections. In particular, the framework focuses on agglomerative clustering techniques and allows ontology engineers to easily experiment with different parameters. OntoLT [3] is an ontology learning plug-in for the Prot ́g ́ ontology e e editor. It is targeted more at end users and heavily relies on linguistic analysis. It basically makes use of the internal structure of noun phrases to derive ontological knowledge from texts. The framework by Velardi et al., OntoLearn [21], mainly focuses on the problem of word sense disambiguation, i.e. of finding the correct sense of a word with respect to a general ontology or lexical database. In particular, they present a novel algorithm called SSI relying on the structure of the general ontology for this purpose. Furthermore, they include an explanation component for users consisting in a gloss generation component which generates definitions for concepts which were found relevant in a certain domain. TextToOnto [17] is a framework implementing a variety of algorithms for diverse ontology learning subtasks. In particular, it implements diverse relevance measures for term extraction, different algorithms for taxonomy construction as well as techniques for learning relations between concepts [16]. The focus of TextToOnto has been so far on the algorithmic backbone with the result that the combination of different algorithms as well as the interaction with the user had been neglected so far. The successor Text2Onto targets exactly these issues by introducing the POM as a container for the results of different algorithms as well as adding probabilities to the learned structures to facilitate the interaction with the user. Common to all the above mentioned frameworks is some sort of natural language processing to derive features on the basis of which to learn ontological structures. However, all these tools neglect the fact that the document collection can change and that it is unfeasible to start the whole learning process from scratch. Text2Onto overcomes this shortening by storing current results in stores and calculating POM deltas caused by the addition or deletion of documents. Very related is also the approach of [12] in which a qualitative calculus is presented which is able to reason on the results of different algorithms, resolving inconsistencies and exploiting synergies. Interesting is the dynamic aspect of the approach, in which the addition of more textual material leads to a reduction in the number of hypothesis maintained in parallel by the system. Furthermore, as argued in the introduction, previously developed ontology learning frameworks all lack an explanation component helping the user to understand why something has changed in the underlying POM. In addition, most tools do not indicate how certain a learned object actually is, thus making it more difficult for the user to select only the most reliable findings of the system. 5 Conclusion and Outlook We have presented our framework Text2Onto with the aim of learning ontologies from textual data. Its novel aspects as compared to similar frameworks are: (i) the independence of the actual ontology model or knowledge representation language, (ii) the introduction of probabilistic ontology models allowing more sophisticated models of user interaction and (iii) the integration of data-driven change discovery strategies increasing the efficiency of the system as well as the traceability of the learned ontology with respect to changes in the corpus, thus making the whole process more transparent. In future versions of Text2Onto a graphical workflow engine will provide support for the automatic or semi-automatic composition of complex ontology learning workflows. For transforming the POM into a consistent OWL or RDF ontology we aim at a tight integration with the KAON evolution framework [20] which will allow to detect and resolve inconsistencies in the generated POMs. The development of the explanation component will be carried on with particular regard to the DILIGENT methodology [18]. By generating machine readable explanations we will make a major step in the direction of making Text2Onto part of the DILIGENT process. We are currently preparing an evaluation setting for comparing the results of the newly developed ontology learning algorithms with previous implementations provided by TextToOnto and other ontology learning tools. Moreover, a detailed user evaluation will offer valuable clues to the usability of the graphical user interface and the benefits gained from the availability of an explanation component. 6 Acknowledgments Research reported in this paper has been partially financed by the EU in the IST2003-506826 project SEKT (http://www.sekt-project.com) and the IST-200134038 project Dot.Kom (http://www.dot-kom.org). We would like to thank our students Simon Sparn, Stephan Oehlert, G ̈nter Ladwig and Matthias Hartung u for their assistance in implementing the system and all our colleagues for fruitful discussions. 
Wikipedia2Onto – Building Concept Ontology Automatically, Experimenting with Web Image Retrieval. Keywords: wikipedia, semantic concept, ontology, web image classification Received: May 15, 2009 Given its effectiveness to better understand data, ontology has been used in various domains including artificial intelligence, biomedical informatics and library science. What we have tried to promote is the use of ontology to better understand media (in particular, images) on the World Wide Web. This paper describes our preliminary attempt to construct a large-scale multi-modality ontology, called AutoMMOnto, for web image classification. Particularly, to enable the automation of text ontology construction, we take advantage of both structural and content features of Wikipedia and formalize real world objects in terms of concepts and relationships. For visual part, we train classifiers according to both global and local features, and generate middle-level concepts from the training images. A variant of the association rule mining algorithm is further developed to refine the built ontology. Our experimental results show that our method allows automatic construction of large-scale multi-modality ontology with high accuracy from challenging web image data set. Povzetek: Prispevek opisuje izgradnjo velike multimodalne spletne ontologije AutoMMOnto. 1 Introduction Real-world images always involve pictures with various backgrounds, object aspects, poses and appearances. Taking the animal classes in Figure 1 as an example, human-beings can easily differentiate the four classes. However, computers are not able to identify the difference in the same way. The varied background environment of the same Arctic Fox class can introduce great variance in global image features, while the subtle fur color difference between Arctic Fox and Fennec Fox makes it difficult to classify from local image features. It is also hard to identify the different distribution of colors over Maned Wolf or Dhole from spatial features. On the other hand, cues from the text on the corresponding web page could make a substantial contribution to the performance of image classification. For example, even a single keyword Kashmir could indicate the Dhole class, as Kashmir is the habitat of Dhole. Similar useful relationships which help to narrow down the final concepts include name, diet, and distribution relationships. Therefore, an effective way is to combine the images features with the text information for image retrieval, where ontology is utilized for this purpose. Ontology, which clearly defines concepts and their relationships in a domain, has been widely used in many information retrieval fields, including document indexing, i.e., extracting semantic contents from a set of text document, image retrieval and classification, i.e., using concepts either from image features or surrounding text for content representation, and video retrieval, i.e., using text in video captions for semantic concept detection. Note that most of the approaches involve external lexical dictionary or online category as ontologies. They certainly improve the performance. However, they also introduce the following major questions: 1. Is ontology just as same as a hierarchical collection of concepts? 2. Ontology has to be manually built, which is extremely time consuming. Can it be done automatically? 3. Can Ontology be scalable when it is extended to large domains? Through research on the use of ontology to better understand media information, we have provided our answers to the aforementioned questions: 1. Ontology is not just a hierarchical collection of concepts with parent-child relation. Details of the differences will be apparent as you read the details in this paper. 2. We do agree that one main difficulty that hedges against the development of ontology approaches is the extra work required in ontology construction and annotation. But there is a hope, and this paper describes our original attempt to use both structural and content features of Wikipedia to build a proposed hierarchy with not only hyponymy(is-a) or meronymy(part-of) relationships but also more real-life relationships. Therefore, the resulting semantic concept hierarchy of the built ontology, called AutoOnto, is consistent with real world knowledge and can be used to map text information on the web page to detect semantic concepts. 3. Scalability is indeed a problem when a single party or a consortium tries to create a whole ontology structure. However, the problem could be solved when we can import existing ontologies or newly created ontologies to merge with other ontologies. The important issue here is to understand and handle the similarities/dissimilarities of concepts existing in the respective ontologies, which is current interest to relevant groups in AI and ontologyFigure 1: An example of web image classes in our data set. Even though these images are portraying 4 difficult animal classes, it is easy for human-beings to identify the classes: Arctic fox has light-colored fur; Fennec fox has a pair of grotesque ears and ET-Style face; Maned wolf is featured with its black long legs; and Dhole has white fur spreading from its jaw to abdomen. However, it is not easy for image processing approaches to tell the classes apart due to the lack of discriminative low-level features. Note that we have manually built a text ontology, called ManuOnto, and shown that it can effectively help machine understand multimedia in a better way in our previous work [9]. In this paper, we first show that AutoOnto captures the relationships between concepts as well as, if not better than the manually-built ontology with bigger knowledge coverage and higher efficiency. Then, we train classifiers according to our 164 dimensional features (SIFT with opponent color angle) and generate middle-level concepts from the training result and integrate the AutoOnto to form AutoMMOnto (Auto Multi-Modality Ontology). The MAP results of our experiment on Google (top 200 retrievals) Image search, AutoOnto and AutoMMOnto (AutoOnto+ visual descriptions) are 0.7049, 0.8942 and 0.9125 respectively. We have therefore shown that our method allows automatic construction of large-scale multi-modality ontology with high accuracy from a challenging web image data set. Our contribution in this paper is concluded as follows: We propose a method to build large scale concept ontology from Wikipedia in a cost effective way. The generated ontology is able to extract additional information from the web pages and increase the concept detection accuracy. We also propose an association rule mining algorithm to refine relationships in the ontology. The resulting relationship set are more concise with higher precision. The rest of this paper is organized as follows. Section 2 introduces the related works. Section 3 discusses the Wikipedia category and structure information. Section 4 discusses how we use Wikipedia to automatically build the concept ontology. In Section 5 we propose an association rule mining algorithm to discover the key semantic relationships. Experiment results on our collected web image database are given in Section 6. We conclude this paper in Section 7. 2 Related works Due to the dependency between the external knowledge source and semantic concepts, the chosen knowledge source will affect the derived concepts and relationships for ontology construction, which is used to classify the existing ontology construction approaches. WordNet [1], developed at Princeton University, has been commonly used as such a lexical dictionary as it is able to group words into sets of synsets. The structured network makes it easy to derive semantic hierarchies for various domains. Some typical examples which practise WordNet directly as an ontology for object recognition and video search include [2, 3, 4]. However, the reason why WordNet works fine in these experiments is that only common concepts (e.g., car, dog, grass, tree) and relationships (hypernymy, meronymy) are employed. If concepts and relationships outside the scope of WordNet, they will not be included and cannot be utilized. For example, WordNet has limited coverage of less popular or more specific concepts like "mountain bike" or "bush dog". This limitation decides that WordNet only works on sparse or general concept domains. Also, WordNet is also disconnected from the update of natural language vocabulary which changes with almost everyday. Therefore, it is not able to work on domains with novel topics and concepts. Besides the above approaches, WordNet has also been used for assisting ontology building. For example, in [16], WordNet is dynamically included to extend a knowledge acquisition tool CGKAT. Particularly, toplevel concept of WordNet ontology is subordinated into Sowa Ontology for finalizing an ontology. Similar approach is also proposed in [17], which incorporates general purpose resources like WordNet and open web directory to build large scale ontology for document indexing and categorization. Particularly, it takes two steps to build an ontology. Firstly, an initial ontology consists of two sub-trees from both the web directory and WordNet synset graph respectively. Then, it iteratively fills the gaps and enriches the existing ontology with new concepts and relationships until the ontology is verified by domain expert to be usable. As a result, the ontology building process is only semi-automatic. Also, the proposed method lacks support from solid experiment results and the performance in real application is yet to be evaluated. With the rapid development of Internet, online categories seem to be a better choice for ontology construction, especially when some popular online categories also provide easy access [5, 6]. By indexing a huge number of web pages/topics, online categories cover most real world objects, activities, news and documents in a timely manner. Besides the hierarchical structure offered by these categories, web page submitters and category indexers also provide more related concepts with varied relationships, which further extend the coverage. Some approaches which use online resources to construct knowledge base include [7], wherein specific domain knowledge of animal is extracted from online animal category and image features to construct ontology for web image classification. The application indicates that with the evolution of ontologybased applications, finding a proper knowledge source has become an important issue. Among the existing online categories, there is an increasing interest in using Wikipedia as the resource for knowledge mining. In [18], a refinement on the Wikipedia category network is implemented step by step to generate taxonomy of is-a semantic links. All syntaxbased, connectivity-based, lexicon-syntactic based and inference-based methods are used to remove noisy links and set up correct is-a links. To compare and analyse the performance of the Wikipedia-based ontology, the manually built ontology ResearchCyc [19] and WordNet are used as the performance baseline. The evaluation shows that the automatically-built taxonomy is comparable with the two existing ontologies. However, the construction is discontinued at the level of domains. While the authors put more emphasis on drawing out a taxonomy of 105,418 is-a links, the broad coverage also makes the taxonomy insensitive to specific applications, as different applications need different emphasis on the domain knowledge. Other than using external resources to construct ontology, existing large-scale ontology constructions usually involve mass manual work. For example, LSCOM [8] aims to design a taxonomy with a coverage of around 1,000 concepts for broadcast news video retrieval. This approach is hampered by the tens of millions of human judgments required, which has been proved to be very ineffective and costly. In a word, most ontology constructions are either constructed on dependent domain or still involve mass manual work. And even those semi-automatic construction processes rely heavily on external knowledge resources, like the aforementioned lexical dictionary and online categories. Another disadvantage is apparent as the important merit of either dictionary or category is a hierarchical graph which connects concepts together. As a result, only shallow relationships like hypernymy/hyponymy(is-a) or meronymy(part-of) could be mined. These relations are not sufficient enough to support information mining from web images, which are usually attached to web pages with text information. Mining through such kind of text corpus involves more than the aforementioned semantic relationships. An ontology with enriched knowledge provides more discriminative information in web image retrieval, classification and annotation. Referring to the existing work, we can see that an advanced ontology for multimedia research and applications should meet the following requirements: 1) The ontology should be constructed automatically, so that when it is applied to extended domains, the scalability will not become the bottleneck. 2) The ontology should involve more than domain-specific concepts. Also, besides is-a or part-of relationships, deeper semantic relationships should also be included so that the ontology is a better imitation of human general knowledge. 3 Wikipedia concepts and structure Wikipedia is by far the biggest online free encyclopedia. It provides definitions for more than 2 million words and phrase concepts. This number is still growing as Wikipedia is based on online collaborative work and anyone can freely access, create and edit the page content of each concept. This open feature makes Wikipedia an up-to-date knowledge source, where even the latest concepts can be found. It also covers many concepts which are not commonly used and included in other electronic lexical dictionaries. In the following subsections, we will introduce some of Wikipedia's features which make it suitable for ontology construction. 3.1 Wikipedia category The underlying structure of Wikipedia can be described in two network graphs: category graph and article graph. In both graphs, nodes represent articles and edges represent links between articles. Basically, all the Wikipedia web pages are put into a subject category according to general knowledge. This structure is depicted as the category graph which has been proved to be a scale-free, small world graph by graph-theoretic analysis[20]. The category graph is formed following the taxonomy of concepts. Therefore, the links in category graph indicate either is-a or part-of relationships between the two connected concepts (a sample of the category graph is given in Figure 2). In this sense, the semantic relationships provided by the category graph is quite similar to the relationships provided by WordNet. When referring to specific article, the Wikipedia classification is listed in a separate Categories section. Besides the category graph, there is also an article graph which indicates the cross-references between Wikipedia web pages. In particular, the articles are nodes of the graph, which are hyperlinked to corresponding Wikipedia articles. These links indicate a direct semantic relationship between the two connected concepts. Compared with WordNet which mainly organizes word concepts according to synset, Wikipedia category provides a more formal classification of concepts. As a result, the extracted concepts and relationships are closer to a formal ontology with various semantic relationships. 3.2 Wikipedia web page In Wikipedia, each web page defines one concept according to general knowledge. Disambiguation is removed by separating different senses in different web pages. The searching in Wikipedia is straightforward as each web page has already been associated with the keywords. In most cases, the page title is the indexed keywords. The text information on the web page is divided into sections. Each section describes one aspect of the concept in details. Taking the concept Aardwolf as an example (see Figure 3), the main web page content includes physical characteristics, distribution and habitat, behaviour, and interaction with humans. From the viewpoint of concepts, each section is connected to the main concept with semantic relationships depicted as section titles. A concept graph is easily drawn from this web page content structure. On the right, the web page also provides a section of Scientific classification, which lists the zoology taxonomy of the animal. By integrating different concepts under the same domain Animalia, a big hierarchy picture can be easily constructed with the concepts positioned under corresponding branches. Compared to our manually built Animal Domain Ontology [7], the hierarchy generated from Wikipedia Scientific classification is more formally defined, and is considered to contain rigid domain information. 3.3 Concept coverage In comparison to WordNet, whose total number of words is limited to around 147,278, Wikipedia certainly contains more information. For our case, only 12 out of the 20 class names are covered by WordNet. Class names such as African wild dog, bat-eared fox, black jackal, bush dog, cape fox, Ethiopian wolf, fennec fox, golden jackal are all missing from WordNet. Such limitations make WordNet an incomplete appropriate resource for ontology learning. On the contrary, Wikipedia is more suitable for this task. The total number of words has reached 2 million and it keeps increasing significantly daily. It can cover almost all the relevant concepts in our experiment. 4 Automatic ontology construction – Wikipedia2Onto In this section we discuss the construction of our multimodality ontology. Similarly to our previous manually construction process, the automatic process includes 3 steps. Particularly, the key concepts in the animal domain and the taxonomic relations are firstly extracted from Wikipedia. Then, the narrative descriptions of particular animals, including relevant concepts and non-taxonomic relations, are extracted. Finally, the visual descriptions of each concept are added. Note that we do not use the XML corpus provided by Wikipedia directly for construction. Instead, we use a web page crawler to download relevant concept web pages before ontology building in advance. Such an approach makes it more flexible to build ontology for specific domain. Meanwhile, a dynamic connection to Wikipedia can ensure “freshness” of our concepts as Wikipedia web pages are edited from time to time. 4.1 Key concepts and taxonomic relations extraction Wikipedia has provided an entire category of many meaningful concepts, which is formed according to hypernymy relationships between concepts. In other words, Wikipedia category provides taxonomy of general concepts in natural language, which is much more precise than our in-door manually built one. Therefore, our Animal Domain Ontology, which is used to describe the taxonomy information of animal concepts, can be directly obtained from Wikipedia category. However, as the Wikipedia concepts under animal domain have some special content features, we use the Scientific Classification entry on each concept page as a shortcut. This entry provides animal taxonomy in a top-down manner, from Kingdom, Phylum, Class, Order, Family, Subfamily, Genus to Species. We then extract the hierarchy structure from this entry and form our Animal Domain Ontology. For example, Phylum is defined as a sub–class of Kingdom, while Class is defined as a subclass of Phylum. Since our ontology is only defined for general web image classification, we stop at Family level and do not go beyond Subfamily. Taking Aardwolf as an example, this concept belongs to the family of Hyaenidae. So when an input query suggests a concept of Hyaenidae, Aardwolf will also be considered as a matched concept. 4.2 Narrative descriptions extraction In the definition of ontology, what is alluded to but not formally stated is the modelling of concept relationships. In order to show that ontology is more than a set of related keywords, we have to prove that every concept in the ontology is different from a plain word. It should be understood as concepts supported by structures. When building the Textual Description Ontology, our main concerns are twofold: an ontology, which depicts the real world, should contain more descriptive concepts and relationships. These relationships convey general knowledge according to domain knowledge. On the other hand, the related concepts should contain a hierarchical structure, so that when we do concept inference, additional facts could be generated. Here is an example to illustrate the above concerns. South Africa is where the species cape fox lives. Therefore, South Africa is linked to cape fox with a named relationship hasDistribution. Given two other relations Zimbabwe is a part of South Africa and South Africa is part of Africa, one could reasonably infer that cape fox can also be found in Zimbabwe. And this possibility increases when additional information matches. Therefore, the first step is to find all the important terms. Some pre-process includes crawling Wikipedia web page of relevant concept and using HTML parser to filter irrelevant HTML codes. After that, we analyse the web page content to extract useful concepts and relationships. It is worth noticing that at the beginning of each web page, where a short paragraph is given as a brief introduction of the particular concept, some words are emboldened as alternative name or synonymy to the main concept. By extracting these words, a synonymous set is first constructed for the original concept. We use a hasName relationship to link it to the original concept. This relationship extends the naming information. In the next step, by analysing HTML tags of document title, section title and links to other pages, we locate the title of each section. Before we look into the details of the section content, we exam the section title to see if it contains relevant semantic relationships, like information about Distribution, Habitat, Diet, etc. Once the relevant keywords are discovered in the section title, we look into the details of the section and find candidate concepts for that particular relationship. Candidate concepts are defined as those that have their own Wikipedia web pages. For the normal plain text on the Wikipedia web page, we believe it is of trivial importance, thus has less contribution to the concept detection. Based on this assumption, we extract a set of concepts from the section for each relationship. While not all the candidate concepts are correct, an association rule mining is discussed later to improve the accuracy of the generated ontology. After the relationships and related concepts are collected, we do further hierarchical construction among all the concepts. This step is done based on the Wikipedia category structure, which offers a systematic categorization of all the concepts. The category information is listed as a separate section at the bottom of each Wikipedia web page. In most cases one Wikipedia concept belongs to several categories, some of which serve for Wikipedia administration purposes, such as Wikipedia administration. We remove these categories and keep the rest, which follow different categorical classification. And for each related category, we move one step further to find its parent category. In our current implementation we do five iterations, and construct a hierarchical structure of five levels for each concept. This step helps to formulate the information and introduce more structured concepts on top of the current ontology. To evaluate the performance of the proposed ontology system with other textual aware methods, we also follow the text processing part of [10] and use Latent Dirichlet Allocation(LDA) to find 10 latent topics from the web page text. And we take the top 20 words from each latent topic as the topic representation. However, the resulting clusters of words do not show explicit semantic meanings. We presume that it is due to the relative smaller size of text corpus. Therefore, the ontology approach is more appropriate on our median-sized data set. 4.3 H. Wang et al. Informatica 34 (2010) 297–306 Visual descriptions extractions In this section we discuss the visual description features for our concept ontology. We collect a median size collection of 4,000 animal web images together with the corresponding web pages as our experiment data set. More specifically, the data set contains 20 animal categories under the domain of canine. For our experiment, we use recognition techniques to build a visual vocabulary and train classifiers using support vector machine (SVM). We do not generate our own object detection techniques as these techniques have been extensively discussed in computer vision researches. Our aim is also to show that instead we follow the object detection techniques whose superiority has been proved in the latest researches[11]. We first use Harris-Laplace detector[12] which is scale invariant and detects cornerlike regions in the images as interest point and then use SIFT[13] descriptor to represent the shape information around the interest point. Color descriptor is also combined with SIFT descriptor. A 20 by 20 image patch around the centre of the interest point is generated to extract opponent angle features. In addition, a shift along the horizontal or vertical axis is made when boundary is within the patch range. The final descriptor is a vector of dimension 164, where 128 dimensions are from SIFT descriptor and 36 dimensions are from opponent angle descriptor. We build a vocabulary of 1,000 visual words based on k-means clustering result of feature vectors from all images. For each image in the data set, a histogram of visual words is calculated and then each image is represented by a vector whose dimension is 1,000. After feature space construction, half of the data set is used as training sample, which is of size 2,000. The training set is further divided into 5 parts for cross validation. After training, the relations between image feature concepts and the animal concepts are obtained. After construction, we use association rule mining to refine the initial ontology. 5 Association rule mining for ontology Wikipedia is an online collaborative work and the content is maintained by users, therefore a certain level of inherent noise must be expected. When we extract real-world relations besides hypernymy and meronymy relations into the ontology, we are extracting those relations from the Wikipedia web pages with text analysis techniques. A small set of wrong relations could be extracted either due to the complexity or correctness of the texts and the strategy we used for relation extraction. For association rule mining[14], the research has evolved from a flat structure with a fixed support value to variances that consider complex tree or graph structure with different support values. In order to enhance the correctness of semantic relations extracted, we develop a variant of association rule mining method which considers the hierarchical structure of the ontology and propose a new quality measure called Q measure for relation pruning. Here, we use Figure 5 to illustrate the idea of the Q measure. We can see concept Even-toed Ungulates has three children in the ontology, namely Deer, Yak, and American Bison. If the relation Gray_Wolf hasDiet Eventoed Ungulates is correct, the three relations Gray_Wolf hasDiet Deer, Gray_Wolf hasDiet Yak, and Gray_Wolf hasDiet American Bison should also be correct if a minimum support level is present. Given a sufficient large number of documents collected, the three relations should have the same frequencies (i.e., the expected value of 1/3). But in realty and with a smaller number of documents, the three relations have different frequencies. We therefore could compute a variance-like value Q by N Q  [ i 1 freq (C i ) 1 2  ] freq ( R) N (1) where Ci represents a child rule of a generalized rule R, and N is the number of children rules of R. For those relations with parent concepts, they would have a lower Q value although they have high frequencies. We therefore can efficiently remove those relations by looking at the Q value and the predefined support threshold. 6 Experiment result In this section, we evaluate the performance of our approach by using the built AutoOnto and AutoMMOnto for image retrieval. The matchmaking of concept ontology is defined as a process that requires the user specified domain concept repository to take an image's detected concept as the input, and return all the matched domain concepts which are compatible with the concept generated from the input concept. From the matchmaking result we can conclude which predefined concept the generated image concept corresponds to and what relationship can be find between two given concepts. In this step, reasoners(semantic matchmakers) are used to derive additional facts which are entailed in any optional ontologies and predefined rules, through process and reason over the knowledge encoded in the ontology language. We use both the description logic reasoner RACER[15] and an enhanced ranking algorithm [9] in the experiment. The matched concepts are attached with the web images as semantic labels. There are 20 classes of web images in our database and each class has 200 web images downloaded from Google Image Retrieval. The performance is computed using Average Precision (AP), which is defined as the average (interpolated) precisions at certain recalls where R is the total number of correct images in the ground truth, k is the is number of current retrievals, I j  1 if image ranked at jth position is correct and I j  0 otherwise, P(ri )  Rj j is the interpolated precision, and {r,P(r)} are the available recall-precision pairs from the retrieval results. By using AP, the PR curve can be characterized by a scalar. A better retrieval performance, with a PR curve staying at the upper-right corner of the PR plane, will have a higher AP, and vice versa. In the current experiment, we set j = 200. As MAP is sensitive to the entire ranking with both recall and precision reflected in this measurement, we will also give Mean Average Precision (MAP) We compare our result to both the Google Image Retrieval results and the manually built ontology results(namely ManuOnto and ManuMMOnto). The corresponding comparisons are shown in Table 1 and Table 2 respectively, where the Average Precision (AP) values for each class using different approaches are presented. From Table 1, we can conclude that text ontology improves the retrieval performance by formulating the text information into structured concepts. From Table 2, we can observe that the AutoMMOnto approach gives comparable performance to the ManuMMOnto approach. And in most classes, AutoMMOnto generates even better results by extracting more concepts from the web page text. And the MAP of the Google, ManuMMOnto and AutoMMOnto are 0.7049, 0.8942 and 0.9125, respectively. The result of MAP also shows an overall improvement. It is worth adding, AutoMMOnto requires minimal level of human involvement: Only the main domain concepts, which are the image classes in our case, are given by users according to experimental domain to build up the whole concept hierarchy in the domain. The result is encouraging, as it proves that it is viable to build largescale concept ontology from Wikipedia automatically for effective web image retrieval. Ranking results from several sample classes are also shown in Figure 6. 7 Conclusion and future works In this paper we have proposed Wikipedia2Onto, an approach that uses the content and structure features of the online encyclopaedia Wikipedia to build large-scale concept ontology automatically. The constructed ontology has automatically extracted more descriptive semantic relationships than most existing ontologies. More importantly, this ontology is a ready structure that can be used in semantic inference. Through association rule mining, our approach has detected 743 concepts with high accurate corresponding relations. Finally, it is shown that our approach will help to improve precise retrieval for images (with free text information) for various domains. The proposed approach largely dispenses with the conflict between cost and precision in ontology-based applications. We would also like to conclude by drawing the attention of the readers to Figure 7. The results from our AutoMMOnto search for “wild dog in Kashmir region” further show the potential of ontology in the better understanding of multimedia. 
On-To-Knowledge: Ontology-based Tools for Knowledge Management. Executive Summary. On-To-Knowledge, the European EU-IST project No. 10132, builds an ontology-based tool environment to speed up knowledge management, dealing with the large numbers of heterogeneous, distributed, and semi-structured documents typically found in large company intranets and the World-Wide Web. Results aimed for by the project are: (1) a toolset for semantic information processing and user access; (2) OIL, an ontologybased inference layer on top of the World-Wide Web; (3) an associated methodology; (4) validation by industrial case studies. This paper gives an overview of the On-To-Knowledge approach to knowledge management. 1. Heterogeneous Information Resources Need Semantic Access Support for information and knowledge exchange is a key issue in the Information Society. The exponential growth of online information on intranets and the Web leads to information overload. To cut down on the time wasted in searching and browsing, and reduce associated user frustration, much more selective user access is needed. This is possible by automatic meaning-directed or semantic information processing of online documents. The European IST project On-To-Knowledge builds tools that achieve this. Ontologies [1,2] are the key technology used to describe the semantics of information exchange. Defined as “specifications of a shared conceptualization of a particular domain”, they provide a shared and common understanding of a domain that can be communicated across people and application systems, and thus facilitate knowledge sharing and reuse. Ontologies will play a key role in growth areas such as knowledge management [3,4] and electronic commerce. In the US, funding agencies have recognized the importance of these issues by setting up the DAML program (http://www.darpa.mil/iso/ABC/BAA0007PIP.htm) that aims at machine-processable semantics of information resources accessible to agents. The World-Wide Web (WWW) has drastically boosted the availability of electronic information. Already the present first generation of the web has changed our daily practice, and these changes will become even more significant in the near future. However, the Web itself has to change if it is to reach the next level of user service [5]. Currently, the Web is an incredibly large, (mostly) static information source. The main burden in information access, extraction, and interpretation still rests with the human user. Document management systems now on the market have severe weaknesses: • Searching information: Existing keyword-based search also retrieves irrelevant information that uses a certain term in a different meaning, and misses information when different terms with the same meaning about the desired content are used. • Extracting information: Currently, human browsing and reading is required to extract relevant information from information sources since automatic agents do not possess the commonsense knowledge required to extract such information from textual representations, and they fail to integrate information spread over different sources. • Maintaining weakly structured text sources is a difficult and time-consuming activity when such sources become large. Keeping such collections consistent, correct, and upto-date requires mechanized representations of semantics that help to detect anomalies. • Automatic document generation would enable adaptive websites that are dynamically reconfigured according to user profiles or other aspects of relevance. Generation of semi-structured information presentations from semi-structured data requires a machine-accessible representation of the semantics of these information sources. Tim Berners-Lee coined the vision of a Semantic Web that provides much more automated services based on machine-processable semantics of data, and on heuristics that make use of these metadata. The explicit representation of the semantics of data accompanied with domain theories (i.e., ontologies) will enable a Knowledge Web that provides a qualitatively new level of service. It weaves together a net linking incredibly large parts of human knowledge and complements it with machine processability. Various automated services will support the human user in achieving goals via accessing and providing information present in a machine-understandable form. This process will ultimately lead to a highly knowledgeable world-wide system with specialized reasoning services that support us in many aspects of our daily life, becoming as central as access to electric power. The competitiveness of companies depends heavily on how they exploit their corporate knowledge and memory. Most information in modern electronic media is mixed-media and rather weakly structured. This holds for the Internet but also for large company intranets. Finding and maintaining information is a hard problem in weakly structured representation media. Increasingly, companies realize that their intranets are valuable repositories of corporate knowledge. But with the now rapidly increasing volumes of information, turning this into useful knowledge has become a major problem. Knowledge Management is about leveraging corporate knowledge for greater productivity, value, and competitiveness [3,4]. Due to Internet-enhanced globalization, many organizations are increasingly geographically dispersed and organized around virtual teams. Such organizations need knowledge management and organizational memory tools that encourage users and foster collaboration while capturing, representing and interpreting corporate knowledge resources and their meaning. On-To-Knowledge provides the tools to speed up knowledge management in large distributed organizations, by applying ontologies to electronic information as a basis for semantic information processing and fast, meaning-directed user access. 2. Tool Environment for Ontology-based Knowledge Management The On-To-Knowledge architecture and all its major components are shown in Figure 1. To illustrate these components and their interactions, we present a simple querying 2 scenario, where a user poses a query to the system that must be answered on the basis of a set of weakly structured data sources in a repository. Two example scenarios in our case studies are: • Querying a skills description repository on the Swiss Life intranet, where this repository is filled with a large variety of weakly structured documents (CV's, project descriptions, course descriptions, etc.). • Locating the material that is required to answer a query at one of the helpdesks operated by BT. Again, much of the relevant material is very heterogenous in nature: technical specifications of devices, previous fault reports, customer data, etc. The sequence of numbers in Figure 1 indicates the steps that must be taken in order to perform any of the above queries. Each of the components in Figure 1 is based on existing solutions already provided On-To-Knowledge consortium partners [6-10]. The red labels indicate which partner is providing crucial technology to each of the steps of the scenario. Step [1]. The system interacts with a user in order to elicit a specific query to be answered. Both the interaction with the user and the resulting query are entirely in terms of a domainspecific ontology, expressed in the OIL language developed within the consortium (see Section 3). The required ontologies are constructed using tools such as OntoEdit (http://ontoserver.aifb.uni-karlsruhe.de/ontoedit/), developed by the University of Karlsruhe [10]. Such an ontology-based user interaction has as main advantage that the user is shielded from any document-specific representation details, and can instead communicate in meaningful domain-specific terms. Furthermore, it makes the system much more robust against changes and variability in the formats of the underlying documents. Step [2]. The user interaction results in a specific query to be answered by the data repository layer. We rely on the Resource Description Framework (RDF, http://www.w3c.org/Metadata/) currently being developed by the World-Wide Web consortium (W3C), to structure the data repository and to express queries over this 3 repository. The required translation from OIL-based user interaction to RDF-based queries is feasible because OIL is itself definable in terms of RDF-Schema definitions. Step [3]. The consortium is developing an RDF query engine to efficiently process queries over medium-size data-repositories (with up to a million RDF triples in the repository). The University of Karlsruhe’s SilRI engine (http://www.aifb.uni-karlsruhe.de/~sde/rdf/) is a starting point for such an engine. Besides RDF, XML may also be used to represent part of the semantically annotated data in the repository. In that case, an XML query language like XQL or XML-QL forms the basis for an XML-based query engine. Step [4]. Of course, the above steps all assume that the data repository is filled with data that is annotated with sufficiently rich semantic information. Furthermore, the annotations must be related to the ontological vocabulary that was the basis for the original user query. Different technologies will be exploited to provide these annotations, depending on whether we are dealing with weakly structured data sources, or data sources that consist of free text only. In the first case, we will use wrapper technology in the vein of Jedi or W4. In the second case, the Corporum technology from CognIT ([9], http://www.cognit.no/) is the main platform for concept extraction from free text. Other tools will be based on automated summarization technology as developed for ProSum by BT [7,8]. Steps [5,6]. After the RDF query has been executed over the data repository, the resulting information is communicated to the user. Again, this must be done using an ontology-based vocabulary. Furthermore, powerful graphical visualizations of query results in the context of large data sets are developed. Examples of such visualizations are the semantic sitemaps produced by the WebMaster tool of AIdministrator [6] (for some results see Section 4). 3. OIL: Inference Layer for the Semantic World-Wide Web The technical backbone of On-To-Knowledge is the use of ontologies for the tasks of meaningful information access, integration, and mediation. A major result from the On-ToKnowledge project is OIL (the Ontology-based Inference Layer) [11]. OIL is a representation and inference layer on top of the Web that is based on ontologies. It unifies three important aspects provided by different communities: (i) formal semantics and efficient reasoning support as provided by Description Logics, (ii) epistemologically rich modelling primitives as provided by the Frame community, and (iii) a standard proposal for syntactical exchange notations as provided by the Web community. • Description Logics (DL). DLs describe knowledge in terms of concepts and role restrictions that are used to automatically derive classification taxonomies. The main effort of the research in knowledge representation is in providing theories and systems for expressing structured knowledge and for accessing and reasoning with it in a principled way. OIL inherits from DL its formal semantics and the efficient reasoning support developed for these languages. In OIL, subsumption is decidable and with the developed FaCT engine we provide an efficient reasoner for this. • Frame-based systems. The central modelling primitives of predicate logic are predicates. Frame-based and object-oriented approaches take a different point of view. Their central modelling primitives are classes (frames) with certain properties called attributes. Many other refinements of these constructs have been developed leading to the success of this modelling paradigm. Therefore, OIL incorporates the essential modelling primitives of frame-based systems into its language. OIL is based on the notion of a concept and the definition of its superclasses and attributes. Relations can also be defined not as an attribute of a class but as an independent entity having a certain domain and range. Like classes, relations can be arranged in a hierarchy. 4 • Web standards: XML and RDF. Modelling primitives and their semantics are one aspect of an ontology-based exchange language. Given the dominance and importance of the WWW, the syntax of such a language must be formulated using existing web standards for information representation. As already proven with XOL (http://www.ai.sri.com/pkarp/xol/), XML can be used as a serial syntax definition language for ontology-based information exchange. OIL is closely related to XOL and can be seen as an extension of it. Another candidate for a web-based syntax for OIL is RDF and RDFS. The RDF framework for the encoding, exchange, and reuse of structured metadata provides a means for adding semantics to a document without making any assumptions about the structure of the document. RDF schemas (RDFS) provide a basic type schema for RDF. Objects, Classes, and Properties can be described. In relation to ontologies, RDF provides two important contributions: a standardized syntax for writing ontologies, and a standard set of modelling primitives like instance-of and subclass-of relationships. Therefore, OIL offers two syntactical variants: one based on XML schema and one based on RDF schema. Why not Ontolingua? Ontolingua (http://ontolingua.stanford.edu/) is an existing proposal for an ontology language. It has been designed to support the design and specification of ontologies with a clear logical semantics based on KIF. Ontolingua extends KIF with additional syntax to capture intuitive bundling of axioms into definitional forms with ontological significance; plus a Frame Ontology to define object-oriented and framelanguage terms. The problem with Ontolingua is its high expressive power provided without any means to control it. Not surprisingly, no reasoning support has been provided for Ontolingua. OIL takes the opposite approach. We start with a very simple and limited core language. The web has proven that restriction of initial complexity and controlled extension when required is a very successful strategy. OIL takes this lesson to heart. The use of OIL is currently evaluated in two running IST projects On-To-Knowledge and Ibrow (http://www.swi.psy.uva.nl/projects/ibrow/home.html). In On-To-Knowledge, OIL will be extended to a full-fledged environment for knowledge management in large intranets and websites. Unstructured and semi-structured data will be automatically annotated, and agent-based user interface techniques and visualization tools will help the user to navigate and query the information space. Here, On-To-Knowledge continues a line of research that was set up with SHOE and Ontobroker [5]: using ontologies to model and annotate the semantics of information resources in a machine-processable manner. 4. Business Applications in Semantic Information Access Industry case studies. On-To-Knowledge is carrying out three industrial case studies to evaluate the tool environment for ontology-based knowledge management (Section 2) and the associated web inference layer OIL (Section 3). These case studies are chosen such that they ensure a broad coverage, involving three different industry sectors (insurance, telecom, energy) in three different countries, and facing different knowledge management problems. Swiss Life: organizational memory. Swiss Life's vision is to build an organizational memory with an intranet-based portal. Three case studies explore the problem space: 1. A skills database contains a large variety of structured and unstructured documents like CVs, recruitment profiles, course and project descriptions. Today these documents do not exist or are not integrated into a single repository. Furthermore, there is no common vocabulary (i.e. ontology) that guarantees a unified usage and understanding of the documents, resulting in insufficient retrieval results. 5 2. Information about an insurance product comprises documents for sales persons, for training purposes, about performing office tasks, etc. This information is created in different places, in different formats and often not distributed to the right places. 3. The IAS document ("International Accounting Standards") is part of the global Swiss Life Intranet, called "GroupNet". The document's 1000 web pages make it very hard to find relevant passages, even though there is a division into chapters and sections. BT: call centres. Call Centres are an increasingly important mechanism for customer contact in many industries. Every transaction should emphasize the uniqueness of both the customer and the customer service person. To do this one needs effective knowledge management [7, 8]. This includes knowledge about the customer but also knowledge about the customer service person, so that the customer is directed to the right person to answer their query. This knowledge must also be used in a meaningful and timely way. One or more of BT’s own Call Centres will be targeted to identify opportunities for effective knowledge management using the On-To-Knowledge tools. More specifically, call centre agents tend to use a variety of electronic sources for information when interacting with customers, including their own specialized systems, customer databases, the organization’s intranet and, perhaps most importantly, case bases of best practice. The On-To-Knowledge techniques provide an intuitive front-end to these heterogeneous information sources, to ensure that the performance of the best agents is transferred to the others. EnerSearch: virtual enterprise. EnerSearch is a virtual organization researching new ITbased business strategies and customer services in deregulated energy markets (e.g., [12], see further http://www.enersearch.se). Its research affiliates and shareholders are spread over many countries: its shareholding companies include IBM (US), Sydkraft (Sweden), ABB (Sweden/Switzerland), PreussenElektra (Germany), Iberdrola (Spain), ECN (Netherlands), and Electricidade do Portugal. Essentially, EnerSearch is a knowledge creation company, knowledge that must be transferred to its shareholders and other interested parties. Its website is one of the mechanisms for this. However, it is rather hard to find information on certain topics – the current search engine supports free text search rather than content-based search. Therefore, the EnerSearch case study evaluates the On-To-Knowledge toolkit to enhance knowledge transfer to (1) researchers in the EnerSearch virtual organization in different disciplines and countries, and (2) specialists from shareholding companies interested in getting up-to-date information about R&D results on IT in Energy. 6 Some first results of On-To-Knowledge techniques are shown in Figure 2. It shows two semantic structure maps of the EnerSearch website, produced by the WebMaster tool of AIdministrator [6], and based on a domain ontology concerning important IT in Energy research topics. Every node represents a webpage (that can be directly opened in a browser by clicking on the node); edges denote hypertext links. Left, we see a map of subtypes (subtopics) of agent research by EnerSearch. It is easy to see how subtopics are related and find the relevant webpages. Right, we see an interactively generated sitemap showing how the topic of e-commerce intersects with other topics (dark blue nodes). A nice feature of the visualization is that spatial proximity correlates very well with semantic closeness. Methodology. In addition to the toolset and the OIL language, On-To-Knowledge is developing an associated methodology for ontology-based knowledge management. Input to this are existing European research results, such as the CommonKADS approach to knowledge engineering and management [3], experiences from knowledge-based software engineering [12] and tool development [5-10], ontology composition [2] and information retrieval techniques [14], and feedback from industry case studies. The methodology will also cover how to develop the business case for ontology-based knowledge management. Conclusion. World-Wide Web and company intranets have boosted the potential for electronic knowledge acquisition and sharing. Given the sheer size of these information resources, there is a strategic need to move up in the data – information – knowledge chain. As a necessary step, On-To-Knowledge provides innovative tools for semantic information processing and thus for much more selective, faster, and meaningful user access. 
Building Domain Ontologies from Text for Educational Purposes. Abstract—This paper presents a semiautomatic framework that aims to produce domain concept maps from text and then to derive domain ontologies from these concept maps. This methodology particularly targets the e-learning and Artificial Intelligence in Education (AIED) communities as they need such structures to sustain the production of e-learning resources tailored to learners’ needs. This paper details the steps to transform textual resources, particularly textual learning objects (LOs), into domain concept maps, and it explains how this abstract structure is transformed into a formal domain ontology. A methodology is also presented to evaluate the results of ontology learning. The paper shows how such structures (domain concept maps and formal ontologies) make it possible to bridge the gap between e-learning and Intelligent Tutoring Systems by providing a common domain model. Index Terms—Applications and expert knowledge-intensive systems (education), intelligent Web services and Semantic Web knowledge acquisition, knowledge reuse. Ç 1 INTRODUCTION T HE importance of automatic methods to enrich knowledge bases from free text is acknowledged by the knowledge management and ontology communities. Developing a domain knowledge base is an expensive and timeconsuming task, and static knowledge bases are difficult to maintain. This is especially true in the domain of online training. Generally split into e-learning, adaptive educational hypermedia, and Intelligent Tutoring System (ITS) communities, the online educational community lacks common views, methods, and resources to build a knowledge base [10]. In fact, integration and cooperation between the e-learning and the ITS communities can only benefit all groups. On one hand, e-learning-based environments focus on the reusability of learning resources. However these resources are not adaptable to suit learners’ needs, they fail to use explicitly stated instructional strategies, and they lack rich knowledge representations. On the other hand, ITSs exploit rich knowledge structures, provide adaptive feedback, and implement pedagogical strategies. However, their knowledge base is not generally reusable as it is dependent on the application domain and proprietary programming. If we consider learning objects (LOs) as resources to semiautomatically build ITS knowledge bases, then it should be possible to exploit the benefits of both worlds. In fact, various studies have requested that LOs be enriched semantically [21], [26], [31], as well as announcing the creation of the educational Semantic Web [4]. The LORNET project [29], [38], a significant Canadian initiative to reach this goal, aims at providing Semantic Web applications for e-learning and Knowledge Management systems. Along the same line of research, the Knowledge Puzzle Project proposes building a knowledge base that is usable by both communities (ITS and e-learning). A domain ontology is central to this knowledge base. Other required structures include the learning objective and instructional strategy specifications. This paper focuses mainly on the domain model and describes a semiautomatic methodology and tool, TEXCOMON, to build domain ontologies from English text. One of the distinctive features of TEXCOMON’s approach lies in the use of intermediate knowledge models (concept maps) to generate the domain ontology. Since this ontology is dedicated to education, we propose the use of concept maps as intermediate structures. Concept maps tend to make the structure of a body of knowledge much more significant for human users than other forms of knowledge representation [35]. Hence, they are more easily validated and enriched by a domain expert. Concept maps also foster meaningful learning and index sentences at a fine-grained level, which is required for efficient LO indexing and retrieval. In order to promote interoperability and reuse, concept maps pass through an export process that outputs a lightweight domain ontology. Building ontology learning frameworks still requires the supervision of a domain expert. To validate the resulting ontology, this paper also describes an evaluation methodology and presents the results obtained in an evaluation with a corpus of text documents. The Knowledge Puzzle offers a series of services that can be used in both e-learning and ITS environments. Such services are used to exploit the generated knowledge base for computer-based training. The paper is organized as follows: First, related work is presented (Section 2) before the philosophical foundations of the Knowledge Puzzle are described and certain definitions are provided (Section 3). Second, the semiautomatic methodology for knowledge acquisition from text is described, as are the domain concept maps and the generation of ontologies (Section 4). Third, we present an approach to evaluate the ontology and perform comparative analyses with Text-To-Onto [28] (Section 5). Fourth, the value of the approach for the educational community is highlighted by demonstrating how the generated ontology can sustain a knowledge base that offers a series of services to the Artificial Intelligence in Education (AIED) community (Section 6). Finally, conclusions are drawn. 2 RELATED WORKS This section presents related work on concept maps and domain ontology generation. It also describes how domain ontologies enrich LOs. 2.1 Generating Concept Maps Concept maps are a valuable resource whose rich structure can be exploited to retrieve information and train learners. This paper introduces a solution to semiautomatically generate concept maps from domain documents. These concept maps are useful to support meaningful learning and serve both as a knowledge base for building domain ontologies and as a skeleton for composing more detailed LOs. Knowledge extraction is based on lexico-syntactic and semantic patterns. Previous and concurrent studies have also attempted to generate concept maps from documents [13], [49]. The main difference with Knowledge Puzzle’s approach is that such studies do not attempt to convert concept maps into domain ontologies. Furthermore, they fail to use other nonverbal forms of knowledge, such as prepositional forms to link phrases, unlike the Knowledge Puzzle. Our approach provides for the progressive construction of concept maps with meaningful domain concepts. In this novel proposal, entire sentences are exploited in order to find as many semantic relationships as possible, whereas other approaches only exploit verbal relationships. Automatic authoring efforts have also been made in the area of adaptive hypermedia systems [4], [5], [15]. Such efforts include adding missing attributes in the domain model by performing link analysis [5] and creating new links using some metrics such as relatedness calculations [15]. However, these efforts did not try to make concept maps emerge from text, which is of major importance for automatic indexing. 2.2 Generating Domain Ontology Generating and populating ontologies from text are two very active research fields. Related projects include: Mo’k [8], Text-2-Onto [22], OntoLT [12], KnowItAll [18], TEXTRUNNER [5], OntoGen [19], SnowBall [1], and OntoLearn [34]. Some studies attempt to handle the entire process of knowledge acquisition (concept, instance, attribute, relationship, and taxonomy), while others only address certain segments of it, using methods such as statistical analysis, linguistic parsing, Web mining, and clustering. A good review on ontology learning from text can be found in [11]. Several projects now use machine learning techniques to generalize, classify, or learn new extraction patterns (i.e., KnowItAll, TEXTRUNNER, OntoGen, SnowBall, and OntoLearn) without necessarily resorting to linguistic analyses. Some combine two approaches, such as Text-To-Onto or Mo’K, which specializes in the learning of conceptual categories. Overall, very few approaches have used concept maps to generate domain ontologies or as a layer to index textual LOs. Also, very few investigations from the AIED community attempt to handle the issue of automatically managing textual documents to improve indexing and retrieval. This work is an effort in this direction, and an interesting and recent study, the Language Technology for e-learning Project [30], further envisions the use of multilingual language technology tools and Semantic Web techniques to improve the retrieval of learning material. 2.3 Learning Object Semantics and Services Providing semantic-rich learning environments is one essential issue in current computer-based education. As stated previously, this is a line of research that is being pursued by a number of efforts, including [26], [31], [37], and [50]. The work presented in [26] and [48] underlines the importance of instructional roles and context in building LOs. This context includes domain ontology, instructional role, and instructional design management. However, no automatic methods for building ontologies are provided for such contexts. This is a significant limitation, given the considerable effort required by designers who articulate the domain knowledge. Moreover, from the beginning, the instructional design is restricted to a single methodology (e.g., IMS-LD), which reduces the ability to benefit from the proposed solution in non-IMSLD platforms. A solution has yet to be provided to move to other standards or learning environments. Thus, it is difficult to anticipate the interest of such solutions in ITS. Research conducted by the LORNET network [37], [38] cites the importance of providing LOs with semantics, although their solution is merely offering a set of authoring tools for inputting such content manually. To the authors’ knowledge, this paper presents the first initiative to exploit semiautomatic ontology learning techniques in order to express LO semantics. 3 THE KNOWLEDGE PUZZLE APPROACH: FOUNDATIONS The Semantic Web vision relies on domain ontologies to describe Web content and make it understandable by software agents. Computer-based educators, particularly those from the e-learning community, realize the importance of this vision to sustain the production of reusable LOs [21], [26]. The relevance of domain ontologies has also grown in ITSs, and its usefulness to model expert and learner models is recognized [44]. On the whole, new generations of robust ontology engineering environments  ́  ́ such as Protege [41] have fostered the creation of ontologies. In such a context, the use of domain ontologies to bridge the e-learning, AIED, and ITS communities seems to be a promising solution for a number of issues pertaining to domain knowledge acquisition and dissemination through computer-based education. Since knowledge acquisition bottlenecks represent the worst problem for knowledgebased systems, it is important to explore semiautomatic methods to generate ontologies. First, however, several theoretical issues must be considered. First issue: can we build domain ontologies from text? We postulate that most conceptual and terminological domain structures are described in documents. Thus, applying an ontology generated from texts seems to be a promising avenue of study. Creating ontology-based metadata that can be understood by machines reflects the vision of the Semantic Web. Semantic languages such as RDF and OWL are used in order to express semantic annotations in a standard way. Therefore, this paper aims at using textual LOs from a given domain as input for a knowledge acquisition process. Second issue: what kind of knowledge should we extract? Educational resources predominantly focus on two types of knowledge: declarative and procedural knowledge. Since ontologies basically represent declarative knowledge, this paper concentrates on such statements. From our perspective, procedural knowledge is best represented through rules and does not belong in a domain ontology. In fact, as described by the Semantic Web architecture, the rule layer is on top of the ontology layer [7]. Another important question is: should ontology acquisition tools be able to produce consistent ontologies from texts of different fields? As answering this question pertains to a long-term issue, the paper focuses on techniques to extract consistent domain ontologies from documents pertaining to a single domain (here, the SCORM domain). Third issue: should educational ontologies be generated through approaches that differ from those used in other domains? To answer this question, sources of knowledge representation were investigated, namely, general network-based representations and specific semantic networks. Due to their human-centered origins and their proximity to the field of lexical acquisition, mapping text content and semantic networks somehow seemed natural. In fact, we believe that semantic networks or concept maps consist of interesting and expressive knowledge models that represent learning content. However, semantic networks suffer from an inherent semantic ambiguity. For example, we were unable to differentiate individuals from concepts in the resulting concept maps. Moreover, due to the direct translation of written sentences into concept map sentences, various terms were used to express synonyms, resulting in further ambiguity. In order to reason consistently, another representation was required to effectively represent the ontological content rather than only the learning content: we needed a mining process over the generated concept maps in order to detect ontological concepts and relationships. Description logic (DL), a subclass of first-order logic, appeared to be the most suitable option to formalize concept maps. As an offspring of semantic network representations, DL was able to adequately represent the resulting knowledge and provide inference capabilities. However, another bridge was required between the concept maps and the domain ontology. More precisely, it was decided to provide formal semantics to concept maps through graph theories. These issues are further described in the remainder of the paper. Finally, an additional concern: the possibility of reusing an approach first dedicated to education in other disciplines. In other words, we wondered if the TEXCOMON process was applicable to any other domain. As shown in Section 6, this was successfully demonstrated by comparing our approach with a state-of-the-art ontology learning tool: Text-To-Onto [32]. 4 THE KNOWLEDGE ACQUISITION PROCESS THROUGH TEXCOMON In computer-based education, particularly in the field of ITSs, domain knowledge is defined as representations of expert knowledge. It is assumed that such representations can be expressed through concept maps [33], [36]. TEXCOMON stands for TEXt-COncept Map-ONtology to indicate the process followed in order to convert texts into domain concept maps, which are in turn transformed into an OWL ontology. This ontology represents the implicit domain knowledge contained in LOs, which has yet to be made accessible in training environments. Fig. 1 shows the domain knowledge acquisition process. Human validation is essential for each step of the knowledge acquisition process. Designing domain ontologies does not follow a linear process: it involves numerous revisions before a final consensual solution is developed. Moreover, as the Knowledge Puzzle platform is designed with the ultimate goal of training, it is important to validate the results obtained at each step of the mining process. Humans should confirm and complete the results to guarantee the quality of the ontology. The learning process in the Knowledge Puzzle’s approach is syntactic and domain independent. The Knowledge Puzzle instantiates a series of extraction rules from a set of domain-independent templates. Contrary to many other ontology learning approaches, it does not rely on a supervised method to guide the learning process (i.e., by providing examples and learning a model). As shown in Fig. 1, the domain knowledge acquisition process relies on a number of steps. The first step involves extracting the document structure and mining domain terms and relationships. This results in terminological concept maps. The second part of the process involves the conversion of concept maps into a domain ontology by detecting classes, associations, attributes, and instances and saving them in OWL. Validation is performed by a human expert at each stage of the process, especially to assess the correctness of the generated keywords and concept maps within the TEXCOMON environment. The domain ontology validation methodology is explained further in Section 5. 4.1 Detecting Document Structure and Keywords Detecting the document structure is not trivial due to the multitude of available formats (txt, doc, pdf, html, etc.). Moreover, this issue cannot be avoided when it comes time to analyze documents and, more specifically, to extract sentences to be parsed. Aside from sentences, there is a need to detect other components of LOs (paragraphs, images, tables, lists, examples, definitions, etc.) to face current elearning challenges. For example, document structure should provide learners with the means to refer to the appropriate portion of a document that fulfills their current needs. This is also necessary for a dynamic LO composition, which must rely on more fine-tuned parts to fulfill a precise learning objective. Some studies have focused on structured documents such as XML or textbook documents, others have worked with pdf documents [17] or definition mining [33], but a complete framework able to handle the multitude of formats and structures is yet to be created. The Knowledge Puzzle is restricted to plain text documents as it focuses on detecting and parsing documents at the sentence level and also since all mentioned formats (word, pdf, and html) can be transferred to plain text. Manual annotation capabilities were supplemented to enable additional structural annotations at both the pedagogical level (e.g., annotating definitions and explanations in texts) and the basic structure level (e.g., tables and images). A long-term goal consists of creating a library of structure extractors that work with various document formats. In its initial version, TEXCOMON works with plain text documents that are automatically partitioned into sets of paragraphs, which are in turn composed of series of sentences. This is performed with annotators developed with the Unstructured Information Management Architecture (UIMA) [47]. The other issue worth considering before beginning a mining process pertains to the set of keywords or seed words from texts that are fed into the system. Generally, such keywords are provided by human experts [25]. In the TEXCOMON approach, a machine learning algorithm, Kea-3.0 [20], is used to extract representative n-grams from documents. The initial algorithm was slightly modified to process one document at a time, in order to avoid working with a collection of documents simultaneously. A Sentence Grammatical Map for the Sentence “An Asset Can Be Described with Asset Metadata to Allow for Search and Discovery within Online Repositories, Thereby Enhancing Opportunities of Reuse” The extracted key expressions (one or more words) are then used to collect sentences in which they are found. This enables domain terms and relationships to be extracted with respect to the detected keywords. 4.2 Key Sentences Syntactic Analysis According to [14], two types of grammars represent the structure of sentences in natural languages: constituency grammars and dependency grammars. The selected grammar strongly influences the types of possible semantic analyses. Constituency grammars describe a phrasestructure syntax. In dependencies, each pair of word is related by a grammatical link called a dependency. From a knowledge representation perspective, dependency grammars have a major advantage when compared to constituency grammars: grammatical link dependencies are intuitively close to semantic relationships. Several analyzers can perform dependency analyses. It is important that analyzers be as accurate as possible, i.e., able to produce accurate dependencies. The results presented in [44] suggest that the Stanford University Parser [28] can generate accurate analyses for most sentences encountered. In addition, research in natural language processing at Stanford University happens to be on the cutting edge of what is being done in the field today. For these reasons, the Stanford University Parser was used to transform sentences from documents into typed dependency representations. Each sentence is represented as a Sentence Grammatical Map, i.e., a set of terms linked by the Parser’s typed dependencies. As an example, Table 1 depicts a map that illustrates different grammatical dependencies. TEXCOMON uses these grammatical maps in semantic analysis. 4.3 Pattern-Based Semantic Analysis In this study, a pattern is represented through a set of input and output links. Such links represent the numerous grammatical relationships that are output by the dependency module [16] of the Stanford University Parser. Once a pattern is identified, a method is triggered to compute the semantic structure associated with it. 4.3.1 Extracting Terminology Terminology extraction refers to the discovery of terms that become potential candidates for concepts in an ontology. It can be facilitated by the exploitation of LOs as the primary source of knowledge: LOs are purely didactic documents, providing definitions and explanations about the concepts to be learned. These concepts share the properties of low ambiguity and high specificity, due to their natural goals in the learning context. A set of rules was established to exploit the grammatical maps so as to retrieve specific predefined patterns. These patterns are used to extract a Sentence Semantic Concept Map from the grammatical one (semantic terms and relationships). Terminology patterns rely on elements such as adjectives or nouns to restrict the meaning of a modified noun, e.g., isa (ITS, Tutoring System). They also constitute a very accurate heuristic to learn taxonomic relationships [29]. For example, extracting terminology patterns allows for defining the domain terms used in the previous example (Table 1), which are Asset, Asset metadata, Opportunities, Reuse, Search, Discovery, and Online repositories. Some of the patterns used in the example are All of these terms become candidate terms to express domain concepts. 4.3.2 Extracting Relationships Domain terms must be related in some ways. Extracting relationships refers to identifying linguistic relationships among the discovered terms. Verbs and auxiliaries, which generally also express domain knowledge, become central to such extraction. Again, grammatical pattern structures similar to the ones employed in the previous step are exploited to extract relationships of interest. A verbal relationship pattern used in the example is shown below: nsubjpassðdescribed À 5; Asset À 2Þ auxðdescribed À 5; can À 3Þ auxpassðdescribed À 5; be À 4Þ nnðmetadata À 8; asset À 7Þ prep withðdescribed À 5; metadata À 8Þ This pattern outputs the relationship “can be described with.” Overall, a set of around 20 patterns was identified. At this stage, it is important to understand that there is no filtering of the “important” domain terms. All the key sentences are parsed, and all the recognized patterns are instantiated. The filtering will be done at a later stage during the conversion into an OWL ontology. Each sentence is associated with its main subject (the term Asset, from the previous example). The process described above is repeated for all of the selected sentences. Domain concept maps are generated around given concepts by listing all the relationships where the concepts appear as main subjects. Each domain concept has a domain concept map that describes it and that links it to various documents through relationships and other concepts. This is called its context. It is then possible to retrieve a term or a particular relationship and to be automatically directed to the source sentence, paragraph, or document. This allows enhanced retrieval of the appropriate knowledge. Contexts are also used to provide synthesized views of a concept, which can be highly useful in e-learning. Finally, one important aspect regarding contexts is that they are not only made up of binary relationships, but they also include paths of information. Such paths are used to supplement, clarify, and expand initial relationships. For example, in the sentence “metadata is-used-to-search LOs within online repositories,” the relationship “within,” which links the concepts “LOs” and “online repositories,” provides additional details to the first relationship. 4.4 Generating Domain Ontologies from Domain Concept Maps Domain concept maps act as skeletons to build domain ontologies. This process implies determining classes, associations, attributes, and instances. 4.4.1 Defining Classes Extracting ontological primitive classes from concept maps is performed by detecting high-density components. In TEXCOMON, a domain term is considered a class if it is the main topic of various sentences, thus being a frequent subject in the domain of interest, and 2. it is linked to other domain terms through semantic relationships. Note that a single concept can be expressed in different manners in a text. TEXCOMON can recognize the base form of a concept through stemming. It uses a Java version of the Porter Stemmer [40] to produce the stem associated with each concept. For example, the words “stemmer,” “stemming,” and “stemmed” have the same root: “stem.” This is particularly useful as it allows for recognizing plural forms and certain conjugated verb tenses. Another way of expressing concepts is through abbreviations (e.g., “SCO” stands for “Sharable Content Object”). Although the Stanford University Parser outputs abbreviation links as typed dependencies, it is not always reliable. Hence, TEXCOMON implements an algorithm to identify correct abbreviations, which are stored as acronyms of the current concept and exported as equivalent classes, as shown below. At the time this paper was submitted, TEXCOMON was not equipped to handle anaphora resolutions and could not process antecedents such as “reference model” and “the model” in the following sentences: “SCORM is a reference model 1⁄2. . .. The model 1⁄2. . ..” 4.4.2 Defining Associations In DL, associations consist of properties within a specific domain and range. Each domain and range refers to a class. Verbal relationships express more specialized relationships, which are important in the domain. Basically, all verbal relationships between pairs of classes are considered to be ontological relationships. The relationships generated include simple object properties such as <owl:ObjectProperty rdf:ID=“may_need”> <rdfs:domain rdf:resource=“#training_resources” /> <rdfs:range rdf:resource=“#metadata” /> </owl:ObjectProperty> An object property can also take the shape of a union of classes in its range or domain. This happens when the same relationship (e.g., describes) is encountered between a concept (e.g., metadata) and many other concepts (e.g., content_objects or assets). 4.4.3 Defining Instances and Attributes Extracting instances enables finding objects that are instances of a particular concept. Hearst [24] first talked about linguistic patterns to identify hyponyms (“is a kind of”). Particularly, the pattern “NP1 such as NP2, NP3, and NP4” expresses a hyponymous relationship. For instance, in the sentence “media such as text and images,” text and images are considered as instances of the concept “media.” It is sometimes difficult to differentiate linguistic expressions revealing “instance-of” relationships from expressions that indicate subclass relationships. Suppose that NP1 represents a concept. TEXCOMON uses the following rules to establish whether a given link consists of a subclass link or an instance link: If NP2, NP3, and NP4 are also concepts, they are considered subclasses of NP1. . Otherwise, if NP2, NP3, and NP4 are not considered concepts, they are stored as instances of NP1. Obviously, the different instance patterns apply only to ontological classes. Examples of extracted instances include . <grouping rdf:ID=“IMS” /> <grouping rdf:ID=“ARIADNE” /> As far as attributes are concerned, they describe the concept itself. They can be extracted by using contextual information or relying on nominal modifiers to express potential properties. TEXCOMON uses the following patterns to extract concept attributes: . < attr> <C> <verb> . . . , where C denotes a concept, and attr denotes a modifier. A sample text that matches this pattern would be the following: . . . inline metadata is . . . , where metadata is a concept. <attr> of <C> (e.g., “identifier of asset”) or <C>’s<attr> (“asset’s identifier”). . <C> have/possess <attr>. Similar techniques to identify concept attributes are found in [3] and [39]. If <attr> is a concept, the attribute is considered an OWL Object Property; otherwise, it is created as a Data Type Property. . 5 DOMAIN ONTOLOGY EVALUATION KNOWLEDGE PUZZLE IN THE Increased use of domain ontologies requires well-established methods to evaluate them. This section investigates the performance of the domain ontology generated through TEXCOMON, based on a certain number of measures. 5.1 Evaluation Methodology Evaluating ontologies remains an ongoing research challenge, and various methods have been proposed, as summarized in [9]. However, one possible criticism of such approaches is that they are only designed to evaluate certain specific characteristics of the ontology. It seems that ontology assessment is even more critical with methods generated automatically. Each extraction step must undergo both quantitative and qualitative evaluations, e.g., evaluation of terms, concepts, taxonomy, and conceptual relationships. For these reasons, a four-dimensional evaluation methodology is proposed: A syntactic evaluation uses RACER-PRO [42] to assess the consistency of the ontology. A structural evaluation strives to detect the structural characteristics of the generated domain ontology. Based on different measures, these characteristics can be helpful for ontology designers who must select the available ontology that best suits their needs. A semantic evaluation involves human domain experts. They assess the quality of the ontology or, at least, the plausibility of its concepts and relationships. A comparative evaluation juxtaposes results by running different state-of-the-art tools on the same corpus. Given that generating domain ontologies is not an exact science in terms of processes and results, one of the most interesting evaluation indicators in this field consists of testing and comparing the available ontology learning tools with the same corpuses. 5.2 Experiment Description We used a corpus about the SCORM standard compiled from two handbooks: SCORM 2004 Third Edition Content Aggregation Model (CAM) and SCORM 2004 Third Edition Runtime Environment [43]. We created 36 plain text documents from these two handbooks (approximately 29,879 words, 1,578 sentences, and 188 paragraphs) by manually excluding sample code and certain chunks such as “Refer to Section” and selecting declarative sentences. From this corpus, TEXCOMON extracted a set of 1,139 domain terms and 1,973 semantic relationships. The essence of the experiment was to select a set of key terms from the SCORM domain in order to detect their characteristics in the generated ontology. The underlying assumption of such experiments is that ontologies are representative of search terms if [2] the search terms exist as classes in the ontology, there is a close structural proximity to the corresponding classes, 3. the corresponding classes are richly described, 4. the corresponding classes are interlinked through many relationships, and 5. the corresponding classes are central in the ontology. Table 2 presents the sought terms considered key concepts for the SCORM domain. These terms have been validated by the domain experts as being representative of the SCORM domain. We do not pretend that this is an exhaustive list or a best set. However, the experts agreed that these keywords should normally exist in an ontology defining the SCORM standard. The other facets of the experiment intended to test whether more or less compact ontologies affect the quality of the results. As stated above, a domain term is considered a concept when it is involved with other concepts in a number of output relationships. Such number of relationships can be parameterized. This experiment considers four values of the number of output relationships (I): I 1⁄4 2 (KP-2), I 1⁄4 4 (KP-4), I 1⁄4 6 (KP-6), and I 1⁄4 8 (KP-8). Finally, seven corpuses from 36 documents were created and organized. Basically, each corpus was a superset of the previous one. This permits assessing the quality of the ontology as new domain documents are added to the previous corpus (Table 3). This also facilitates a better understanding of the contribution of certain specific documents to the ontology. 1. 2. 5.3 Syntactic Evaluation As a knowledge representation module, the syntax and semantics of OWL ontologies must be validated. Reasoners are typically used to compute classes that cannot be satisfied, subsumption hierarchies, and individual types. TEXCOMON uses RACER PRO [42] in order to validate the consistency of the ontology. Concepts that cannot be satisfied signal faulty modeling. Discovering inconsistent concepts catches the attention of human validators who can subsequently correct them. 5.4 Structural Evaluation The structural evaluation approach is based on a set of metrics (defined in [2]) that consider the ontology a graph entity. Initially, such metrics were developed to rank ontologies and sort them for retrieval purposes, similar to Google’s PageRank algorithm. Given a set of search terms, Alani and Brewster [2] searched for the best ontology to represent these terms. These four structural metrics consist of the Class Match Measure (CMM), the Density Measure (DEM), the Betweenness Measure (BEM), and, finally, the Semantic Similarity Measure (SSM). The total score of these four units is added to rank ontologies with respect to specific terms sought. We implemented functions (ONTO-EVALUATOR library) to perform the different computations of the metrics based on the exact formulas described in [2]. The following sections describe all the metrics. 5.4.1 The Class Match Measure (CMM) The CMM evaluates the coverage of an ontology for the provided keywords (Table 2). Given the input keywords, the ONTO-EVALUATOR searches through the ontology classes to determine if the keywords are expressed as classes (exact match) or if they are included in class labels (partial match). Results show that CMM tends to improve as the threshold decreases within a corpus. This indicates that many concepts that contain the sought terms (partial or total match) are deleted as the threshold increases, thus eliminating relevant concepts (according to the domain expert) that, in fact, should have been preserved. For cases that considered exact and partial matches, KP-2 and KP-4 seem to be the best ontologies. An interesting phenomenon occurs when considering solely exact matches (classes whose labels are identical to the sought term): different results are obtained. In this case, KP-2, KP-4, and KP-6 yield identical results with the richest corpus. However, KP-8 performs worse than the others. This indicates that the sought terms, considered key domain terms, are involved in up to seven relationships with other domain terms. Considering exact and/or partial matches may affect other metrics. In fact, most results are presented according to the number of matched classes resulting from the CMM. When the impact of an exact match is clearly identified, it is indicated in the following metrics. 5.4.2 The Density Measure (DEM) The DEM expresses the degree of detail or the richness of the attributes of a given concept. It is assumed that a satisfactory representation of a concept must provide sufficient detail regarding its nature. Density measurements include the number of subclasses, inner attributes, and siblings, as well as the number of relationships maintained with other concepts. The DEM tends to increase proportionally with the number of concepts. Such variations result from the abundant information in the new corpus. For example, in this experiment, Corpuses 6 and 7 contribute many new relationships, which explain a drastic DEM increase, especially when the threshold is 2. 5.4.3 The Betweeness Measure (BEM) The BEM calculates the betweenness value for each sought term in the generated ontologies. It measures the extent to which a concept lies on the paths between others. Class centrality is considered important in ontologies. A high BEM shows the centrality of this class. As in ActiveRank [2], Onto-EVALUATOR uses the BEM provided by JUNG [23]. This algorithm calculates the number of shortest paths that pass through each concept in the ontology (considered a graph). A higher BEM is assigned to concepts that occur in a larger number of shortest paths between other concepts. In this experiment, we noticed that a reasonable number of relationships must be retained in order to reach interesting BEMs. Again, thresholds 2 and 4 seem to generate the best results. 5.4.4 The Semantic Similarity Measure (SSM) The last measure, the SSM, computes the proximity of the classes that match the sought keywords in the ontology. As Alani and Brewster [2] stated, if the sought terms are representative of the domain, the corresponding domain ontology should link them through relationships (taxonomic or object properties). Failure to do so may indicate a lack of cohesion in the representation of the domain knowledge. The SSM is based on the shortest path that links a pair of concepts. The SSM never decreases, regardless of the threshold value. In general, high thresholds result in poorer performance of SSM values. However, with large corpuses, higher thresholds become more interesting. As previously stated, considering solely exact matches has a greater impact on this metric. Exact matches lead to very similar results for KP-2, KP-4, and KP-6, especially with the richest corpus (Corpus 7), where identical results are obtained. This is not the case when both partial and exact matches are investigated. Finally, based on these four metrics, an overall score is computed. Let M 1⁄4 fM1⁄21; M1⁄22; M1⁄23; M1⁄24g 1⁄4 fCMM; DEM; SSM; BEMg, wi be a weight factor, and O be the set of ontologies to rank. The score is computed as follows [2]: Identical or different weights can be assigned to each metric. The overall score is further explained in the comparative evaluation, where TEXCOMON and Text-ToOnto ontology scores are presented. 5.5 Comparative Evaluation Comparative evaluations are performed by juxtaposing overall scores of the TEXCOMON ontologies (KP-2, KP-4, KP-6, and KP-8) and those of the Text-To-Onto ontologies. To perform such comparisons, two ontologies (TTO-1 and TTO-2) are generated from each corpus with Text-To-Onto, a state-of-the-art tool in ontology learning from text. The main difference between these two ontologies resides in the use of a different support in association rule learning (respectively, 0 and 0.1). To generate a domain ontology using Text-To-Onto, the latter was run on each of the seven corpuses using the KAON Workbench [32]. The following actions were performed for each corpus: Term extractions. These actions consist of using a number of statistical measures to identify domain terms found in texts. . Instance extractions. These actions aim at populating the domain ontology. . Association rule extractions with a minimum support of 0 and another of 0.1. Learned associations are added to the ontology as properties. Association rule learning endeavors to discover frequently co-occurring items within a data set to extract rules related to such items. The support of association rules equals the percentage of groups that contain all of the items listed in such association rules. This experiment shows that even the support of a 0.1 threshold discarded all association rules generated by Text-To-Onto. In fact, TTO-2 results are significantly disparate compared to those from TTO-1, which contains numerous meaningless properties that enhance the value of certain structural metrics. Hence, it was decided to present results for both ontologies, TTO-1 and TTO-2, although the Knowledge Puzzle Ontologies and TTO-2 are actually being compared. . Relation learning. This action aims to extract verbal relationships. . Taxonomy learning. This action, using the Taxo Builder tool, aims at discovering hierarchical relationships. The combination-based approach from Hearst’s patterns and heuristics is used. The FCAbased approach is not exploited for two reasons: first, no comparison basis existed for the Knowledge Puzzle, and second, the authors were not strongly convinced by the results of the formal concept analysis (neither with verbs nor with lexicographer classes). The Pruner and the OntoEnricher were not used. In fact, OntoEnricher is supposed to enhance the ontology by using other external resources such as Wordnet. However, we wanted to compare information extraction from the same corpus without resorting to other knowledge sources. The pruner was tested but actually suggested pruning concepts that should not be removed from the resulting ontology (for example, adl initiative, activity tree, and content organization). We believe that this is because Text-To-Onto relies only on statistical features (cumulative frequency) to prune some concepts and tends to keep only statistically significant concepts. The overall scores of Text-To-Onto and TEXCOMON ontologies are generated with the same metrics. Overall scores. The total score is computed once the four measures are applied to all generated ontologies. The total score is calculated by adding all the measurement values, taking into account the weight of each measure, which can be adapted to reflect the relative importance of each value for ranking purposes. Scores are computed for ontologies generated from the entire corpus, namely, Corpus 7. When using equally distributed weights (0.25) for all metrics (Table 4), it is clear that TEXCOMON outperforms Text-To-Onto (when compared to TTO-2). KP-8 is the only ontology whose score is lower than TTO-2. Moreover, when considering scores from a CMM with exactly matching input terms, using Corpus 7 and a weight distribution of 0.5, 0, 0, and 0.5, we obtained the results shown in Table 5. Here, KP-4 has a better overall score than KP-2. This means that when two metrics, namely, CMM and SSM, are more important for designers, KP-4 would be the best ontology option. One last example investigates the results of considering solely the CMM. Here, KP-2, KP-4, and KP-6 obtain identical scores and ranks (Table 6). Another type of assessment, called semantic evaluation, aims at detecting to what degree and how well the generated domain ontology reflects the domain knowledge. We believe that such evaluation can only be performed by human domain experts. 5.6 Semantic Evaluation Semantic analyses rely on human experts to assess the validity of the ontology. This evaluation comes to reinforce the results of the previous evaluations (syntactic, comparative, and structural). Table 7 summarizes the evaluation of the four ontologies KP-2, KP-4, KP-6, and KP-8, using the mean scores for pertinence, as expressed by two experts. These experts are specialized in the implementation of SCORM-compliant applications and the deployment of SCORM-based content. As shown in Table 7, such positive results are promising. The same procedure is then repeated for the Text-ToOnto ontologies (Table 8). The “Pertinent Defined Classes” column disappears in Table 8 simply because Text-To-Onto does not extract any defined classes. We can notice that the scores for pertinent primitive classes and hierarchical relationships are nearly identical: in fact, the only difference between TTO-1 and TTO-2 lies in the use of a different support in association rule learning. Moreover, TTO-1 generated a very important number of unlabeled relationships (5,683), among which only 18 relationships were considered pertinent by the two experts. The experts discarded the other relationships due to their very low support (< 0.1). 6 RESULT, ANALYSIS, AND DISCUSSION Results of this experiment show that TEXCOMON ontologies obtained higher scores than the Text-To-Onto ontologies, especially when compared with TTO-2. One interesting feature is the variation of weights and exact/partial matches and their impact on the overall ontology scores. The experiments suggest that partial matches can skew results and larger weights should be used with exact matches. Basically, some important questions must be taken into account in order to perform such variations: First, what are the most important metrics according to the domain, the goal(s), and the needs? . Second and most important, given an ontology with low connectivity (KP-2), is it possible to obtain a more compact ontology and preserve the performance or scores of KP-2? If the answer to the last question is affirmative, a more compact ontology should be favored over one that is less compact, as it includes more strongly interconnected concepts and still conserves the sought domain terms. For example, in Table 5, KP-4 must be selected, whereas Table 6 shows KP-6 to be the best ontology: it has the same score as KP-2 and KP-4, yet it is much more compact than both of them. Table 9 compares the output of TEXCOMON and TextTo-Onto in terms of the number of concepts and relationships (taxonomic and nontaxonomic). Notice that . . . . TEXCOMON results can be parameterized, which is not the case for Text-To-Onto. Actually, ontology designers may be interested in larger or denser ontologies, and they should be given the opportunity for calibrating the generating process. The decreasing number of concepts and relationships in TEXCOMON is consistent with the threshold increase. An interesting aspect surfaces in the number of nontaxonomic links in TTO-2 (i.e., 33) compared to TTO-1 (i.e., 5,683). This drastic drop pertains to the 0.1 support used in TTO-2, meaning that TTO-1 created association rules with a support lower than 0.1. Such relationships contribute to a somewhat improved performance of TTO-1, especially for SSM measurements, although they actually have no “meaning” or real interest as ontological relationships. Moreover, the tremendous number of extracted relationships makes the task difficult for ontology designers. Another way of comparing both systems relates to the use of a keyword to observe results in the ontologies (KP and TTO). Again, significant differences appear between TTO-1 and TTO-2. Tables 10 and 11 illustrate such statistics for the terms “SCO” and “asset.” TTO-1 reveals numerous properties (i.e., 118 properties in Table 10 and 172 in Table 11), especially when compared with TTO-2 (i.e., 0) and TEXCOMON. A closer look at the relationships actually generated shows an important number of noisy relationships. Overall, Text-To-Onto presents two problems: first, it fails to extract relationship labels between concepts, especially relationship output by association learning, and second, it does not save the complete label of concepts and only stores the stem. TEXCOMON handles both types of labels (complete labels and stems) and also provides labels for relationships. In general, TEXCOMON provides interesting results. For example, concepts are generally rich, and they have a sufficient quantity of parents (a multilevel ontology rather than a flat structure). It was also possible to generate some defined classes (by stating an equivalent class relationship between a concept and its acronym), which had not been done before. Additionally, conceptual relationship learning aspects are particularly interesting. Another interesting facet is that TEXCOMON offers the possibility of calibrating thresholds to suit ontology designers’ wishes. Given a set of sought terms considered to be important domain concepts: . . Thresholds can be calibrated by emphasizing CMM if the most important feature is a partial or exact match of search terms as ontological concepts. If the important feature consists of having richly described concepts with an important number of attributes and relationships, the DEM should have a heavier weight in the overall evaluation. If the important feature targets richly interconnected concepts to make them central to the ontology, semantic similarities and betweenness should be favored. We do believe that all measures are important. In general, when taking into account the overall scores, ontologies KP-2 and KP-4 seem satisfactory given the corpus sizes. Although there is no single right way to evaluate ontologies, certain lessons can be drawn from these experiments: . 1. 2. 3. 7 In the absence of a gold standard for a particular domain ontology, it is not always possible to create one. Hence, another type of ontology assessment must be selected. Comparing the generated domain ontology to those generated with state-of-the-art tools can be beneficial as this shows the added value of new tools or platforms. This confirms the interest for comparative evaluations as proposed in this paper. Evaluating ontologies from a structural perspective can also be relevant, as shown in [2]. Comparing this structural evaluation with other generated ontologies, as conducted herein, is meaningful. THE INTEREST OF THE APPROACH FOR THE AIED AND E-LEARNING COMMUNITY The Knowledge Puzzle approach proposes the implementation of AIED techniques for LOs, which means capabilities for representing domain knowledge in LOs, demonstrating reasoning pertaining to this domain model, building instructional scenarios according to certain pedagogical theories, and adapting learning content to suit learners’ needs. This can be achieved by using ontologies and exploiting a number of services offered to course designers, learners, and educational systems. The domain ontology is provided through the process described in this paper. The following sections illustrate the interest of the approach for the online educational community and underline how the domain ontology is exploited in an educational context. In the context of the Knowledge Puzzle Project, we implemented an ontology-based Semantic Web Architecture that represents different models in the traditional ITS architecture through ontologies. Dynamic Learning Knowledge Objects (LKOs) are generated and act as small tutoring systems: they are equipped with a domain model, a learner model, and a tutor model. These LKOs exhibit various characteristics: they are active, domain knowledgeable, independent, reusable, and theory aware. We believe that this architecture pools a synergy of the strengths found in the fields of e-learning and ITSs. Such architecture is documented elsewhere [51], but the gist of the approach is briefly summarized here. 7.1 Providing Services for Course Designers Given the tremendous amount of LOs, providing the means to annotate contents in a semiautomatic manner by generating domain ontologies is of the utmost importance. In the context of the Knowledge Puzzle Project, designers are provided with a series of tools to generate accurate representations of LOs and their domain contexts. This semiautomatic annotation facility offers authoring support for course designers who can search through learning content more efficiently. However, domain knowledge is not the only aspect considered. In fact, certain studies highlight the importance of an instructional role ontology [48], [51] to define knowledge chunks in LOs that can be linked to domain concepts (for example, a definition for “SCORM”). These annotations, manually performed by the Knowledge Puzzle tools, can be recycled efficiently in other curricula that use such search facilities. Designers are also provided with the ability to state a learning objective as a competency to be mastered over domain concepts. Rulebased authoring tools are offered to link competency levels and instructional roles. Finally, the proposed platform enables the defining of rule-based instructional scenarios that exploit semantic annotations. These scenarios guide course designers in the process of creating learning resources, according to a chosen pedagogical theory. 7.2 Providing Services for Learners The first service provided for learners comprises the ability to obtain LOs tailored to their needs and profile, which is an important issue in computer-based learning. This point is further explained in Section 7.3. A learner model is stored in an IMS ePortfolio to save information pertaining to the learners’ prior knowledge and acquired competencies (which are linked to the domain model). This portfolio can then be imported by any training application that is compatible with the standard, including the Knowledge Puzzle. The second service consists of using concept maps for constructivist learning: concept maps are not only considered intermediary templates to build the domain ontology. In fact, studies have revealed concept maps to be useful training assets in constructivist environments [30]. The Knowledge Puzzle defines a formal relationship between a concept and the generated concept map to enable reuse in training sessions. This formal relationship represents the concept context. This is very important within constructivist environments where prior knowledge is used as a framework to understand and acquire new knowledge. The Knowledge Puzzle offers capabilities to explore learning material and related domain concepts. Moreover, as expert systems can justify their answers by providing inference execution traces, the Knowledge Puzzle can show textual contents, thus providing solid proof of its “expertise.” This can be done at a more or less fine-grained level and related to the current concept, instance, or association. 7.3 Providing Services for Educational Systems One of the most critical issues in current efforts toward semantic-rich learning environments pertains to offering services for dynamic LO composition given a set of parameters, including learners’ profiles and competencies to be mastered. The importance of better fine-tuned retrieval of learning content was brought up through the discussion of an ontology of instructional roles [48]. In fact, due to the very nature of their pedagogical goals, LOs include various types of instructional roles such as definitions, examples, and exercises. Thanks to the generated domain ontology, links between LOs and domain concepts are made explicit. Moreover, thanks to the instructional role ontology, links between instructional roles and LOs are also explicit. Finally, instructional roles are applied to domain concepts, thus providing an interrelated and meaningful structure (e.g., an explanation about concept X, a definition of concept Y, etc.). This structure is essential to offer composition services based on a given learner profile, the learning objective, and a specific instructional theory. The service automatically generates an adapted LO called LKO. As shown in Fig. 2, an LKO is encapsulated as an applet that contains a course view (with the planned curriculum) and a concept map view that indicates the various concepts and their links with one another. Finally, another important service enables ontologies with robust semantics and languages to offer a certain level of reasoning for LO content. In fact, one of the main objectives when building a domain ontology is to provide systems with the ability to reason over this domain. However, to date, less attention was paid to this issue by the computer-based education community, which tends to focus more on knowledge representation facets. Reasoning involves a number of abilities considered by the Knowledge Puzzle: The ability to query the ontology. The Knowledge  ́  ́ Puzzle uses the Protege OWL Java API and the SQWRL [46] query language and API to retrieve individuals or tuples  ́  ́ that match a given query. Protege also has the capability to use built-in SQWRL queries to examine the structure of an OWL ontology. This is helpful to reason over LO content by setting up high-level rules that retrieve . . . . concepts that are either more specific or more general, all the concepts related to a source concept X, properties that relate concept X to its context, and properties between concept X and concept Y. The ability to reason over the ontology. In the context of the Knowledge Puzzle Project, reasoning over the ontology enables the discovery of related LOs or segments of LOs. It also makes it possible to create a pedagogical engine based on instructional theories [51] using rules defined in SWRL and the Jess Rule Engine. Other competencies include the abilities to explain the ontology, by using the source document from which a particular concept, association, or individual was created, . to extend learner queries by using the domain ontology structure, and . to cluster similar LOs according to their content. Moreover, state-of-the-art DL reasoners such as RACER [42] exhibit satisfactory performance for knowledge bases with large TBOXES. This is particularly interesting for this study, as the main task of our learning tool entails the extraction of a TBOX rather than an ABOX from texts. The terms ABOX and TBOX are used to describe two different types of knowledge in ontologies. TBOX statements describe a system in terms of classes and properties. ABOX are TBOX-compliant statements about that conceptual schema. Since the object of this research is more on defining a conception about the domain of interest, the data mining task focuses on TBOX extraction. Other current efforts include the option of providing automatic explanation facilities, as well as the ability of supplying answers to questions from texts, which is generally a challenging issue, even more so in the field of education. . 8 CONCLUSIONS AND FURTHER WORKS The Knowledge Puzzle Project proposes an entire framework to acquire and exploit knowledge in the fields of e-learning and ITSs. The proposed solutions for knowledge acquisition stems from a hybrid approach composed of natural language processing, pattern matching, and machine learning. On the one hand, it contributes to the emergence of semantic-rich learning environments and borrows techniques from artificial intelligence to enrich LOs. On the other hand, it uses textual LOs or any textual document from a given domain as material to extract a relevant knowledge base for such domain. Given the difficulty of creating such a knowledge base, semiautomatic methods are essential to reduce the burden on experts, and they are necessary for ITSs. Furthermore, these methods enable the exploitation of LOs in ITSs. This opportunity equips ITSs to access learning material that, up to date, was devoted to traditional e-learning applications. In fact, this is an important issue due to the increasing number of LOs, and their wide adoption in the worlds of business and research. Used as backbones to sustain entire frameworks, ontologies are dedicated to define the domain model, the expert model, the learner model, and the tutor model. Given the key role of ontologies in many applications, it is essential to provide tools and services to help users design and maintain high-quality ontologies. Automatic methods coupled with well-defined evaluation methods are crucial to their successful implementation in real-world settings. This paper presented a methodology to assess the quality of generated domain ontologies, and the results were compared with state-of-the-art tools in ontology learning from text. A corpus of text documents related to e-learning standards was used to illustrate this effort. Moreover, the interest of the approach also resides in services that can be offered to designers, learners, and educational systems, in order to exploit this knowledge base. These services include an on-the-fly composition process for LKOs, which differ from traditional LOs: they are generated according to a specific instructional theory, learner profile, and competence-based learning objectives, and they are enriched by domain structures that explain their content. Other services comprise authoring assistance and guidance components for course designers, as well as search and reasoning facilities for the domain ontology. A number of enhancements and extensions are possible. We would like to enrich the pattern knowledge base with new structures and explore other ways of expressing patterns. Moreover, further thorough ontology and concept map evaluation techniques must be performed. Additionally, the increasing number of available ontologies raises concerns for their alignment and updating. At this stage of the project, automatically updating an ontology necessitates the execution of the text mining process over an enriched corpus. This execution creates a new OWL file that takes into account the enriched corpus. Future efforts should be directed at updating the existing ontologies by only inserting new ontological objects and checking the consistency of the modified ontology. 
